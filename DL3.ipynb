{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-14 13:42:49.483731: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import pretty_midi\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "import itertools\n",
    "import librosa\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('/home/gorelikk/ASS3/lyrics_train_set.csv', header=None, usecols=[0, 1, 2])\n",
    "train_set.columns = ['Artist', 'Song_name', 'Lyrics']\n",
    "test_set = pd.read_csv('/home/gorelikk/ASS3/lyrics_test_set.csv', header=None, usecols=[0, 1, 2])\n",
    "test_set.columns = ['Artist', 'Song_name', 'Lyrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lexical_Richness(data):\n",
    "  data['Unique_Words'] = data['Lyrics'].apply(lambda x: len(set(x.split())))\n",
    "  data['Total_Words'] = data['Lyrics'].apply(lambda x: len(x.split()))\n",
    "  data['Lexical_Richness'] = data['Unique_Words'] / data['Total_Words']\n",
    "\n",
    "  top_lexical_richness = data[['Artist', 'Song_name', 'Lexical_Richness']].sort_values(by='Lexical_Richness', ascending=False).head(10)\n",
    "\n",
    "  # Plotting\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  plt.barh(top_lexical_richness['Song_name'], top_lexical_richness['Lexical_Richness'], color='skyblue')\n",
    "  plt.xlabel('Lexical Richness')\n",
    "  plt.ylabel('Song Name')\n",
    "  plt.title('Top 10 Songs by Lexical Richness')\n",
    "  plt.gca().invert_yaxis()  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Lexical_Richness(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Lexical_Richness(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to count words in lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words per song in the Train Set: 299.48\n",
      "Average number of words per song in the Test Set: 269.60\n"
     ]
    }
   ],
   "source": [
    "def count_words(lyrics):\n",
    "    return len(lyrics.split())\n",
    "\n",
    "train_set['Word_Count'] = train_set['Lyrics'].apply(count_words)\n",
    "test_set['Word_Count'] = test_set['Lyrics'].apply(count_words)\n",
    "\n",
    "mean_word_count_train = train_set['Word_Count'].mean()\n",
    "mean_word_count_test = test_set['Word_Count'].mean()\n",
    "\n",
    "print(f\"Average number of words per song in the Train Set: {mean_word_count_train:.2f}\")\n",
    "print(f\"Average number of words per song in the Test Set: {mean_word_count_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique artists in the Train Set: 226\n",
      "Number of unique artists in the Test Set: 5\n"
     ]
    }
   ],
   "source": [
    "num_artists_train = train_set['Artist'].nunique()\n",
    "num_artists_test = test_set['Artist'].nunique()\n",
    "\n",
    "print(f\"Number of unique artists in the Train Set: {num_artists_train}\")\n",
    "print(f\"Number of unique artists in the Test Set: {num_artists_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocabularies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "filters = '!\"#$%()*+,&-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "for lyrics in train_set.Lyrics.tolist():\n",
    "    words = re.sub(f'[{filters}]', '', lyrics).split()\n",
    "    vocab.update(words)\n",
    "vocab.discard('')\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "index2word = {i: w for w, i in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7537"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert lyrics to indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['tokens'] = train_set['Lyrics'].apply(lambda x: [word2index.get(word, 0) for word in re.sub(f'[{filters}]', '', x).split()])\n",
    "test_set['tokens'] = test_set['Lyrics'].apply(lambda x: [word2index.get(word, 0) for word in re.sub(f'[{filters}]', '', x).split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Data Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class for handling song data, which includes lyrics and MIDI music features.\n",
    "    \n",
    "    Attributes:\n",
    "        data (DataFrame): A pandas DataFrame containing song metadata and lyrics.\n",
    "        midi_path (str): Directory path where MIDI files are stored.\n",
    "        word2vec_model (dict): Pre-trained word2vec model for converting words to vectors.\n",
    "        vocab (list or dict): Vocabulary used in the dataset.\n",
    "        method (str): Specifies the method to extract features from MIDI files.\n",
    "        word2index (dict): Dictionary mapping words to their indices.\n",
    "        index2word (dict): Dictionary mapping indices back to words.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, word2vec_model, vocab, midi_path, method):\n",
    "        \"\"\"\n",
    "        Initializes the SongDataset class with the provided parameters and preprocesses the lyrics data.\n",
    "        \n",
    "        Parameters:\n",
    "            data (DataFrame): Song data.\n",
    "            word2vec_model (dict): Word2Vec model for word vectorization.\n",
    "            vocab (list or dict): Vocabulary list or dictionary.\n",
    "            midi_path (str): Path to the folder containing MIDI files.\n",
    "            method (str): Method to use for extracting MIDI features ('model1' or 'model2').\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.midi_path = midi_path\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.vocab = vocab\n",
    "        self.method = method\n",
    "        self.data['Lyrics'] = self.data.apply(lambda row: self.filter_chars_from_lyrics(row['Lyrics']), axis=1)\n",
    "        self.word2index = word2index\n",
    "        self.index2word = index2word\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an item by its index and processes it into model inputs.\n",
    "        \n",
    "        Parameters:\n",
    "            idx (int): Index of the data point to retrieve.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple containing the input features and target labels for the model.\n",
    "        \"\"\"\n",
    "        row = self.data.iloc[idx]\n",
    "        artist, song_name, lyrics = row['Artist'], row['Song_name'], row['Lyrics']\n",
    "        if self.method == 'model1':\n",
    "            midi_f = self.features_model1(artist, song_name)\n",
    "        elif self.method == 'model2':\n",
    "            midi_f = self.features_model2(artist, song_name)\n",
    "        word_vec, labels = self.find_tokens_vec(lyrics)\n",
    "        inputs = torch.cat([word_vec, midi_f.repeat(word_vec.size(0), 1)], dim=-1)\n",
    "        return inputs, labels\n",
    "\n",
    "    def filter_chars_from_lyrics(self, lyrics):\n",
    "        \"\"\"Removes special characters from lyrics to simplify the text.\"\"\"\n",
    "        return re.sub(f'[{filters}]', '', lyrics)\n",
    "\n",
    "    def features_model1(self, artist, song_name):\n",
    "        \"\"\"Extracts MIDI features using the first model approach.\"\"\"\n",
    "        midi_file = self.retrieve_midi(artist, song_name)\n",
    "        try:\n",
    "            midi = pretty_midi.PrettyMIDI(os.path.join(self.midi_path, midi_file))\n",
    "            beats = midi.get_beats()\n",
    "            notes = np.concatenate([np.array([[note.start, note.end, note.pitch, note.velocity]]) for instrument in midi.instruments for note in instrument.notes])\n",
    "            midi_features = torch.from_numpy(np.concatenate([beats, notes.flatten()])).float()\n",
    "        except Exception as e:\n",
    "            midi_features = torch.zeros((113,), dtype=torch.float32)\n",
    "        return midi_features\n",
    "\n",
    "    def features_model2(self, artist, song_name):\n",
    "        \"\"\"Extracts MIDI features using the second model approach, including chroma and is_drum.\"\"\"\n",
    "        midi_file = self.retrieve_midi(artist, song_name)\n",
    "        try:\n",
    "            midi = pretty_midi.PrettyMIDI(os.path.join(self.midi_path, midi_file))\n",
    "            features = []\n",
    "            for instrument in midi.instruments:\n",
    "                notes = instrument.notes\n",
    "                beats = midi.get_beats()\n",
    "                velocity = np.mean([note.velocity for note in notes])\n",
    "                pitch = np.mean([note.pitch for note in notes])\n",
    "                chroma = midi.get_chroma()\n",
    "                chroma_mean = np.mean(chroma, axis=1) if chroma.size else np.zeros(12)\n",
    "                is_drum = 1 if instrument.is_drum else 0\n",
    "                features.extend([len(notes), np.mean(beats), velocity, pitch, is_drum])\n",
    "                features.extend(chroma_mean.tolist())\n",
    "            midi_features = torch.tensor(features).float()\n",
    "            expected_length = 140  \n",
    "            features += [0] * (expected_length - len(features))\n",
    "            midi_features = torch.tensor(features).float()\n",
    "        except Exception as e:\n",
    "            midi_features = torch.zeros((140,), dtype=torch.float32)  # Adjust the size if necessary\n",
    "        return midi_features\n",
    "\n",
    "\n",
    "    def find_tokens_vec(self, lyrics):\n",
    "        \"\"\"Converts lyrics into vectors using the word2vec model and prepares labels.\"\"\"\n",
    "        vec, labels = [], []\n",
    "        lyrics_tokens = self.filter_chars_from_lyrics(lyrics).split()\n",
    "        lyrics_tokens = [word for word in lyrics_tokens if word]\n",
    "        for word in lyrics_tokens:\n",
    "            labels.append(self.word2index.get(word, 0))\n",
    "            if word in self.word2vec_model:\n",
    "                vec.append(self.word2vec_model[word])\n",
    "            else:\n",
    "                vec.append(np.zeros((300,)))\n",
    "        vec = torch.tensor(vec, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        return vec, labels\n",
    "\n",
    "    def retrieve_midi(self, artist, song_name):\n",
    "        \"\"\"Retrieves the MIDI file corresponding to the specified artist and song name.\"\"\"\n",
    "   \n",
    "        artist = artist.lower().replace(' ', '_')\n",
    "        song_name = song_name.lower().replace(' ', '_')\n",
    "        file_name = f\"{artist}_-{song_name}.mid\"\n",
    "        try:\n",
    "            midi_file = next(filter(lambda x: x.lower() == file_name, os.listdir(self.midi_path)))\n",
    "            return midi_file\n",
    "        except StopIteration:\n",
    "            print(f\"Warning: No MIDI file found for {artist} - {song_name} in {self.midi_path}\")\n",
    "        return None\n",
    "        # artist = artist.replace(' ', '_')\n",
    "        # song_name = song_name.replace(' ', '_')\n",
    "        # file_name = f\"{artist}_-_{song_name}.mid\"\n",
    "        # midi_file = next(filter(lambda x: x.lower() == file_name, os.listdir(self.midi_path)))\n",
    "        # return midi_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    A custom collate function for batching song data, which includes padding sequences\n",
    "    so that all inputs and labels in the batch have the same length.\n",
    "    \n",
    "    This function is typically used to prepare batches of data when using variable-length\n",
    "    sequences in models that require input tensors of the same shape, such as RNNs.\n",
    "\n",
    "    Parameters:\n",
    "        batch (list of tuples): A list where each tuple contains inputs and labels for a single data point.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Contains two tensors:\n",
    "            inputs_padded (Tensor): The padded input features tensor.\n",
    "            labels_padded (Tensor): The padded labels tensor.\n",
    "    \"\"\"\n",
    "    inputs, labels = zip(*batch)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "    return inputs_padded, labels_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLyrics(nn.Module):\n",
    "    \"\"\"\n",
    "    A deep learning model based on the LSTM (Long Short-Term Memory) architecture for sequence processing,\n",
    "    particularly suitable for tasks like language modeling or any sequence prediction tasks.\n",
    "\n",
    "    Attributes:\n",
    "        first_lstm (nn.LSTM): The first LSTM layer that processes the input sequence.\n",
    "        second_lstm (nn.LSTM): The second LSTM layer that further processes the sequence from the first LSTM layer.\n",
    "        first_linear (nn.Linear): A linear transformation layer applied after the LSTM layers.\n",
    "        second_linear (nn.Linear): The final linear layer that outputs predictions for each token in the sequence.\n",
    "        dropout (nn.Dropout): Dropout layer to reduce overfitting by randomly setting a fraction of the input units to 0.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, vocab_size):\n",
    "        \"\"\"\n",
    "        Initializes the LSTM model with two LSTM layers and two linear layers.\n",
    "\n",
    "        Parameters:\n",
    "            input_size (int): The number of input features per element in the sequence.\n",
    "            hidden_size (int): The number of features in the hidden state of the LSTMs.\n",
    "            vocab_size (int): The size of the vocabulary, defining the output dimension of the final linear layer.\n",
    "        \"\"\"\n",
    "        super(LSTMLyrics, self).__init__()\n",
    "        self.first_lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.second_lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.first_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.second_linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, input, hidden=None, return_state=False):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The input sequence to the model.\n",
    "            hidden (tuple, optional): The initial hidden state for the LSTM layers.\n",
    "            return_state (bool, optional): Whether to return the hidden states along with the model's output.\n",
    "\n",
    "        Returns:\n",
    "            Tensor or (Tensor, tuple): The output from the final linear layer, and optionally the hidden states.\n",
    "        \"\"\"\n",
    "        input = input.to(torch.float32)\n",
    "        input = input.to(device)\n",
    "        out, hidden = self.first_lstm(input, hidden)\n",
    "        out, hidden = self.second_lstm(out, hidden)\n",
    "        out = self.dropout(self.first_linear(out))\n",
    "        logits = self.second_linear(out)\n",
    "        if return_state:\n",
    "            return logits, hidden\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EarlyStopping Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopping\n",
    "\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 5\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.val_loss_min = val_loss\n",
    "            if self.verbose:\n",
    "                print(f'New best score ({val_loss:.6f}).')\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.val_loss_min = val_loss\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, validation_dataloader, input_size, hidden_size, epochs, writer, device, vocab_size, learning_rate, optimizer_name):\n",
    "    \"\"\"\n",
    "    Trains an LSTM model on provided training data and evaluates it on validation data.\n",
    "    \n",
    "    Parameters:\n",
    "        train_dataloader (DataLoader): DataLoader for the training dataset.\n",
    "        validation_dataloader (DataLoader): DataLoader for the validation dataset.\n",
    "        input_size (int): Number of input features per element in the sequence.\n",
    "        hidden_size (int): Number of features in the hidden state of the LSTM.\n",
    "        epochs (int): Number of epochs to train the model.\n",
    "        writer (SummaryWriter): TensorBoard writer for logging metrics.\n",
    "        device (torch.device): Device (CPU or GPU) to run the training on.\n",
    "        vocab_size (int): Size of the vocabulary, defining the output dimension of the final linear layer.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        optimizer_name (str): Name of the optimizer to use for training.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The trained LSTM model.\n",
    "    \"\"\"\n",
    "    train_loss, validation_loss = [], []\n",
    "    model = LSTMLyrics(input_size, hidden_size, vocab_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for input_lyrics, labels in train_dataloader:\n",
    "            lyrics = input_lyrics.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(lyrics)\n",
    "            loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "            writer.add_scalar('Loss/Training', loss.item(), epoch)\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for input_lyrics, labels in validation_dataloader:\n",
    "                lyrics = input_lyrics.to(device)\n",
    "                labels = labels.to(device)\n",
    "                logits = model(lyrics)\n",
    "                loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "                writer.add_scalar('Loss/Validation', loss.item(), epoch)\n",
    "                validation_loss.append(loss.item())\n",
    "\n",
    "        mean_train_loss = np.average(train_loss)\n",
    "        mean_validation_loss = np.average(validation_loss)\n",
    "        \n",
    "        print(f\"Training loss for epoch {epoch + 1}: {mean_train_loss}, Validation loss: {mean_validation_loss}\")\n",
    "        early_stopping(mean_validation_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        writer.add_scalar('Mean Loss/Training', mean_train_loss, epoch)\n",
    "        writer.add_scalar('Mean Loss/Validation', mean_validation_loss, epoch)\n",
    "        writer.add_scalars('Loss Comparison', {'Training': mean_train_loss, 'Validation': mean_validation_loss}, epoch)\n",
    "    end_time = time.time()\n",
    "    duraion = end_time - start_time\n",
    "    print(f\"Duration Time:{duraion}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, batch_size, method):\n",
    "    \"\"\"\n",
    "    Prepares a DataLoader for song data, which facilitates batch processing during model training or evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): A pandas DataFrame containing the song data.\n",
    "        batch_size (int): Number of data points to load per batch.\n",
    "        method (str): Specifies the method to extract MIDI features used in the SongDataset.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A DataLoader object that provides iterable over the dataset with specified batch size and shuffling.\n",
    "    \"\"\"\n",
    "    dataset = SongDataset(df, word2vec, vocab, './new_mid', method=method)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    return dataloader\n",
    "\n",
    "train_df, val_df = train_test_split(train_set, test_size=0.1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_1 = prepare_dataloader(train_df, 32, method='model1')\n",
    "val_dataloader_1 = prepare_dataloader(val_df, 32, method='model1')\n",
    "writer = SummaryWriter(log_dir='runs/first_approach')\n",
    "model = train_model(train_dataloader_1, val_dataloader_1, 300 + 113, 128, 25, writer, device, vocab_size, 0.001,'Adam')\n",
    "torch.save(model.state_dict(), 'model_method_1.pth')\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir runs/first_approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for epoch 1: 8.829691648483276, Validation loss: 8.770934104919434\n",
      "New best score (8.770934).\n",
      "Training loss for epoch 2: 8.79765510559082, Validation loss: 8.770934104919434\n",
      "Validation loss decreased (8.770934 --> 8.770934).\n",
      "Training loss for epoch 3: 8.740614891052246, Validation loss: 8.770934104919434\n",
      "Validation loss decreased (8.770934 --> 8.770934).\n",
      "Training loss for epoch 4: 8.415206106085526, Validation loss: 7.409964561462402\n",
      "Validation loss decreased (7.409965 --> 7.409965).\n",
      "Training loss for epoch 5: 7.816450004992277, Validation loss: 7.409964561462402\n",
      "Validation loss decreased (7.409965 --> 7.409965).\n",
      "Training loss for epoch 6: 7.816450004992277, Validation loss: 7.409964561462402\n",
      "Validation loss decreased (7.409965 --> 7.409965).\n",
      "Training loss for epoch 7: 7.510769605636597, Validation loss: 6.47917111714681\n",
      "Validation loss decreased (6.479171 --> 6.479171).\n",
      "Training loss for epoch 8: 7.510769605636597, Validation loss: 6.47917111714681\n",
      "Validation loss decreased (6.479171 --> 6.479171).\n",
      "Training loss for epoch 9: 7.510769605636597, Validation loss: 5.923556327819824\n",
      "Validation loss decreased (5.923556 --> 5.923556).\n",
      "Training loss for epoch 10: 7.260638351793642, Validation loss: 5.923556327819824\n",
      "Validation loss decreased (5.923556 --> 5.923556).\n",
      "Training loss for epoch 11: 6.535389589540886, Validation loss: 5.406398582458496\n",
      "Validation loss decreased (5.406399 --> 5.406399).\n",
      "Training loss for epoch 12: 6.435174654511845, Validation loss: 5.406398582458496\n",
      "Validation loss decreased (5.406399 --> 5.406399).\n",
      "Training loss for epoch 13: 6.116544585478933, Validation loss: 5.0457481145858765\n",
      "Validation loss decreased (5.045748 --> 5.045748).\n",
      "Training loss for epoch 14: 5.894111127388187, Validation loss: 4.695901359830584\n",
      "Validation loss decreased (4.695901 --> 4.695901).\n",
      "Training loss for epoch 15: 5.795800140925816, Validation loss: 4.434438019990921\n",
      "Validation loss decreased (4.434438 --> 4.434438).\n",
      "Training loss for epoch 16: 5.6785003609127465, Validation loss: 4.29125369919671\n",
      "Validation loss decreased (4.291254 --> 4.291254).\n",
      "Training loss for epoch 17: 5.558383510467854, Validation loss: 4.29125369919671\n",
      "Validation loss decreased (4.291254 --> 4.291254).\n",
      "Training loss for epoch 18: 4.983702839431116, Validation loss: 4.29125369919671\n",
      "Validation loss decreased (4.291254 --> 4.291254).\n",
      "Training loss for epoch 19: 4.765702644390847, Validation loss: 4.29125369919671\n",
      "Validation loss decreased (4.291254 --> 4.291254).\n",
      "Training loss for epoch 20: 4.725836078325908, Validation loss: 4.29125369919671\n",
      "Validation loss decreased (4.291254 --> 4.291254).\n",
      "Training loss for epoch 21: 4.707789415972574, Validation loss: 4.29125369919671\n",
      "Validation loss decreased (4.291254 --> 4.291254).\n",
      "Training loss for epoch 22: 4.599818531781027, Validation loss: 4.091617441177368\n",
      "Validation loss decreased (4.091617 --> 4.091617).\n",
      "Training loss for epoch 23: 4.568459051686364, Validation loss: 3.979937336661599\n",
      "Validation loss decreased (3.979937 --> 3.979937).\n",
      "Training loss for epoch 24: 4.386151285469532, Validation loss: 3.979937336661599\n",
      "Validation loss decreased (3.979937 --> 3.979937).\n",
      "Training loss for epoch 25: 4.3523176486899215, Validation loss: 3.979937336661599\n",
      "Validation loss decreased (3.979937 --> 3.979937).\n",
      "Duration Time:340.81893277168274\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_2 = prepare_dataloader(train_df, 32, method='model2')\n",
    "val_dataloader_2 = prepare_dataloader(val_df, 32, method='model2')\n",
    "\n",
    "writer = SummaryWriter(log_dir='runs/Approach2')\n",
    "\n",
    "model = train_model(train_dataloader_2, val_dataloader_2, 300 + 140, 128, 25, writer, device, vocab_size, 0.001,'Adam')\n",
    "torch.save(model.state_dict(), 'model_method_2.pth')\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir runs/Approach2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, input_size, hidden_size, vocab_size):\n",
    "    model = LSTMLyrics(input_size, hidden_size, vocab_size)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Song Function for Testing\n",
    "def generate_song(model, first_word, midi_f, seq, train_dataset):\n",
    "    output_sequence = []\n",
    "    song_words = [first_word]\n",
    "    hidden = None\n",
    "    input_sequence = seq\n",
    "    while len(output_sequence) < 200:\n",
    "        logits, hidden = model(input_sequence, hidden, return_state=True)\n",
    "\n",
    "        soft_tensor = torch.softmax(logits, dim=-1)\n",
    "        next_word_idx = torch.multinomial(soft_tensor, num_samples=1).item()\n",
    "\n",
    "        while next_word_idx == vocab_size - 1:  # To avoid padding index\n",
    "            next_word_idx = torch.multinomial(soft_tensor, num_samples=1).item()\n",
    "\n",
    "        song_words.append(train_dataset.index2word[next_word_idx])\n",
    "        word_vector = train_dataset.find_tokens_vec(train_dataset.index2word[next_word_idx])[0]\n",
    "        next_word = torch.cat([word_vector, midi_f], dim=1)\n",
    "        input_sequence = torch.tensor(next_word, dtype=torch.float, device=device)\n",
    "        hidden = tuple(h.detach() for h in hidden)  # Detach each hidden state\n",
    "        output_sequence.append(next_word)\n",
    "        if len(output_sequence) % 10 == 0:\n",
    "            input_sequence = seq\n",
    "            hidden = None\n",
    "\n",
    "    song = \"\\n\".join(\" \".join(song_words[i:i+5]) for i in range(0, len(song_words), 5))\n",
    "    return song\n",
    "\n",
    "\n",
    "    print(song)\n",
    "\n",
    "input_size_model1 = 300 + 113  \n",
    "input_size_model2 = 300 + 140  \n",
    "hidden_size = 128  \n",
    "\n",
    "first_model = load_model(\"/sise/home/gorelikk/ASS3/model_method_1.pth\", input_size_model1, hidden_size, vocab_size)\n",
    "second_model = load_model(\"/sise/home/gorelikk/ASS3/model_method_2.pth\", input_size_model2, hidden_size, vocab_size)\n",
    "\n",
    "# Create the Test DataLoader for both approaches\n",
    "test_dataset_model1 = SongDataset(test_set, word2vec, vocab, '/sise/home/liorkob/DL/midi_files/', method='model1')\n",
    "test_dataloader_model1 = DataLoader(test_dataset_model1, batch_size=1, shuffle=True)\n",
    "\n",
    "test_dataset_model2 = SongDataset(test_set, word2vec, vocab, '/sise/home/liorkob/DL/midi_files/', method='model2')\n",
    "test_dataloader_model2 = DataLoader(test_dataset_model2, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "# Prepare sequences and features for testing\n",
    "test_seq_model1 = [lyrics[:, 0, :] for lyrics, _ in test_dataloader_model1]\n",
    "test_seq_model2 = [lyrics[:, 0, :] for lyrics, _ in test_dataloader_model2]\n",
    "\n",
    "test_data = [(artist, song) for artist, song in zip(test_set['Artist'], test_set['Song_name'])]\n",
    "midi_features_model1, midi_features_model2 = [], []\n",
    "\n",
    "for artist, song_name in test_data:\n",
    "    midi_features_model1.append(test_dataset_model1.features_model1(artist, song_name).view(1, 113))\n",
    "    features_model2 = test_dataset_model2.features_model2(artist, song_name)\n",
    "    # Ensure features_model2 has the correct shape\n",
    "    if features_model2.size(0) != 140:\n",
    "        padded_features = torch.zeros(140, dtype=torch.float32)\n",
    "        length = min(features_model2.size(0), 140)\n",
    "        padded_features[:length] = features_model2[:length]\n",
    "        features_model2 = padded_features\n",
    "    midi_features_model2.append(features_model2.view(1, 140))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing effects of the initial word and melody on the generated lyrics:\n",
      "\n",
      "Analysis for strategy: random\n",
      "\n",
      "Song:  eternal flame by the bangles\n",
      "Model: 1\n",
      "Initial Word: barry\n",
      "Generated Lyrics:\n",
      "barry book shakingshaking kept chicken\n",
      "dixie kraft poor covered fit\n",
      "disgrace mortars denver rolls problems\n",
      "highland cloud bringing blackstreet christ\n",
      "assassin mostly pray sharing sears\n",
      "racing gained royce kidding bereft\n",
      "lonesome nasn wonders grown sunrisethis\n",
      "fables breathless shame schnee free\n",
      "bailamos exactly order win longlegged\n",
      "kept jokers gym drumbeat cookie\n",
      "competitive thousands copa better rulin\n",
      "miracle despise cheese point nurses\n",
      "citizens hee spitten whom learnin\n",
      "decision turning rocking em ronnie\n",
      "carols mystical responded breaking france\n",
      "plates no leatherface cops dudududuu\n",
      "country hesitations slapped scorn aiy\n",
      "dumm clownin karma cuerpo wooo\n",
      "commands jacket grain shave wei\n",
      "bes cell strap tucked nought\n",
      "cowards river klein ammonia thunder\n",
      "ness differences sein saw intent\n",
      "ones billy plant sherry spine\n",
      "tableaux aunt rocky delighted theaters\n",
      "room twotenth nineteenninety catchin cassette\n",
      "manic romantics mindin who sola\n",
      "springtime pot stereo fortunes relate\n",
      "none 9 curse know miser\n",
      "skam evilness continued trip dealin\n",
      "read stages bedop aunt cooks\n",
      "ransom gibberish teeth exit spine\n",
      "para cocacola uhoo rank constantly\n",
      "cover holidays drain souped lyrical\n",
      "guitar simpler wei valiums noone\n",
      "moonshine stare shed graveyards butterly\n",
      "coming thingthanks smooth du shadows\n",
      "off wandas hope watching successful\n",
      "scenery singers plant hand gypsum\n",
      "strung memphis gang perhaps actress\n",
      "phrases packin snortin cold schulden\n",
      "deeds\n",
      "\n",
      "\n",
      "Song:  honesty by billy joel\n",
      "Model: 1\n",
      "Initial Word: barry\n",
      "Generated Lyrics:\n",
      "barry watching pos notorious falter\n",
      "hall domino thick mortal round\n",
      "renewed broom wed meant squeezing\n",
      "hitter brakes oozin favourite unopened\n",
      "victory glisten rum preis uphill\n",
      "chain violence slippityslide trail id\n",
      "burst simon butterfly scars rude\n",
      "ham settles size golden fellow\n",
      "liebten crooked dukedom sat passages\n",
      "sunshine solo striving mcs adore\n",
      "regular moan lipstick eazy uhhuhhuhhuh\n",
      "guilty counseling beside charade sittin\n",
      "cosmic tame 380 slim collaborating\n",
      "ring reggae bankroll trophies thither\n",
      "nitro manner walkman translate claim\n",
      "fahrn mail weight bride nude\n",
      "somethings appetite nearest message molasses\n",
      "livings bogart righteous max wondered\n",
      "angel picking kite problem ripe\n",
      "acted explain enforce hellier sad\n",
      "alleyway greater uns powder trail\n",
      "shocked docs react degradin hesitations\n",
      "park fighting souljahz necklace tiembla\n",
      "tear see lesser domino endless\n",
      "brambles bridges leavin lurking maradyke\n",
      "strictly prayers mounted biz anybodys\n",
      "property tendency contender dozen nada\n",
      "awake frantic hypnotised noch collide\n",
      "school tonsils threw callin tarry\n",
      "mcs sailor parted paintings gangsta\n",
      "blaze ihren shorts nobodys balls\n",
      "craazy proves meters mee succeed\n",
      "flowin member yearn arm yuletide\n",
      "presence waters shoutin nursing stations\n",
      "gun fabulous goin fields finest\n",
      "psychedelic male cast ronnie bones\n",
      "choke today tha unhappy face\n",
      "forevers whoaohoh writer tryina meaning\n",
      "plant lap does lying kissin\n",
      "autumn steal gain hatte dry\n",
      "themes\n",
      "\n",
      "\n",
      "Song:  lovefool by cardigans\n",
      "Model: 1\n",
      "Initial Word: barry\n",
      "Generated Lyrics:\n",
      "barry givin scour shakes shalalala\n",
      "neighborhood twirl comet cleanin tickin\n",
      "rot creativity serious pose engines\n",
      "hendrixs querida fiery slapped wiff\n",
      "aboud someones sea jiggy waves\n",
      "deserve fittest ceasing kneel cape\n",
      "brand grieving hed heavy accidentally\n",
      "nail becomes twirl touch forces\n",
      "vienna sneakin streetz experiment rack\n",
      "ti delighted handshake thugz hex\n",
      "george hated hero bring cock\n",
      "cowards devil cobwebs dort shack\n",
      "rump whore zigzags rasta feelins\n",
      "stiff dres dodoublegs backs fooled\n",
      "happier sinatra aisle king delicate\n",
      "that nazis heartbreaker thy gettin\n",
      "flash hooo sitting goodbying began\n",
      "wan misled characters lonesome pedestrians\n",
      "workin cleared cheer hardest passing\n",
      "bow flushed southern compare ohso\n",
      "japanese comrades plate come still\n",
      "smiled til debo mortalise frosty\n",
      "who unreal bud blueprints ohso\n",
      "bottle flipside beach actors johnnys\n",
      "warriors antone sharp saffron okay\n",
      "foreign abandoned barrier sometimes begin\n",
      "lord sync could mean cages\n",
      "bond strange claiming thin particular\n",
      "nile pilots letting robas stamps\n",
      "kinds sticking purpose ole lump\n",
      "whoawhoawhoawhoa their shareef resistance prom\n",
      "fails forevermore grounded youve doc\n",
      "monroe cada mo pump echoing\n",
      "ts schau wail serve evilness\n",
      "trace heavens becoming temper restlessness\n",
      "hardearned everybodys wave soup accidentally\n",
      "tcb cute blamin opened everyting\n",
      "reaching gleaming master hmmm holds\n",
      "1990 s blocks heats hear\n",
      "greeting longer promises cuento slamming\n",
      "replies\n",
      "\n",
      "\n",
      "Song:  barbie girl by aqua\n",
      "Model: 1\n",
      "Initial Word: barry\n",
      "Generated Lyrics:\n",
      "barry roams awfully awful death\n",
      "brushes boast hurtin con mother\n",
      "downers video moor goosebumps shy\n",
      "upper another blade cannibalism little\n",
      "shore feet bitterness betta collaborating\n",
      "short loss june alegria jumping\n",
      "layin absolutely barking maison malts\n",
      "healed darling sheriffs dear downass\n",
      "universe trophies flag engineer yesterday\n",
      "dream shoot victorious local highways\n",
      "bekannt alien fantasize draw veins\n",
      "fellows nada youyou deceiving bend\n",
      "lumberjack recently wicked around fly\n",
      "gators barricade fighters seat type\n",
      "kept suitcases drank backs busta\n",
      "benson leatherface hateful person leads\n",
      "unfurl copacabana slipping religiosa marx\n",
      "proved bopa delaying caps hiphop\n",
      "smoke mask barrel mountain longer\n",
      "trail raised barrel thuggish boardwalk\n",
      "christmases gats crews ab bathroom\n",
      "kennt marched box conscience pager\n",
      "homework hoodlums tire pasan d\n",
      "daytime ooh viene x gathered\n",
      "cafe irons layer lawsuit isnt\n",
      "group traveling auditorium looked maids\n",
      "wa tainted nurse snatch sights\n",
      "bell rude relying steps daughter\n",
      "colors marilyn appear of maxine\n",
      "try ack auch alive bitchs\n",
      "senses about yonder millions point\n",
      "birds eloquence argue five od\n",
      "native jerry ceremonia suck stolen\n",
      "lait boogie action swayin chat\n",
      "jc symbol guard doing schau\n",
      "earlobe greener steps calling cuz\n",
      "blueprints fret unlike france verdict\n",
      "pull gunned solve pretending ruin\n",
      "guzzle vincent cymbals fuckin communication\n",
      "account nichts handshake reveal educated\n",
      "woah\n",
      "\n",
      "\n",
      "Song:  all the small things by blink 182\n",
      "Model: 1\n",
      "Initial Word: barry\n",
      "Generated Lyrics:\n",
      "barry gigolo closing meyers selling\n",
      "steadys fountain guiding drowning talked\n",
      "parks putting promises pills type\n",
      "static room motion makebelieve bows\n",
      "motherfucker this servant ammo lonely\n",
      "aerosol bruise liked swirling grandma\n",
      "fired care pacing regret landed\n",
      "spiritual check mop sirius bow\n",
      "roots hooligan trigger closest allentown\n",
      "funnin laying press ceremonia kneesll\n",
      "adventure neon unh rhymes wishin\n",
      "come itll denied correct broadway\n",
      "temp hourglass key nightmare jiggy\n",
      "did healing hmm israel constant\n",
      "lamb compares intention island shade\n",
      "delighted nation desolate ability stan\n",
      "wherever every am anyways max\n",
      "greasers def fast carly running\n",
      "jag hurries hulk echoes toe\n",
      "travel feats lawabiding familys accord\n",
      "closing bank else piercing hounds\n",
      "rainy knock tampa presidents chat\n",
      "masters gleaming exit neither loves\n",
      "lucky kani intact doing johnnys\n",
      "wok blessed drapes fishes thuggish\n",
      "lap deed rescued urge carried\n",
      "donut breaths trailer soul wake\n",
      "planet powers dream sexual partial\n",
      "duke demand toda collar haters\n",
      "barely mothers monie spinning lane\n",
      "coal food breaths good bitchs\n",
      "dudududuudu fellow jot met need\n",
      "cup choose typical fallback hand\n",
      "bring vd mostly sess beats\n",
      "complication dick steely stone africa\n",
      "parties cest incredible wont strangers\n",
      "least quickest staple fags sincerely\n",
      "phat bombs scuff jar without\n",
      "tap pacing deck slipping banging\n",
      "brace packing putting oil woowoo\n",
      "voodoo\n",
      "\n",
      "\n",
      "Song:  eternal flame by the bangles\n",
      "Model: 2\n",
      "Initial Word: barry\n",
      "Generated Lyrics:\n",
      "barry disappearing strangest confidence carnival\n",
      "myself defeated yever keen slavery\n",
      "borne sidewalk savings sees goodbying\n",
      "woman itll dying bluntz cest\n",
      "weary bop painful motherfuck few\n",
      "succeed stealing feathers ring farms\n",
      "mirrors returning hittin babe beard\n",
      "victory presidents lack chance airs\n",
      "abomb joe bop lone superstar\n",
      "staple direction oops system reasons\n",
      "thumbs brick adventure stalls race\n",
      "jack homies auch globe throw\n",
      "misunderstood language i livez hesitate\n",
      "spotlight fashion wooded berserk mondays\n",
      "honestly extreme target weight ja\n",
      "soulmates boss suspect touched happening\n",
      "government beating rulin spine yeh\n",
      "semester intergalactic mop dissed root\n",
      "suelo garrett urban missionaries bang\n",
      "inspire styles semester bless morning\n",
      "asap bustin cough rolling opposite\n",
      "hangup gurus populr prove disgraced\n",
      "quick everyday sisters undinal lows\n",
      "blind works whoopityayeoh photograph sigh\n",
      "really st messin carol lone\n",
      "delusions stroll riding lizards mockingbird\n",
      "der preachin der fahrn sag\n",
      "id cock con wake bond\n",
      "saturday eyez refugees karat liar\n",
      "skelter position rover hold around\n",
      "tryna pound singers jimmy drown\n",
      "eek soda crackle intact trues\n",
      "block longer never layin reicht\n",
      "demented delicate school passing automatically\n",
      "punches sis criminal trivial physically\n",
      "thugging whirlpool jedes para plays\n",
      "bones situation hunter holly hos\n",
      "wire helps eternally male made\n",
      "snitchass wild about zooped flopped\n",
      "cheeks railroad momomony phat contradiction\n",
      "manifest\n",
      "\n",
      "\n",
      "Song:  honesty by billy joel\n",
      "Model: 2\n",
      "Initial Word: barry\n",
      "Generated Lyrics:\n",
      "barry sincerity 1990 several jumping\n",
      "intervene apartment verstehe styling la\n",
      "guarantee haze ohsofrail puffin nothing\n",
      "spock nowits hey wicker dining\n",
      "remedy assassins echoed o cleanse\n",
      "hour mexican antone gleam lennon\n",
      "lifetime teaser rim scoff imaginary\n",
      "drugs siing sweat reach choke\n",
      "missing hating courtyard jag thought\n",
      "mate forgettable bringin wasted always\n",
      "vicinity crocodiles softer mtv aint\n",
      "nine phonies against greatest decir\n",
      "nearly tire slap 1916 r\n",
      "mmmm bugle tribe target users\n",
      "hardheaded stool jede wreckage heal\n",
      "stapler prize created rolls gitchi\n",
      "extraterrestrial talks design suckafree route\n",
      "sluts condom 2 possibilities songs\n",
      "torches motives hoo moss diaper\n",
      "shamari understanding tombstone fizz dismissal\n",
      "stacks pocket whipped curb moonshadow\n",
      "assail replace endeavor hoped cheek\n",
      "sqeezer lately undertaker canyons coupled\n",
      "theydve moms looking millions nashville\n",
      "sunglasses bunnies daddys shove spectators\n",
      "instante appreciate hated rendezvous trip\n",
      "britney snows sombrero ignoring duke\n",
      "pleading frameless skirts punker sloppy\n",
      "hands darlin correct peligro soar\n",
      "touches legend cowards makeup serious\n",
      "louder aim molesta hurts needing\n",
      "pleas dependencies hotter balloon blistering\n",
      "danced sells booze gossip might\n",
      "hardly invented yesterdays four waterfall\n",
      "spirit knowledge 1916 wandering creep\n",
      "minarets throttle pour sitting depends\n",
      "danforth screamed bold constantly blinded\n",
      "blank retrace usually isnt weeks\n",
      "stumm strays stand earned automatic\n",
      "walk drag doing test couldve\n",
      "explodes\n",
      "\n",
      "\n",
      "Song:  lovefool by cardigans\n",
      "Model: 2\n",
      "Initial Word: barry\n",
      "Generated Lyrics:\n",
      "barry work yesterday sounds papa\n",
      "tvfunk thorn parking vivid bangbang\n",
      "drapes goat widowed notion giant\n",
      "secretly believin kisses jumbo patch\n",
      "bride stopping shoots seine lorre\n",
      "rice milkmans mirage mommy close\n",
      "chauncey sidewalks bends reaction irenes\n",
      "birchmount statue saturday quitter laid\n",
      "pissed knowledge yukmouth passing charlotte\n",
      "canyons staying scribble populr report\n",
      "daisy shift mississippi capsize users\n",
      "louise pleasure stealin delightful porsche\n",
      "counter roads mens jealous think\n",
      "s permission rude dissolves blamin\n",
      "expired love rather revolutionary crawlin\n",
      "adrenaline glamor hasnt ability defeated\n",
      "woah berserk neck princess khan\n",
      "experiment fountain summit crawlin fucked\n",
      "equally trot id jade licks\n",
      "lift dans trumped ounce cooks\n",
      "harm warnin blame winds spit\n",
      "track 13 gang splinters famous\n",
      "claret loose brown sense abilene\n",
      "anothin roars lower oer believes\n",
      "gloves thumbs executed glasgow rash\n",
      "band doorway load hose n\n",
      "avenue earlobe close horizons powdered\n",
      "catches verse soapsud son souls\n",
      "brooks crocodiles alligator seal advancement\n",
      "gaining going joseph inspired answer\n",
      "slate smokes toughest kinderlied spricht\n",
      "jacked off dif grace insincerity\n",
      "remain deceived coastline family freaks\n",
      "caroling apologize laundry nibble confirmation\n",
      "thrown notes for disposable bittersweet\n",
      "ocean retrace jot ahh filling\n",
      "helter distance sappy playettes sagtbaby\n",
      "quicker interesting diamonds troubles shop\n",
      "wayyeah nicer rief hell reply\n",
      "harder nah sliced stuff anyways\n",
      "stage\n",
      "\n",
      "\n",
      "Song:  barbie girl by aqua\n",
      "Model: 2\n",
      "Initial Word: barry\n",
      "Generated Lyrics:\n",
      "barry bust figure merge handmedowns\n",
      "predict maria duke violent gusto\n",
      "woooooo memorize chambers trends boooooooooooooo\n",
      "whipped needin end prodded hometown\n",
      "aguilera ti em drinking page\n",
      "tus jock shorty tv prosperity\n",
      "separate merge driving ragged strap\n",
      "unkind undertaker inflate landscape parkin\n",
      "slipped sentir you extra fog\n",
      "both 6 nick planned bitin\n",
      "maintain amazed mistakes divides gay\n",
      "sendin wars if es strength\n",
      "responsible areawhy laying starter ignore\n",
      "exultation florida plains wa revolver\n",
      "wishes downers tomorrow sweater son\n",
      "twist knowledge sentenced eddy banging\n",
      "reallife pimpin preys te boast\n",
      "mission haste buck forte blueberry\n",
      "spirit careless threat sip lebenslust\n",
      "san comparing employ lock flesh\n",
      "technique cozy jockin takes el\n",
      "window e kahlua start sculpting\n",
      "dances organised kommissar circling behaved\n",
      "hollow turned egypt forgetting insincerity\n",
      "therell picked desto hell switch\n",
      "haze about wail use kuniva\n",
      "desert nineteenninetyfour milli ohohoh really\n",
      "lenya allisons backs zooped sights\n",
      "fork lack sock younger trusts\n",
      "abide butterfly tonsils militia lesson\n",
      "packed now hair finer replaced\n",
      "tarry endeavor busy daytime lawsuit\n",
      "statue tribe suffer childoh alone\n",
      "buttwasting busy flat creatures cada\n",
      "trailer twirl auditorium go pressures\n",
      "lifetime pos bogie hurry happen\n",
      "flatbed sieeign lebn sherbet orient\n",
      "regretting blackeyed manger fired pocket\n",
      "reap fought hoodlum deals backwards\n",
      "milli wondered arunnin twilight missed\n",
      "golets\n",
      "\n",
      "\n",
      "Song:  all the small things by blink 182\n",
      "Model: 2\n",
      "Initial Word: barry\n",
      "Generated Lyrics:\n",
      "barry about christmasing smilin held\n",
      "bows superlative flood shuttin beside\n",
      "bible fresh kray frust deserve\n",
      "soapsud vulnerability scared unafraid hendrixs\n",
      "benson arcade kim 4th rustles\n",
      "chance christmasing lesson crashing rude\n",
      "peaceful dja fronting alles suddenly\n",
      "mag di nameless grain breakup\n",
      "aisle god continue input licks\n",
      "eve suicide screwing dddo spring\n",
      "pillars their walkin trophies resolve\n",
      "getting fiji uncle nacht den\n",
      "mony ebert removed handmedowns work\n",
      "kitchen fantasy treat shorty fightin\n",
      "mud crackin frontiers porters deep\n",
      "fantasies tide tingling fifty fish\n",
      "pressed soapsud santa plants burned\n",
      "alone kinky oftentimes arrow twentynine\n",
      "big eroded futures should backs\n",
      "homies lebte angry arise plough\n",
      "label commands sergeants fists lil\n",
      "vertigo summoned blare dust experiment\n",
      "her advisory six married dim\n",
      "physically anyway switchin snatchin larger\n",
      "intentionooh crying winter shining marys\n",
      "unborn manhole target treadmill glide\n",
      "cuento checked filled different whoride\n",
      "welfare flying wastin widow amuse\n",
      "dealing kon played immediately enemy\n",
      "motherfucking fags hourglass barrio loving\n",
      "owned guest eargasms horses mann\n",
      "snow brians somehow rise whisper\n",
      "burden miedo flag drives shapes\n",
      "guantanamo growing chevy stopped away\n",
      "und ti key prey floods\n",
      "ringaringaring und fire moments physical\n",
      "photos active lang haunted finishedhe\n",
      "confirmation releasin hits rise hair\n",
      "echoed rub wept waiter conviction\n",
      "clams bent wiggy older window\n",
      "heaven\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Analysis for strategy: most_common\n",
      "\n",
      "Song:  eternal flame by the bangles\n",
      "Model: 1\n",
      "Initial Word: you\n",
      "Generated Lyrics:\n",
      "you at shiny sayin away\n",
      "images gunned pants youyou creep\n",
      "practice anything tu merge chuckie\n",
      "books makeup penetrator freak mend\n",
      "notion stern slaughter lone flatbed\n",
      "yoo hundert song skip rejoices\n",
      "fickle twelve electric sins hercules\n",
      "finding sock majestic sitting normal\n",
      "neon silhouette poets lawyer faggot\n",
      "cafe flashes impale honeymoon good\n",
      "ohohohohohoh shoots foods abused delighted\n",
      "house toward wrappin kamen amazing\n",
      "watchin unwinds sheds come eighteen\n",
      "marx mold present molten shitll\n",
      "dj year youth sins stresses\n",
      "says round overdose charmless turnin\n",
      "steal cribs attitude wasted flocks\n",
      "goat eminem thugs legal chalk\n",
      "von lab extraterrestrial shimmering unkind\n",
      "nick indeed sistah delightful clear\n",
      "strange yall ho lone blaze\n",
      "gunk parted tv ask candle\n",
      "tocas lawyer kurt further lingers\n",
      "woest bert ghost cooks gloom\n",
      "throwin ones breathed parted vain\n",
      "heartbeat listenin rice few shaking\n",
      "foreigners frown sittin cameras ourselves\n",
      "hardly enemy strait picked delusions\n",
      "popping heat flows there laundry\n",
      "disco extreme rump shapes hows\n",
      "bleedin within york luni surrender\n",
      "hazel this actin mellow basehead\n",
      "criticize han medicine manpower cherish\n",
      "eats timbaland daly traverse spect\n",
      "regular corn yakety woodwork disown\n",
      "kisses mil pleading rationale readers\n",
      "began leadingstill pleasure joke twenty\n",
      "bankroll later looking clack causin\n",
      "race treks fought illinois dull\n",
      "sale twain relax desperadowhy und\n",
      "thy\n",
      "\n",
      "\n",
      "Song:  honesty by billy joel\n",
      "Model: 1\n",
      "Initial Word: you\n",
      "Generated Lyrics:\n",
      "you preachin squeeze afraid edge\n",
      "eskimo strapped daly nada grandma\n",
      "mashed feast cuff dudududuu overload\n",
      "levee willplay timbaland floss egypt\n",
      "under career disappointments silhouette surrounding\n",
      "whine paper sole bay surrender\n",
      "hitch differences stock confidence con\n",
      "struggle paranoid skies foul powers\n",
      "big duchess che deally nananana\n",
      "sees moe loved haters prayin\n",
      "paranoid orders suspect marriage limousine\n",
      "troubles dreaming shoppers purple sliced\n",
      "twelfth cmon account empty sheltered\n",
      "fits strength slugs ye misunderstood\n",
      "platinum hypocrite pickin al trench\n",
      "jade ohohohohohoh killer restin fenders\n",
      "fulfilled controversy tail um youd\n",
      "pieces exercising massachusetts guessed bushes\n",
      "inspires conversation lank reflecting respond\n",
      "catacombs stressin theology prayed warped\n",
      "drowsy cleanse from collecting buffalo\n",
      "fenkel however eileens shinning arrange\n",
      "packin drivin leader round unkind\n",
      "deck whore defeat beats page\n",
      "exists poisoned thugz super molten\n",
      "friday pills prince recipe cutouts\n",
      "disappointments strings arrow normal rumblin\n",
      "everyones bags blunted ceremonia ten\n",
      "thousands stood issue quartet overdose\n",
      "slightly dre calls handsome winston\n",
      "comparing flavor guile vuelve bereft\n",
      "track nightmare mo working hazel\n",
      "three force panama task accelerate\n",
      "hophophop filled petals noon repeating\n",
      "bissau reaction daddy musica bushes\n",
      "souls push daughter marmalade given\n",
      "diggity dreamin galahad poppin paved\n",
      "sculpting laying initial vehicle deed\n",
      "sighs lifetime sentir luck immortality\n",
      "themselves diesel sweetest unite plants\n",
      "eah\n",
      "\n",
      "\n",
      "Song:  lovefool by cardigans\n",
      "Model: 1\n",
      "Initial Word: you\n",
      "Generated Lyrics:\n",
      "you gravity awoken boudoir ts\n",
      "when practiced liberty intention illusion\n",
      "be cynics peligro crawl posters\n",
      "stayin praises freaky be twohour\n",
      "think unopened vienna drew heavens\n",
      "crawls turtle strangers mortality grind\n",
      "decision lickin hoop whispers pastime\n",
      "clout annoy broads letters tourists\n",
      "likes asks was chillin flashbulbs\n",
      "dessert lantern rumblin sides bit\n",
      "crumbled sniff oldies intervene pretended\n",
      "entre despair dad conversation symphony\n",
      "lazy kitchen uhh homepiece motherfuck\n",
      "plant suppose trusted typical david\n",
      "reveal touches loc checking causin\n",
      "bent rush llevo flair plight\n",
      "answers refuge only delightful done\n",
      "employ euch mega mans reflection\n",
      "1 pacific nicer vital corners\n",
      "wake stacked whoopityayeoh k shit\n",
      "temper ei grandjust whey obvious\n",
      "ear thee frightful berserk roars\n",
      "plain fu joy quedarme funds\n",
      "noob simple soir giddy tina\n",
      "bolo morgan comen bumping sweeter\n",
      "years dishes schon lolhere overload\n",
      "consideration grandmothers playing cid ohoh\n",
      "rage blamin who paints grande\n",
      "might drum santa believing strings\n",
      "crumbled biz downtown water involved\n",
      "wanted sun shadys yoc medical\n",
      "butterfly swingers gladly lola design\n",
      "gangster stadium stirring raising john\n",
      "frust genie eagle cameras dress\n",
      "monroe brew stopped rellin spills\n",
      "versatile childhood lipstick es coal\n",
      "everybody rawkus season bitch peter\n",
      "punches inspection hoes bo sucked\n",
      "easy pieces behind say aways\n",
      "be likes became husband bathroom\n",
      "pipes\n",
      "\n",
      "\n",
      "Song:  barbie girl by aqua\n",
      "Model: 1\n",
      "Initial Word: you\n",
      "Generated Lyrics:\n",
      "you ignoring pillow last mammals\n",
      "starin mommy complain phil book\n",
      "coffee zu pumping pastime live\n",
      "candle pour citizens hailie learnt\n",
      "echoing creation eats fuego aware\n",
      "busy christmases sinners unknown nothings\n",
      "laced abstishit hear carlisle young\n",
      "left salvation disagree average nighty\n",
      "sailor asks yeahyeahyeahyeah shelter pipe\n",
      "nonono marys alternative tore lawyers\n",
      "revolving cleans freewheeling told forgot\n",
      "conmigo whoride styling pair amazed\n",
      "divides sombrero solitary wird graces\n",
      "opposite longhaired 64 shoutin sour\n",
      "hereafter reprobate lu bowelshaking our\n",
      "remedy circles b lurkin weed\n",
      "lennon broads diez struggling border\n",
      "da sea danced stumble blaze\n",
      "patch greet hates assure gangsta\n",
      "instrumental cares virgin paints trees\n",
      "foe tammed protest kickin bronco\n",
      "and doctor conmigo queer stop\n",
      "crime solo winds decided miss\n",
      "bane grace ragged untouched shakingshaking\n",
      "snuff spends egyptian soot cancer\n",
      "learned closest lend jock shadys\n",
      "1990 layer contender rosary difference\n",
      "beats finest killas big expires\n",
      "temple round popular raps trends\n",
      "while rye communication cleans dining\n",
      "thingthanks yourself leaving makebelieve ebony\n",
      "television may alamo pullin motherfuckers\n",
      "woooooo heute highest message tuesday\n",
      "shorts clips tengo sisters feelins\n",
      "regular singers pullman avalon full\n",
      "decorate quieres rumored 1990 threat\n",
      "ray meet tanks dim kind\n",
      "grind auch sneakin babylon amadeus\n",
      "hellier jasmine curricular shiny dududuudududud\n",
      "patient impale deprived sammy brother\n",
      "lands\n",
      "\n",
      "\n",
      "Song:  all the small things by blink 182\n",
      "Model: 1\n",
      "Initial Word: you\n",
      "Generated Lyrics:\n",
      "you slacking beauty been amuse\n",
      "cymbals regret noon haggard halfway\n",
      "baths parts learn sparkling drawn\n",
      "pointing raven fucked tainted refuge\n",
      "un naughtynaughty mainstream clams smiled\n",
      "shorts train tower factors bucket\n",
      "camino itd copy diga steal\n",
      "wade thugging slash nix flying\n",
      "off boudoir weak sinking thy\n",
      "moonshadow bump rushes miracle confusion\n",
      "sending insincere restless sqeezer low\n",
      "ford raised thoughts spent yall\n",
      "older youve itdown rhinestone contracts\n",
      "army wet uninvited minor gas\n",
      "trying fill divine rot nightmare\n",
      "phatness yknow paintings logs flopped\n",
      "holds revolving raye autograph x2\n",
      "nur cash begging haunted ascapped\n",
      "silver want dying filled collab\n",
      "ventana devotion commands children hurtin\n",
      "whips clay bail spare farewell\n",
      "curves spin torn whoh sendin\n",
      "hahaha 1990 norma happier avec\n",
      "sheet stevens breathing laser from\n",
      "their confess lack sherry renee\n",
      "rhode intergalactic dyin song red\n",
      "border ringaringaring candy abut fiends\n",
      "back main realize bolo dream\n",
      "dart lauryn bolo sails twilight\n",
      "ain given times counted floss\n",
      "less bo ahuggin lock art\n",
      "are boughs diggy hourglass piano\n",
      "querida shirt garrett stops bob\n",
      "hustler klein wail hundert 1951\n",
      "motherfuckin farewell punching ignorant daa\n",
      "demented zipped angry cowards treasures\n",
      "wicked screamed loc picking expedition\n",
      "mouths dial miedo ending shook\n",
      "deacon accusation hazel equality incredible\n",
      "geht prisoner bond fhrt whirlpool\n",
      "anthrax\n",
      "\n",
      "\n",
      "Song:  eternal flame by the bangles\n",
      "Model: 2\n",
      "Initial Word: you\n",
      "Generated Lyrics:\n",
      "you x scandals instruments neighborhood\n",
      "owned tumble cheated gaffle jumped\n",
      "pops downtown while slow lived\n",
      "tremble gears longhorn start gotta\n",
      "murderous wigs ho grieve timbaland\n",
      "label rot halls sent insecure\n",
      "highways mo mesmerized smile ghetto\n",
      "slams chameleons minus unconscious sangre\n",
      "covered empire hoped breaks oughta\n",
      "popped identical flippin rule nicole\n",
      "numb debo vulnerability actor ihr\n",
      "deity haunted r rhymes flickering\n",
      "faint using pillow fleet righteous\n",
      "diga disconcerting louder from trusting\n",
      "classes worry scenes halfblind safe\n",
      "siskel x6 hundert lieber talking\n",
      "flame cuz shores learns left\n",
      "stale empty break fillin family\n",
      "cute native cheating aboard pot\n",
      "rednose kicked mange september mmmm\n",
      "nubs shut brighter usa flex\n",
      "accused meadow pregnant e saffrons\n",
      "fighting rolling sleigh georgia slittin\n",
      "backwards dry grabbed cards video\n",
      "rich broncin earths wanting kani\n",
      "pleasin passes hay compliment triumphant\n",
      "difference um illusions achanged glistening\n",
      "suckafree whine assassins fuckin tapes\n",
      "idiots adieu responsible aways arent\n",
      "ps splinters slamming arrange remained\n",
      "boom emerged surround lifeline nsync\n",
      "che values twen iron brooklyn\n",
      "did very presidents scientologists saint\n",
      "growth furniture cuts faces employ\n",
      "grocery filling settin chose rushing\n",
      "difference looking hope cry joy\n",
      "thrilling been eastwood ahelpin nearer\n",
      "gave truth permission grease says\n",
      "manner finally trends treating ive\n",
      "gum sideways dear create works\n",
      "sweettalkin\n",
      "\n",
      "\n",
      "Song:  honesty by billy joel\n",
      "Model: 2\n",
      "Initial Word: you\n",
      "Generated Lyrics:\n",
      "you seem suck factories fellows\n",
      "asshole lovin manger caps motherfuckers\n",
      "cutie crosses zur hip himself\n",
      "kisses intergalactic abstract permission mockingbird\n",
      "trivial crack shut socked opened\n",
      "woah nina impossible shouldnt tha\n",
      "mirrors stroked marx operate stops\n",
      "lined rings bomb carnation greedy\n",
      "releasin wok said grabs forgetting\n",
      "toast fan healed clever wooded\n",
      "sonny professional beware slavery odyssey\n",
      "glory fighters poke manpower cast\n",
      "courage stays rimes aint york\n",
      "calling eighth bazaar splits babys\n",
      "crookedness familys liberty woest martha\n",
      "hold public dark detention awith\n",
      "fria either gathering asleep description\n",
      "carried explained sky tender troubles\n",
      "war read lark wait opening\n",
      "thangs everlastin forwards endearing dodge\n",
      "1969 landing han king fulfilled\n",
      "rhyming pale badge starry count\n",
      "gates station successful poisoned danforth\n",
      "murda free book joint expired\n",
      "sailing flood flexing streetz lebte\n",
      "concert sorrow uhh thoughts bouncebouncin\n",
      "lahaina coast waterfall sola cares\n",
      "landing woowoo moonlight weave skip\n",
      "mighty someone craving doubt flags\n",
      "clout gentleness saffron bones sweetly\n",
      "create towns stir each louie\n",
      "accept happily bother equip report\n",
      "guevara uptown mondays lest rabbi\n",
      "siebzehn dancing peter ahah certain\n",
      "ten understood checked knock melt\n",
      "fully sign woooooo smacked cradel\n",
      "average tienes daytime japan nobody\n",
      "dependencies proved blink passes chance\n",
      "intimate sails favourite blanket slit\n",
      "together swore tang unbelievable writer\n",
      "tom\n",
      "\n",
      "\n",
      "Song:  lovefool by cardigans\n",
      "Model: 2\n",
      "Initial Word: you\n",
      "Generated Lyrics:\n",
      "you suffered cursed stuck drought\n",
      "quiero heute ringin refuge provide\n",
      "diggy deliverance token niggaz nowits\n",
      "vienna bedrooms searching cripin certain\n",
      "che honeymoon sunset hookah flowin\n",
      "macheath shove piss honor betta\n",
      "vienna trial talked carnival moments\n",
      "fucked childoh tvfunk heard joy\n",
      "aah indecisions tumble bath factors\n",
      "shivers gangsta wrapped stereo strapped\n",
      "memorize renee mortars fullest ally\n",
      "people han giant ul nashville\n",
      "suelo owens flames insecure pushed\n",
      "unnecessary smeared doy tooth innuendo\n",
      "facing chorusx2 season refined shaqs\n",
      "trivial two chasing plough auf\n",
      "busted preventing solely eating haunt\n",
      "boohoohoo weazy aguilera fundementally yell\n",
      "told cabbage daytime starting flash\n",
      "starter similar collects adrift sin\n",
      "worry function moonlight shalalala rapophile\n",
      "forget breal treks rid frying\n",
      "facts lebn return seem prizefighter\n",
      "aguilera pearls wealthy fry teacher\n",
      "beauty holler sellin origami swing\n",
      "tender llevo crops changed discovered\n",
      "obscure fiend whoaohohohoh hurl formulate\n",
      "forwards bend slave echoes chainedup\n",
      "maison frightened keep scold expert\n",
      "bleak resist live lots 4th\n",
      "factors walking flopped latest torso\n",
      "jumped flower beat sola thugging\n",
      "murderous honey ups pursue ich\n",
      "ether plans beyond laughter meaning\n",
      "known satin luck desert popping\n",
      "cest ness second thanks nicole\n",
      "pullman bleeding arunnin detention hottest\n",
      "bracelets teaser verdict treks honies\n",
      "steeple work confess rimes extra\n",
      "basehead sorrows chrome suitcases fu\n",
      "contender\n",
      "\n",
      "\n",
      "Song:  barbie girl by aqua\n",
      "Model: 2\n",
      "Initial Word: you\n",
      "Generated Lyrics:\n",
      "you nicer sammy guarantee appealing\n",
      "giving react disguising fooling hairy\n",
      "people sheep sudden wooo slavery\n",
      "cmon coma recognize steed conductors\n",
      "popularity tear haha young broom\n",
      "fit hellier bartender younger bartender\n",
      "nimble rescue astray lupe oldfashioned\n",
      "umm zombie thugs flow unglued\n",
      "aint sewed mo sagt collectin\n",
      "babylon eyebrow monkey polish leavin\n",
      "guantanamo thorns trial fish pretended\n",
      "replies happy miedo startin ancient\n",
      "soothed unimportance charmless yeah glide\n",
      "daytime cebu lion phone lower\n",
      "stopped howd acruisin uninvited frame\n",
      "proud flunk built looks regardless\n",
      "whrenddessen decisions syne pleasures actors\n",
      "care flors brush mid80s numbness\n",
      "warmth theres score stacked means\n",
      "baptized phatness eh stranger obscure\n",
      "illusions scold lose homes cripin\n",
      "hardcore bottomline imaginary top evry\n",
      "voyage firmament rief feats castle\n",
      "vibration preis eins cemetery ohso\n",
      "bushes buyin error der mable\n",
      "waitress say sluts fefe ceiling\n",
      "curtain rapper everlastin chauffeured lamps\n",
      "hide writer fuckin actually sink\n",
      "kills underground anthems mountains greatest\n",
      "side coz bouncing royce pearls\n",
      "boogie easy unspoken covered degenerate\n",
      "snortin dub witch fleet daddyo\n",
      "k sands ashamed tapestry endlessly\n",
      "honor continue wait cliff barrio\n",
      "bill youd archie endure revealed\n",
      "empty portion idol dab jock\n",
      "bethlehems rot fourteen midst areawhy\n",
      "big group quickly rhode lust\n",
      "facts ah ignored gon unfolding\n",
      "fefe dudes kind hits liquor\n",
      "tingling\n",
      "\n",
      "\n",
      "Song:  all the small things by blink 182\n",
      "Model: 2\n",
      "Initial Word: you\n",
      "Generated Lyrics:\n",
      "you day forward groove pills\n",
      "spilled hush eargasms berlin carefully\n",
      "tan thumb ohso odds dixie\n",
      "crowds shines pacing molten opening\n",
      "birds digest pervert dudududududududududuududududududududuud bends\n",
      "exempt cupid lightnin nighttime rest\n",
      "righteous moose yesterdays jokes lo\n",
      "someone becoming sammy silencio haunted\n",
      "beg gab jealous boughs til\n",
      "gone aunt bottle role rawkus\n",
      "homies thief sock roost woest\n",
      "thing july oohooh well shield\n",
      "gloves waiting personality planning dream\n",
      "coladas asured myself sit dwell\n",
      "murdered firm curse earlobe cleans\n",
      "started among cared rush wore\n",
      "preis chariot strife billboard unimportance\n",
      "relive running stereo gunned tank\n",
      "problems followup bath chess disaster\n",
      "donde leone glasgow tell illusions\n",
      "watched buster floss wonders hey\n",
      "transmitted jeep bride aire motherfucking\n",
      "set sunbeam wave athinking ace\n",
      "peligro bailamos 4th hot twilight\n",
      "killers apologize proof prove face\n",
      "erase torso pillar tugboat ringtingtingling\n",
      "fade resides dudududuu mmmhmm slippin\n",
      "seems crack spaces trespassin chameleon\n",
      "ayin shark jock tugboat empire\n",
      "saffron drop divides linen lifeline\n",
      "flippin slate since apple booze\n",
      "peas mary kissing abhors boss\n",
      "achanged prisoners desperado this kings\n",
      "cupid often click garden rite\n",
      "surfaced victorious plan molesta dabei\n",
      "ships owned guitarpicking cannot minds\n",
      "abilene uso realized plan naked\n",
      "healer balzac simone fields socked\n",
      "acruisin where phone criticize rump\n",
      "stray pushing ohso ait gathered\n",
      "stayed\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Analysis for strategy: least_common\n",
      "\n",
      "Song:  eternal flame by the bangles\n",
      "Model: 1\n",
      "Initial Word: cabbage\n",
      "Generated Lyrics:\n",
      "cabbage kinds large grandmothers pages\n",
      "liking rednosed quiero season drag\n",
      "knocks felt stares ts cheese\n",
      "jag frankincense gators device remembering\n",
      "midst add haste hounded saints\n",
      "petals castles sons seduce hawk\n",
      "gang baths nevertheless unite sang\n",
      "pequeno prendido maps yourself does\n",
      "occupy belong bowelshaking ihr soaring\n",
      "snuff allisons ragged suffered highway\n",
      "prey coucher pd homefront hookah\n",
      "shadows responded crashing stars bullshit\n",
      "glee transmitted range bothers shitfilled\n",
      "sweeter pa kinds says times\n",
      "passage therefore whoaoh rumors sound\n",
      "saffrons various vita motherfucking overdrive\n",
      "went dashing fiesta motherfuckin sherm\n",
      "witchs copacabana le blackeyed dart\n",
      "stages antone nor hath rhyme\n",
      "nude victrola devoted immediately odyssey\n",
      "zur foreign boy womb tar\n",
      "dusta platinum callar clue mirrors\n",
      "stroked joyous ow fault sacred\n",
      "boomshake needs herr thumping winner\n",
      "lame toast sacred moonshine trues\n",
      "quickest boots dazzling sushi pearls\n",
      "waited quiet thugz fulfilled dadoronron\n",
      "truly tar crumbled paranoid evils\n",
      "warum bes kaempferts two jacket\n",
      "wallet intact souljahz bopa imitating\n",
      "stitch frank leads already shall\n",
      "mistakin start sorrows warriors photos\n",
      "physically intrigue verdict messin afraid\n",
      "audience certain close whispered know\n",
      "gro clique dreamless silencio paying\n",
      "mistery visit combing token engineers\n",
      "praise lewis haa favorite hangup\n",
      "admit hot sunshine history tat\n",
      "treffen conceal conspire stepping marching\n",
      "girlfriend cats wish living tang\n",
      "static\n",
      "\n",
      "\n",
      "Song:  honesty by billy joel\n",
      "Model: 1\n",
      "Initial Word: cabbage\n",
      "Generated Lyrics:\n",
      "cabbage eah hackensack abilene pasan\n",
      "gibberish physically divorce trail willing\n",
      "planes boardwalk skis media sean\n",
      "slipping stares ford tits royce\n",
      "grab coz raye thatthathatthat crowds\n",
      "feeeeling sayer dif style masters\n",
      "anywhere behave farm distant recipe\n",
      "nuts sundoobiest phones groceries suspect\n",
      "martin bates cosa flushed combination\n",
      "digest towel obey surprise staind\n",
      "serious banging melodic chop permission\n",
      "man reason watchin native fair\n",
      "ron haunting fitness brianbsb instead\n",
      "de fed achanged fate knows\n",
      "shooting became balled talent death\n",
      "button nearest loved 4 erupt\n",
      "lavas heedless deceased strung seeing\n",
      "woohoo lamb amazing weird proof\n",
      "skirts jasmine rapper ashamed rank\n",
      "spring luft practice offered leaf\n",
      "remains island dough ignore naturally\n",
      "knife fragrant rover trains paying\n",
      "drums plays dearly continued charade\n",
      "shapes pony expert sheets players\n",
      "bough robas wait cannibals repeat\n",
      "roughness slick passage rally conductors\n",
      "fiends keeps kani perhaps boat\n",
      "phil everlasting am laundromat prove\n",
      "tvfunk carefree creatures pistol undertaker\n",
      "stands wrote bird resucito scientologists\n",
      "male creature shapes lookin lose\n",
      "marx eight false flags fence\n",
      "folks instantly teachers disappeared den\n",
      "whoever sum motherfuckers bar course\n",
      "present freight diddley recently deck\n",
      "dawgs limos rag bothers sigh\n",
      "three suckafree holding piercing allentown\n",
      "charm advice fifteen baghdad fan\n",
      "repeatx2 mystery kids rhyming shouldve\n",
      "incredible seems elses cheese hell\n",
      "wheels\n",
      "\n",
      "\n",
      "Song:  lovefool by cardigans\n",
      "Model: 1\n",
      "Initial Word: cabbage\n",
      "Generated Lyrics:\n",
      "cabbage bouncebouncin expect as avalon\n",
      "headed route kani groceries biz\n",
      "envy 53 chalk sets consideration\n",
      "pick uhh pinching splendor breaker\n",
      "rabbi bodies crooked tow questions\n",
      "longhaired ruby preicous blessings releasin\n",
      "dans romances flors ended watercolor\n",
      "locd tidings cocky pickin feet\n",
      "surgeons public ceremonia slop destruction\n",
      "limos happiness mortars emptiness trusted\n",
      "died kojak kiss bruise twisted\n",
      "masquerade stumbling shine favorite team\n",
      "butterly undertaker leather bout havin\n",
      "manger promise romantic greet guitar\n",
      "accent yup disappeared corpse pining\n",
      "throne trailer absent spitten beeell\n",
      "sleepy cigarettes nashville stay jill\n",
      "firmament freaky keeping trouble hawk\n",
      "reluctantly wardrobe polluted kiddies only\n",
      "desperately pulled eighty kissing hell\n",
      "legitimate innocent wallet bouncin yule\n",
      "mover miami plates lebte broom\n",
      "hardearned mo news seasons pack\n",
      "lives se style alternative baba\n",
      "havin guantanamo dialed lived leavin\n",
      "athletic eventually tragedy caroling kitchen\n",
      "redeemed flashbulbs lucky suddenly oleary\n",
      "sleepy fills dro sea week\n",
      "cal followup depends 14 aboard\n",
      "bodys sis oklahoma hittin antichrist\n",
      "phased adrenaline quiet ninetythree packin\n",
      "noel getting whom overload queer\n",
      "ghosts contract gegen clock boast\n",
      "wailed coupled wigga lawyer arrival\n",
      "45 twenty net gotta kray\n",
      "ting hooray view wrongs are\n",
      "awhile flags anguish baths dadoronron\n",
      "listning keeps notice kahlua nasty\n",
      "oops distortion aww illusion lick\n",
      "cheaters jergens appeared moans clubs\n",
      "survives\n",
      "\n",
      "\n",
      "Song:  barbie girl by aqua\n",
      "Model: 1\n",
      "Initial Word: cabbage\n",
      "Generated Lyrics:\n",
      "cabbage memries blast daily oldfashioned\n",
      "population crazy broomstick downtown theres\n",
      "rid holiday school finds eagle\n",
      "aha driving seine nurse industry\n",
      "amor triangle america crutch desert\n",
      "weaving waits burden eight tale\n",
      "behold bounds spreadin holidays lawyers\n",
      "publishin needle clack molasses eah\n",
      "pupils desert wanna yellow cribs\n",
      "chariot spring flavor leon counts\n",
      "decide our egyptian weit tranquility\n",
      "breeze controlled milli carpet task\n",
      "far special log scattered degradin\n",
      "fought this drawer compulsive exists\n",
      "cake abortion boss homie atheist\n",
      "banging responses evergreens cartier proceeding\n",
      "leatherface odds rabies soulful murderer\n",
      "comes booty crawls dessert moan\n",
      "red accord sunglasses log beads\n",
      "snuff wan relate burns jazz\n",
      "ended sane neglected hoppin retard\n",
      "later on plates seize chasing\n",
      "fine roller quickest sqeezer yesterdays\n",
      "sesame go dreamworld twenty shook\n",
      "vienna majestic yo writes id\n",
      "theaters wrong wrap berserk breast\n",
      "mood lifes overtime curls accepts\n",
      "single rumors lightning jasmine cheek\n",
      "baghdad frankincense feels corvette tellin\n",
      "noche sheltered talked justice dust\n",
      "i decent triumphant grandjust breaker\n",
      "preventing rush hoes fightings cages\n",
      "automatic clap r aways transmitted\n",
      "moist thumping whore built psychedelic\n",
      "trusting wire veins dunk uhuh\n",
      "glimmer pushing racin sits prizefighter\n",
      "written skam death getting chauncey\n",
      "mark marys jaws jfk decency\n",
      "snowbird gravity bulletproof flunk two\n",
      "playettes glitter boulevard boom dawg\n",
      "justify\n",
      "\n",
      "\n",
      "Song:  all the small things by blink 182\n",
      "Model: 1\n",
      "Initial Word: cabbage\n",
      "Generated Lyrics:\n",
      "cabbage waste mejor tina darken\n",
      "nothing startin believe especially waits\n",
      "tamikra ii shows faking bloody\n",
      "til lil vuelve wrongs my\n",
      "for rip american thick murdered\n",
      "wrinkled phat breakup viagra refresh\n",
      "mold regretting dip sleeved definitely\n",
      "boast everyday fade montreal hopefully\n",
      "familys inch enter cooly 20\n",
      "gone thighs uninspired glitter ginger\n",
      "iqs chambers homework yoo anothers\n",
      "probably wonders notices painful healed\n",
      "feint disgrace wears bugging beatles\n",
      "eastern silencio iiiii slouch enthrone\n",
      "watched started bless cried blame\n",
      "executed attraction bedford peeps retard\n",
      "woooooo waist so slowin nor\n",
      "smile pays dhc entered highest\n",
      "twice tavern testify ah tryna\n",
      "okay malcolm whens coral fist\n",
      "monstrous where lovers things peel\n",
      "begging woher church worker initial\n",
      "youre heartbeat ups involved starts\n",
      "taking feeeeling regular liked wake\n",
      "trues naughtynaughty gloria duck dried\n",
      "dispute rodgers ooh breakdown fore\n",
      "touches sings fix explode net\n",
      "songs klar nada sex slob\n",
      "na rejoices aiy technique fearing\n",
      "oz why marketplace photographs revolver\n",
      "computers brighter wade dave turtle\n",
      "pinch mammals dazwischen delight oouh\n",
      "albums honestly opaque collecting price\n",
      "65 hackensack value shone skill\n",
      "fashioned flood bizarre suddenly temp\n",
      "replies dancer thrill deny pressin\n",
      "paint tool degradin complete freak\n",
      "preacher ability idiots walkin sure\n",
      "em grown stoppin daily cheeks\n",
      "stash honesty boots led fourteens\n",
      "spell\n",
      "\n",
      "\n",
      "Song:  eternal flame by the bangles\n",
      "Model: 2\n",
      "Initial Word: cabbage\n",
      "Generated Lyrics:\n",
      "cabbage bake happier judges geht\n",
      "fist carols gently parts teaseteasetease\n",
      "steeple quite slow kamen artists\n",
      "buys brainiac then slop di\n",
      "cover brainiac rides porters flip\n",
      "justice fools copacabanalike diga hago\n",
      "worrying knife ich graveyards gum\n",
      "fields bitterness mid80s placed chain\n",
      "competitive tied sat wide instruments\n",
      "also kneel minds waves silent\n",
      "alleyway tall mistery chess brainiac\n",
      "jimmy drowns jc lines furious\n",
      "filling manifest jitterbug reaction dort\n",
      "prancer kankakee yearn fantasy vexes\n",
      "hated feelin dispute mist friendship\n",
      "slut eyesight burnin misfortune pages\n",
      "gall cantcha song everywhere lawyers\n",
      "rely raye architect being cellophane\n",
      "science jumped disgraced springtime hats\n",
      "come sunday tightly waterbed ers\n",
      "johnny maria refrain particular escorted\n",
      "anne rubs melting hath fiji\n",
      "tempted seagal snow woah auch\n",
      "cynic loses digest doesnt several\n",
      "gotten chinatown blade fortune eee\n",
      "moved colitas york suffocates analyst\n",
      "simone ugh ya games hm\n",
      "swears guarantee alibis mega eatin\n",
      "reflecting 59 jean besar verbs\n",
      "bone wealthy chew strived understood\n",
      "tits smoking shoulders wondrous actually\n",
      "traveling ignorant fallback horse skis\n",
      "scent dipped tackled spirits boomshake\n",
      "tenderness bringin brew dashing attracting\n",
      "temples hero reap helter twohour\n",
      "licks parson popularity true tame\n",
      "ever takin penetrator wintry meat\n",
      "thathatthatthat dudududududududududuududududududududuud family cheaters vestirme\n",
      "trash sentir inch cannibals lightnin\n",
      "long banana preaching dadoronron inviting\n",
      "riders\n",
      "\n",
      "\n",
      "Song:  honesty by billy joel\n",
      "Model: 2\n",
      "Initial Word: cabbage\n",
      "Generated Lyrics:\n",
      "cabbage glamor is gloom radiates\n",
      "hoped stealin exposure bustin accidentally\n",
      "shows sammy charge shrimp wailed\n",
      "asap believing baa mousetrap lane\n",
      "mortars proceeding se sean dearly\n",
      "expired pretty perfect clearly brotherman\n",
      "meaning necklace dreams recognized understanding\n",
      "pervert log blocks neighbor callar\n",
      "highways tiembla clothing dreamworld thatll\n",
      "selection zipped summers apart outside\n",
      "fever beastie covered wants walin\n",
      "pickin comparing missing rocking noise\n",
      "widowed army militia strife acting\n",
      "grabbing shade telephone fro cest\n",
      "totaled diseased attemptin ruff chromium\n",
      "tire agree sagt stuck cobain\n",
      "information twentythree sea stolen believing\n",
      "cahoots whats krunk instante closes\n",
      "raw saddle cobwebs porno save\n",
      "futures march spose reed girlill\n",
      "doorstep merece grows troubled plowshares\n",
      "britney cuts ourselves crawled key\n",
      "rob construction stinking bringin swiss\n",
      "seemed lessons mud fungirls strip\n",
      "disconcerting mm critics grabs rape\n",
      "disown lobby mc garbage lack\n",
      "monaco leaders yeahyeahyeahyeah mounted seek\n",
      "rid naturally staple aquaman ask\n",
      "wasting wigga pedestrians butterfly craazy\n",
      "shouldnt win pocket travel tugboat\n",
      "consider erzhl noisy makes caught\n",
      "maids dee bouncin behave delightful\n",
      "baba stale booming combination restaurant\n",
      "ooh died treadmill landed jerry\n",
      "scruff spreadin tame gurus towns\n",
      "his guzzle nameless siento responsible\n",
      "fairy gunned tool handsome shovel\n",
      "bumpin instrumental pinch garrett unconscious\n",
      "lambs picked sincerity star apply\n",
      "feather tragic pacing disconcerting careless\n",
      "chauncey\n",
      "\n",
      "\n",
      "Song:  lovefool by cardigans\n",
      "Model: 2\n",
      "Initial Word: cabbage\n",
      "Generated Lyrics:\n",
      "cabbage survives screamin my dudududududuud\n",
      "raining takin moffats intrigue talked\n",
      "ring filling bowie attracting sideways\n",
      "moms empty blinkered circus boody\n",
      "giving dreamer tuned muezzin clone\n",
      "lovely falter thought woher mtv\n",
      "pleasure alight sad jitterbuggin strut\n",
      "fully tickin mens scripturez deliverance\n",
      "protect target eastwood hardheaded sale\n",
      "from blastin youth irons surgeons\n",
      "reap basehead yesterdays chi threat\n",
      "papa fathers wrists youth ross\n",
      "growin boardwalk okay toughguy gravity\n",
      "vulnerability hotter frank couples sonny\n",
      "grabbing disconcerting divine japanese violet\n",
      "walla teenage spray trouppper rhymes\n",
      "strutting isnt donut blitz moves\n",
      "test common seinen jfk memories\n",
      "spiritual rumblin vows mi homies\n",
      "overdose pressed punkass kennt toddlers\n",
      "reviled misery brightly bruder stilton\n",
      "afternoon degrees chemistry mysterious pigeon\n",
      "swords outrageous thundering sheds groove\n",
      "malibu seated tus pregnant exposure\n",
      "needin basis whole didnt devote\n",
      "trance soul please oop playin\n",
      "sunbeam sorrow tapped lostandfound december\n",
      "oooh reaction heroes voulezvous tempt\n",
      "angels mist repeats diss handsome\n",
      "petrified notheres impale laced gloom\n",
      "siebzehn rich shop claim where\n",
      "rather sung father member porno\n",
      "rend freewheeling key nanny slaughter\n",
      "lettin fold syne responded confess\n",
      "welcome greater created having chauncey\n",
      "shuffles morgan typical refresh buttwasting\n",
      "highland worker fever sneakin remedies\n",
      "transcendental shove sergeant thuggish fried\n",
      "bride infinito shall same colored\n",
      "protest moor yeah oasis sparkling\n",
      "westward\n",
      "\n",
      "\n",
      "Song:  barbie girl by aqua\n",
      "Model: 2\n",
      "Initial Word: cabbage\n",
      "Generated Lyrics:\n",
      "cabbage spitten skies minus shadows\n",
      "fishes somthing sinners funk cubes\n",
      "tap anothers ready atall pictures\n",
      "chemistry tus doll messed half\n",
      "daisy away brightest curse ideas\n",
      "sorrow scold versus delirious radiation\n",
      "pleas only screaming bars blastin\n",
      "crews id dddo swinging times\n",
      "silence pd imagination worth maggie\n",
      "bumpin centurys gather white chick\n",
      "seeing underthe walkman leaf consideration\n",
      "dimes pelicans stumm wrote album\n",
      "brown tar clockwork highway matter\n",
      "imalready bereft dong cowards shattering\n",
      "dickins tryin becomes breaktime wasnt\n",
      "unborn breezes lifelong earlobe querida\n",
      "box live alight deceive head\n",
      "pass rockidol courtroom balloon aware\n",
      "whose twelve partying rein breakers\n",
      "longhaired deaf bang like repeating\n",
      "drowns fed situations saturday knight\n",
      "longest thathatthatthat blazin dog lean\n",
      "landscape distortion suelo supernova golly\n",
      "delighted missing clout slippityslide rag\n",
      "complete fortytwo brew walks lil\n",
      "disagree abroad after handle century\n",
      "carving mantle creatin bounds olden\n",
      "prisoner tempted em sieeieeign jet\n",
      "stories chickity alike corpses st\n",
      "an oohooh theyre boots brown\n",
      "shooting spreadin instantly shallow prepared\n",
      "stool sleep pack back figured\n",
      "zigzags shack bold fitness prayer\n",
      "present lawless lavida whoride mil\n",
      "writer off dich season marketplace\n",
      "martin dress doesnt station seams\n",
      "first remind reversed id fears\n",
      "assure tapped eating freak ocean\n",
      "spins querida killers discovered throttle\n",
      "amazin rush snuff pot o\n",
      "praises\n",
      "\n",
      "\n",
      "Song:  all the small things by blink 182\n",
      "Model: 2\n",
      "Initial Word: cabbage\n",
      "Generated Lyrics:\n",
      "cabbage pronto flavor revolutionary mop\n",
      "joke remembering weekend therefore freeze\n",
      "parental caring lake battling insecure\n",
      "dadoronron eah ammo robbin losers\n",
      "climb snowmobiles letters change tease\n",
      "knocked hoodlum nighttime cranium laced\n",
      "apart flashback scarlet anticipating condom\n",
      "nails hollywood defeated of shinning\n",
      "vicinity kissed anywhere isnt whoopityayeyay\n",
      "boy chambers charm passin rocks\n",
      "d spend piano feeding lung\n",
      "wigga heartbreaker ei meaning morgue\n",
      "notes mover jungle foods chicka\n",
      "automatic plus forgive lion aleluya\n",
      "jockin ginger adown bettin bends\n",
      "cape dem honestly heavy twain\n",
      "over ross turnin cocky length\n",
      "ass rhinestone heed changing cantaloupes\n",
      "bizzy hoes added slipped roll\n",
      "fence larger woe pina confronted\n",
      "lean highway expose serenity strawberry\n",
      "thank class countin rightly floors\n",
      "ah brokin socially union huh\n",
      "cover fifteen coming slavery decision\n",
      "dweller ich homes matilda top\n",
      "crops market slightly avalanche evrydays\n",
      "wail meditation ahyo run tribal\n",
      "lite jfk corpse dawg rightly\n",
      "cleared cross lay catches dependents\n",
      "moons scandals ever superman pervert\n",
      "honor treetops bites note mama\n",
      "guidance stranded pen somethings admit\n",
      "sam bluer garden soldier these\n",
      "chorus breathed help tammy heartache\n",
      "tucked rend gators upstairs fading\n",
      "curse best churchill lone dice\n",
      "raw proved snow mcveigh sunk\n",
      "miracles houses navy jackson zigzags\n",
      "eddy blows mate seven block\n",
      "mama slate rhythm clique correspondent\n",
      "overtime\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "Analysis for strategy: test_lyrics\n",
      "\n",
      "Song:  eternal flame by the bangles\n",
      "Model: 1\n",
      "Initial Word: close\n",
      "Generated Lyrics:\n",
      "close enough through breather whoever\n",
      "playettes momma helps hellier bonethugsnharmony\n",
      "widowed disregard pull smoked hangup\n",
      "backstreet digest malcolm usually even\n",
      "rumor incomprehensible youyou stranglin finish\n",
      "shepards boy schemes claim softer\n",
      "konk pretended ja flight force\n",
      "guns restraining basis guitarpicking donated\n",
      "block y walking streams stressin\n",
      "knew zigzags blare tipped fills\n",
      "sayin touched gang longhorn holster\n",
      "shit moonbeams shallow ripe lovin\n",
      "cleared leone jerk christmases crack\n",
      "case extraterrestrial thrilling fourteens joke\n",
      "island cos hospital verse overworked\n",
      "pleasin confession delivery von beans\n",
      "locs cut trips twen taken\n",
      "clay orders stapler honestly dove\n",
      "rule expensive givin mable olden\n",
      "4th abroad swelter donut bred\n",
      "error han bloody run golly\n",
      "despair releasin 44 triangle number\n",
      "kidding married bags bad tragedy\n",
      "american spiritual irritates praising fearful\n",
      "foot begun century tendency timbaland\n",
      "yard booze waist sit nevertheless\n",
      "cabbage situation raindrops yards rolled\n",
      "cal blistering failing eddy itll\n",
      "reminded 1916 reed greeting galahad\n",
      "ringaling wooo anxiety maybe ryders\n",
      "delaying individuals shits protest past\n",
      "thousand well gives adore photographs\n",
      "parties nanny sticking echoing malcolm\n",
      "suffocates aye todays tended fits\n",
      "lizards embracing overcoat seventeen badtasting\n",
      "hendrixs videotape cooks tambourine 9\n",
      "combination constantly strangled fightin plight\n",
      "fellows winning norman tiree superlative\n",
      "teens lead anspricht daisy cuento\n",
      "loud ce 4h anyways passin\n",
      "dc\n",
      "\n",
      "\n",
      "Song:  honesty by billy joel\n",
      "Model: 1\n",
      "Initial Word: if\n",
      "Generated Lyrics:\n",
      "if sex happier tainted stunt\n",
      "ago savings havent returning searching\n",
      "control intelligence peligro cost nickbsb\n",
      "music restless whod dazzling 10\n",
      "qudate spine box kid wien\n",
      "peter lab largo building showin\n",
      "pronto eyebrow arizona 1951 restless\n",
      "media graduation church little prepare\n",
      "as moans infinito soir inspires\n",
      "blocks yearnings accelerate vienna carefully\n",
      "nuevo tight 4th telling wack\n",
      "hue odyssey christmases curly mp3\n",
      "intent need delusions peak bracelets\n",
      "wrapped wring sonny superlative sueno\n",
      "vez interesting forgiven paradise creatures\n",
      "afternoon destiny america maybe sink\n",
      "larger holes paintings guevara ought\n",
      "ticket noche ow among earned\n",
      "thee guard d12 goodness ignore\n",
      "marx charm later offense laced\n",
      "creole lawyers bent asured chopping\n",
      "tastin cheeks honey admit pad\n",
      "complete stamps savoy losers horizons\n",
      "fags bump admired echoes sadness\n",
      "lying lonesome hey allowed pure\n",
      "daily florida fitness donta guard\n",
      "naked missiles pilots cellophane risk\n",
      "lebte joseph grounded ruff hoo\n",
      "hills sink leading downside toughest\n",
      "moans aladdin sleeve vibe sins\n",
      "dodge twirl eternally carousel pistol\n",
      "stillness sense gees climbed cheat\n",
      "releasin fully mir ticktock spine\n",
      "task awith traffic treating ayin\n",
      "slaughter crimson doz farm plotting\n",
      "repeated fifth meyers conductor sleepin\n",
      "lets sappy defecating sleighing fret\n",
      "coastline cross candy shots affair\n",
      "camino playboy core walk president\n",
      "medo price finer places weighing\n",
      "union\n",
      "\n",
      "\n",
      "Song:  lovefool by cardigans\n",
      "Model: 1\n",
      "Initial Word: dear\n",
      "Generated Lyrics:\n",
      "dear trouble trippin ohh mysterious\n",
      "tv dirty wait steadliy insanity\n",
      "tokyo serenity productions supernova diga\n",
      "amazed rappen bei earthquakes thicker\n",
      "ups antelopes nanana wont scruff\n",
      "clubs crumble tat delivery even\n",
      "til of bolo unknown construction\n",
      "indication nanana dawse bleak sporting\n",
      "hart extra their chorusx2 rave\n",
      "candle flags else warning chat\n",
      "melting hurries flashes read basement\n",
      "wrist earth shelter grind girlfriend\n",
      "spot aquaman begging truly hairy\n",
      "weapons rooms christmas crimson luni\n",
      "lot voices retreat dining woah\n",
      "starry renee poison purse jane\n",
      "waterbed thrilling drums dare sin\n",
      "ladies luni stand search sand\n",
      "origami hating che nimble girlill\n",
      "crowds death homies lot coral\n",
      "prostitute packs mocha toughest theydve\n",
      "sitting choked mony hers daddys\n",
      "alleluia baggy coupe cemetery recording\n",
      "23 horas cat pushed altar\n",
      "greyhound tryina mr least close\n",
      "doney healing moffats bond medical\n",
      "arrive corners sell puff panics\n",
      "clockwork dodge layin fore involved\n",
      "vicinity necklace stamps suitcases central\n",
      "rocks iesha hollow sheds moor\n",
      "uranus impression warnin hypnotic fifi\n",
      "receiving uptown else leave bing\n",
      "forgettable silent noise fore eahm\n",
      "track wack im peel stoned\n",
      "appetite magnet saffron cheese steak\n",
      "wife meaningful caused caps sayin\n",
      "strap nude quite melt x6\n",
      "devils masters sangre shine mister\n",
      "drives wail snow leg busness\n",
      "bouncebouncin 75 trailer hall punker\n",
      "gray\n",
      "\n",
      "\n",
      "Song:  barbie girl by aqua\n",
      "Model: 1\n",
      "Initial Word: hiya\n",
      "Generated Lyrics:\n",
      "hiya sip imagination shaqs para\n",
      "dil all ebud jackets searchin\n",
      "small awoken beneath pearls cd\n",
      "letter playa slate trail rockers\n",
      "disgrace face 48 absent down\n",
      "moms several prozac meet bebopalula\n",
      "feelings cowards worst revolver balls\n",
      "curves shh maker interstate x6\n",
      "haunted looked wise choke cigarettes\n",
      "brain prodded shouting rubs chased\n",
      "remains pan rewritten watched cast\n",
      "sleeve remember used lang ass\n",
      "trains repeat react atheist fantasies\n",
      "bin risks screams reached lab\n",
      "royces bloodier limousines nightmare bet\n",
      "crackin control divorce complain granted\n",
      "collectin pair what booooooooooogie bates\n",
      "soap fourteens eastern johnnys begging\n",
      "mortal pining sunrise denies strangers\n",
      "everywhere cute pure mirage fishin\n",
      "creation flying enforce began highest\n",
      "hurricane midsummers shepards nicer ignoring\n",
      "everlastin foes main deliver huh\n",
      "cares wandas guilty clue searchin\n",
      "biting wheels garrett by oasis\n",
      "rawkus undeniable clitoris pleasures virtuose\n",
      "evergreen righteousness oooh homie trains\n",
      "trivial dimension bo shotgun rob\n",
      "screaming tus old disappeared heebeegeebees\n",
      "still unconscious gewohnt our somebodys\n",
      "5 whispered odds windows presence\n",
      "thirst slate nor callar inspired\n",
      "frozen bowelshaking reallife honies leadingstill\n",
      "ack anywhere seconds dazzling smeared\n",
      "misses steed fog curricular charleston\n",
      "eazy werent maggie hiring slumber\n",
      "fly oouh motives need lightnin\n",
      "shell repeating mca three terminally\n",
      "ale fish spendin cahoots el\n",
      "flicker ron bite spin makes\n",
      "weighing\n",
      "\n",
      "\n",
      "Song:  all the small things by blink 182\n",
      "Model: 1\n",
      "Initial Word: all\n",
      "Generated Lyrics:\n",
      "all mud chromium finger asked\n",
      "bell mummy miss sunrisethis unlimited\n",
      "shouting silhouette amor if kinky\n",
      "frontin bacall excelsis flies notion\n",
      "parkway nearest dreadlock class immediately\n",
      "neighborhood foods acheckin rainbow somewhere\n",
      "mankind fancy piece hole vicinity\n",
      "pussy evrything belief drah capability\n",
      "chauncey harm tide mirrors numbered\n",
      "saints viene file drawer nightmare\n",
      "jaw ashes allisons brains rabies\n",
      "supernova days hurts kit folks\n",
      "du se anschaun cynics countless\n",
      "twirl paused bend frameless torn\n",
      "hago 53 anyway kojak herself\n",
      "deception lazy favorite halftime caroling\n",
      "warnin detention motherfucking logs dismay\n",
      "block stole trojan glows forever\n",
      "tact in x3 log pearls\n",
      "hardly staring covered hm weapons\n",
      "nowadays said better hustling waters\n",
      "comingglad sola please uns just\n",
      "arrant creatures foreigners didnt con\n",
      "talwrts jill asured firmamento stroked\n",
      "wall closer christina vexes cantaloupes\n",
      "dres boats chance everyone dodoublegs\n",
      "barred poison hudson ego mysterious\n",
      "kids abstishit samurai warriors cooly\n",
      "reflected sweet gloria form stan\n",
      "halftime 59 hitch wir dazwischen\n",
      "vodka potent nurses drove fourleaf\n",
      "team bringin breathin fading choirs\n",
      "pretend hey hurry freeze whoah\n",
      "eek crookedness strait winslow matter\n",
      "barricade rainbow fairy unwinds hurries\n",
      "types flashbulbs bodyguards e wade\n",
      "crib frozen unbelievable dependents trank\n",
      "upper myself underneath donkeys die\n",
      "throttle frolic pit lame starin\n",
      "den lethal respond howd were\n",
      "intentionooh\n",
      "\n",
      "\n",
      "Song:  eternal flame by the bangles\n",
      "Model: 2\n",
      "Initial Word: close\n",
      "Generated Lyrics:\n",
      "close trumped womb sorry messing\n",
      "hydro winston charms greedy function\n",
      "playboy warum galveston always pink\n",
      "hydro rockin tragic obsessed dominations\n",
      "shag global imagining click dropped\n",
      "silhouette sometime brigade weepin crouched\n",
      "genie trouble dc lauryn what\n",
      "bothers crackin pages february whoh\n",
      "alone matters boulevard degradin cloudy\n",
      "high gears letter shes record\n",
      "braille feather jeans arena storm\n",
      "steer nicer wasnt wheels yellin\n",
      "tripped rubs accept lives voice\n",
      "voulezvous desolate chronic when biz\n",
      "bridges showgirl winning weit jester\n",
      "asap sheltered miedo barry crawlin\n",
      "chattanooga rotors flatbed gegen impregnate\n",
      "slide claim inland concert prolong\n",
      "coastline gears alien russell creeds\n",
      "boody groaning sister scruff kissing\n",
      "toasted rauch dreamin indeed phoenix\n",
      "learned tiree ticktock dudududuu delight\n",
      "strumming superstitions laid indeed sidelines\n",
      "tis julio cymbals adore clip\n",
      "graph smiling aching parties chromium\n",
      "hercules nazis way audio million\n",
      "wandering decency nowhere sin paranoid\n",
      "girli drift jerk khan tocas\n",
      "guess richard sick smell everlastin\n",
      "hooligan don eddy season planetary\n",
      "little nicky la impressed begin\n",
      "freewheeling frame missing chemistry perfectly\n",
      "assure homeless north ugh glasses\n",
      "pedestrians stay neath flower leann\n",
      "anothin indecisions capped grey chromium\n",
      "asses reach culture figured gym\n",
      "soir pretend line feeeling tendency\n",
      "hand reminisce cloud bother solve\n",
      "basis walin carnation pleasure second\n",
      "communication function x5 lover woohoo\n",
      "angel\n",
      "\n",
      "\n",
      "Song:  honesty by billy joel\n",
      "Model: 2\n",
      "Initial Word: if\n",
      "Generated Lyrics:\n",
      "if jiggy impressed thee somebody\n",
      "charge seen desperately feminist goofin\n",
      "writing masters runway coupe wiser\n",
      "jimi catacombs insecure robbery silly\n",
      "tombs refused led spock wine\n",
      "moe keep has wardrobe thunder\n",
      "first floors avoid thumping casts\n",
      "sting sam cripin information poor\n",
      "rapped strangest disgraced mirrors chameleons\n",
      "jacket release track beauty waz\n",
      "supposed authority cymbals finally dialed\n",
      "chapter wants reputation constellations faking\n",
      "jill grandmothers preaching witchy brooks\n",
      "cried net eazyes mortars dull\n",
      "jade circle met thinking months\n",
      "beer refuse julio mother support\n",
      "causin cloaked meat shimmering generation\n",
      "yea thorn sailing pleasures metaphors\n",
      "flava laughters somebody neat bliss\n",
      "dis celebration authority miller indian\n",
      "women sacrifice at tiled nacht\n",
      "booooooooooogie smell johnson deliver bridge\n",
      "cafe etc fofo grooving important\n",
      "whoever helped ait billy analyst\n",
      "wicker capacity justa blinded burn\n",
      "stadium annoy actress past caps\n",
      "brooks hands but lebenslust bing\n",
      "pleasures traveling staring scour spears\n",
      "remains love dusta kidnap defense\n",
      "midtwenties nineseven singers bubbleheadedbleachblond clean\n",
      "craving hated punk stranglin symphony\n",
      "reason guiding aquaman corpse six\n",
      "licks praises santa pleasures checkin\n",
      "county runway shouldnt waterloo chat\n",
      "chicka instead twocar spose populace\n",
      "words combat reach strength long\n",
      "gather virtual moms doorbell rob\n",
      "gangster kisses noisy bush nicer\n",
      "hour occur miles today dadoronron\n",
      "passing carol vuelve graces decent\n",
      "softer\n",
      "\n",
      "\n",
      "Song:  lovefool by cardigans\n",
      "Model: 2\n",
      "Initial Word: dear\n",
      "Generated Lyrics:\n",
      "dear acting so rauch frightened\n",
      "choose naughtynaughty content size awhen\n",
      "apartment heroes joyous sins blazin\n",
      "break weather colours stepping fear\n",
      "century rulin everyting homies creep\n",
      "giddy steam owe reality greasers\n",
      "defeated lab rhode plays fathers\n",
      "mexico wastes cuss school whoop\n",
      "joint murder contract falls wrongs\n",
      "homes bay fits fully spose\n",
      "moor dolce delirious represent this\n",
      "grit boomshake jealous aladdin manic\n",
      "collecting journey woest soap following\n",
      "lavas crumbled jedes simplemente keepin\n",
      "shower toolio planet nights shovel\n",
      "fears cameras boughs same claret\n",
      "plaques hitter nigga siskel strung\n",
      "weve fate loaded befo dress\n",
      "shakingshaking him snow fearing free\n",
      "closes several hate wigga craazy\n",
      "road feeling petals ohohohohohoh yore\n",
      "winston harder flors moral weaker\n",
      "chaperon reverently prosperity attracted stadt\n",
      "chains stillness hounds raining nowits\n",
      "badtasting leatherface too noise metaphors\n",
      "navy wookie toys contract a\n",
      "safe closing heading bay youyou\n",
      "palace hesitating lowkey feathers al\n",
      "gangster virgins function maps dispute\n",
      "red golly 3 doors jealousy\n",
      "friends waz sleep innocent lody\n",
      "spring cookin blessings obsession unable\n",
      "saddest growing eases ninetythree pa\n",
      "shuffles footsteps women corncob barbed\n",
      "unkindly take track weight morn\n",
      "cantaloupes descubro bleeds untouched airs\n",
      "drugs proves clothes vita chills\n",
      "roost buried musics remedies orgy\n",
      "hope roams flower honey heavn\n",
      "disconcerting roars virgins counseling september\n",
      "bill\n",
      "\n",
      "\n",
      "Song:  barbie girl by aqua\n",
      "Model: 2\n",
      "Initial Word: hiya\n",
      "Generated Lyrics:\n",
      "hiya thundering trail rapper temples\n",
      "diploma baa favorite axe films\n",
      "swallow physically alibis made blind\n",
      "sane length nature used pace\n",
      "paid skies bumpin pickin prints\n",
      "feminist just roads cutie want\n",
      "world earned helping won afar\n",
      "mcs railroad divide asured excited\n",
      "individuals luck sidewalk freaky blowin\n",
      "raising needles union deceiving chorusx2\n",
      "jack lipstick honeymoon punched dashing\n",
      "pound dazed camel decorate sheltered\n",
      "hurry bucket slick cheesy barbed\n",
      "mc inland seems cannot enforce\n",
      "afraid settles bridge fanny remorse\n",
      "dead grow called silk bissau\n",
      "beings sending food smokin downass\n",
      "forwards una diez streetwise dependencies\n",
      "youth drum ignorance creeping blitz\n",
      "furry convince touch schnee ballers\n",
      "independent doz center leann raye\n",
      "southbound fortytwo mainstream stoned superficial\n",
      "lettin lu folks jones dylan\n",
      "minutes doc honda hue building\n",
      "ease throat rocking games comfortless\n",
      "gon definitely whom need weak\n",
      "fact cars x2repeat las sorrys\n",
      "wrinkled ventana daffodils jubilee woodwork\n",
      "golly swearing needle jc burst\n",
      "funds flippin norman arrant leavin\n",
      "tidings twist remembering sein embrace\n",
      "nuclear belief hid jackets sinking\n",
      "cutouts weak error balance tall\n",
      "unlimited bei semester needles fry\n",
      "continued aunt ruthless julio coldblooded\n",
      "cemetery victrola passin following regardless\n",
      "jubilee sean mousetrap disappointments djs\n",
      "adams various westward dumm lay\n",
      "hospital ago dandy bracelets attemptin\n",
      "sluts celebrate dudududuudu style porters\n",
      "bring\n",
      "\n",
      "\n",
      "Song:  all the small things by blink 182\n",
      "Model: 2\n",
      "Initial Word: all\n",
      "Generated Lyrics:\n",
      "all dab 90 juke crooked\n",
      "thing shots mask role tombs\n",
      "strike sitting slogan texarkana bumping\n",
      "monday attracting johnny bath outer\n",
      "euch flag clinging mountain plate\n",
      "ink glisten raindrops pen touches\n",
      "wok hiv sleighing record philly\n",
      "stairs inflate write ffa ringin\n",
      "add stapler yourself definitely laugh\n",
      "nicole peeps graph expert feels\n",
      "landing buckwildin steeple guarantee plastic\n",
      "hates bagged tears suffer hoodlums\n",
      "glue woo mic breather una\n",
      "dial realized gloves ares finger\n",
      "ay airs slowin hoo meanest\n",
      "rode canyons lifts target yeah\n",
      "holes ale crookedness buster shop\n",
      "maria dumb diamond wash peakin\n",
      "dunk worst content mercy disease\n",
      "phoenix voices ummm front depends\n",
      "expression crutch somethings blind correct\n",
      "va remedies hoist sunday photographs\n",
      "gay air plates pastime comingglad\n",
      "nevernever fernando delight nitro below\n",
      "rollercoaster gimme swingers loser lovings\n",
      "bullied matter voice favorite seismograph\n",
      "latex actress reassure greener enemies\n",
      "pumping bathe shook sistah peak\n",
      "goodness fat us paint steppin\n",
      "parks nearer physical clockwork remedy\n",
      "athinking breath losing sip lest\n",
      "pair last lil weekend ihren\n",
      "orient putting saw sean blond\n",
      "torches mrn undisturbed catacombs ringing\n",
      "film ragged dreamed alook autumn\n",
      "rub suns able wrinkled refresh\n",
      "drapes guaranteed hopping blank function\n",
      "front shadows sunbeam righteousness eddie\n",
      "zwei says donta badness blinds\n",
      "medicine servant acid liking yup\n",
      "gods\n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Store the randomly chosen word\n",
    "random_initial_word = None\n",
    "\n",
    "# Function to choose the initial word\n",
    "def choose_initial_word(strategy, test_lyrics=None):\n",
    "    global random_initial_word\n",
    "    if strategy == \"random\":\n",
    "        if random_initial_word is None:\n",
    "            random_initial_word = random.choice(list(word2index.keys()))\n",
    "        return random_initial_word\n",
    "    elif strategy == \"most_common\":\n",
    "        all_words = [word for tokens in train_set['tokens'] for word in tokens]\n",
    "        word_counts = Counter(all_words)\n",
    "        most_common_word = word_counts.most_common(1)[0][0]\n",
    "        return index2word[most_common_word]\n",
    "    elif strategy == \"least_common\":\n",
    "        all_words = [word for tokens in train_set['tokens'] for word in tokens]\n",
    "        word_counts = Counter(all_words)\n",
    "        least_common_word = word_counts.most_common()[-1][0]\n",
    "        return index2word[least_common_word]\n",
    "    elif strategy == \"test_lyrics\" and test_lyrics is not None:\n",
    "        return test_lyrics.split()[0]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Choose from 'random', 'most_common', 'least_common', or 'test_lyrics'.\")\n",
    "\n",
    "# Function to generate and print songs\n",
    "def generate_and_collect_songs(model, model_num, midi_features, test_seqs, test_data, test_set, strategies):\n",
    "    generated_lyrics = []\n",
    "    for i in range(len(test_data)):\n",
    "        test_lyrics = test_set.iloc[i]['Lyrics']\n",
    "        for strategy in strategies:\n",
    "            initial_word = choose_initial_word(strategy, test_lyrics)\n",
    "            generated_song = generate_song(model, initial_word, midi_features[i], test_seqs[i], test_dataset_model1)\n",
    "            generated_lyrics.append((test_data[i][1], test_data[i][0], model_num, strategy, initial_word, generated_song))\n",
    "    return generated_lyrics\n",
    "\n",
    "# Define the strategies\n",
    "strategies = [\"random\", \"most_common\", \"least_common\", \"test_lyrics\"]\n",
    "\n",
    "# Collect and print generated songs\n",
    "def collect_and_print_all_songs():\n",
    "    all_generated_lyrics = []\n",
    "\n",
    "    # Generate Test Songs for Approach 1\n",
    "    lyrics_approach_1 = generate_and_collect_songs(first_model, 1, midi_features_model1, test_seq_model1, test_data, test_set, strategies)\n",
    "    all_generated_lyrics.extend(lyrics_approach_1)\n",
    "\n",
    "    # Generate Test Songs for Approach 2\n",
    "    lyrics_approach_2 = generate_and_collect_songs(second_model, 2, midi_features_model2, test_seq_model2, test_data, test_set, strategies)\n",
    "    all_generated_lyrics.extend(lyrics_approach_2)\n",
    "\n",
    "    return all_generated_lyrics\n",
    "\n",
    "# Generating and collecting lyrics for different initial word strategies\n",
    "all_generated_lyrics = collect_and_print_all_songs()\n",
    "\n",
    "# Save all generated lyrics for the report\n",
    "import pandas as pd\n",
    "\n",
    "generated_lyrics_df = pd.DataFrame(all_generated_lyrics, columns=[\"Song Name\", \"Artist\", \"Model\", \"Strategy\", \"Initial Word\", \"Generated Lyrics\"])\n",
    "generated_lyrics_df.to_csv(\"generated_lyrics_report.csv\", index=False)\n",
    "\n",
    "# Analyze the effect of the initial word and melody on the generated lyrics\n",
    "def analyze_effects(generated_lyrics):\n",
    "    analysis = []\n",
    "    for strategy in strategies:\n",
    "        lyrics_with_strategy = [lyrics for lyrics in generated_lyrics if lyrics[3] == strategy]\n",
    "        analysis.append((strategy, lyrics_with_strategy))\n",
    "\n",
    "    for strategy, lyrics in analysis:\n",
    "        print(f\"\\nAnalysis for strategy: {strategy}\")\n",
    "        for song in lyrics:\n",
    "            print(f\"\\nSong: {song[0]} by {song[1]}\")\n",
    "            print(f\"Model: {song[2]}\")\n",
    "            print(f\"Initial Word: {song[4]}\")\n",
    "            print(f\"Generated Lyrics:\\n{song[5]}\\n\")\n",
    "        print(\"--------------------------------\\n\")\n",
    "\n",
    "print(\"Analyzing effects of the initial word and melody on the generated lyrics:\")\n",
    "analyze_effects(all_generated_lyrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of experiments: 32\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid\n",
    "hyperparameters = {\n",
    "    'hidden_dim': [128, 256],\n",
    "    'learning_rate': [1e-5, 1e-3],\n",
    "    'batch_size': [  32,64],\n",
    "    'optimizer': [ 'SGD','Adam'],\n",
    "    'num_epochs': [20,25]\n",
    "}\n",
    "total_experiments = len(list(itertools.product(*hyperparameters.values())))\n",
    "\n",
    "print(f\"Total number of experiments: {total_experiments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search For Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "train_df, val_df = train_test_split(train_set, test_size=0.2, shuffle=True)\n",
    "\n",
    "def run_experiment(train_df, val_df, hyperparameters, experiment_count, total_experiments):\n",
    "\n",
    "    total_time_start = time.time()\n",
    "    cumulative_time = 0\n",
    "\n",
    "    experiment_number = 1\n",
    "    for hidden_dim in hyperparameters['hidden_dim']:\n",
    "        for learning_rate in hyperparameters['learning_rate']:\n",
    "            for batch_size in hyperparameters['batch_size']:\n",
    "                for optimizer in hyperparameters['optimizer']:\n",
    "                    for num_epochs in hyperparameters['num_epochs']:\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        print(f\"Experiment {experiment_number}/{total_experiments}: Testing parameters: hidden_dim={hidden_dim}, learning_rate={learning_rate}, batch_size={batch_size}, optimizer={optimizer}, num_epochs={num_epochs}\")\n",
    "                        train_dataloader = prepare_dataloader(train_df, batch_size, method='model1')\n",
    "                        val_dataloader = prepare_dataloader(val_df, batch_size, method='model1')\n",
    "\n",
    "                        writer = SummaryWriter()\n",
    "                        model = train_model(train_dataloader, val_dataloader, 300 + 113, hidden_dim, num_epochs, writer, device, vocab_size, learning_rate, optimizer)\n",
    "\n",
    "                        # End timing the experiment\n",
    "                        \n",
    "                        end_time = time.time()\n",
    "                        duration = end_time - start_time\n",
    "                        cumulative_time += duration\n",
    "\n",
    "                        print(f\"Experiment {experiment_number} completed in {duration:.2f} seconds.\")\n",
    "\n",
    "                        experiment_number += 1\n",
    "\n",
    "    writer.close()\n",
    "    \n",
    "run_experiment(train_df, val_df, hyperparameters, 0, total_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search For Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1/32: Testing parameters: hidden_dim=128, learning_rate=1e-05, batch_size=32, optimizer=SGD, num_epochs=20\n",
      "Training loss for epoch 1: 8.972623507181803, Validation loss: 8.977048635482788\n",
      "New best score (8.977049).\n",
      "Training loss for epoch 2: 8.97633981704712, Validation loss: 8.975998878479004\n",
      "Validation loss decreased (8.975999 --> 8.975999).\n",
      "Training loss for epoch 3: 8.97633981704712, Validation loss: 8.972870429356893\n",
      "Validation loss decreased (8.972870 --> 8.972870).\n",
      "Training loss for epoch 4: 8.974388989535244, Validation loss: 8.973215758800507\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 5: 8.974388989535244, Validation loss: 8.973980617523193\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 6: 8.973154749189105, Validation loss: 8.974108854929606\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Training loss for epoch 7: 8.974052810668946, Validation loss: 8.97420150893075\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Training loss for epoch 8: 8.97375622917624, Validation loss: 8.974163234233856\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Duration Time:154.76824855804443\n",
      "Experiment 1 completed in 174.60 seconds.\n",
      "Experiment 2/32: Testing parameters: hidden_dim=128, learning_rate=1e-05, batch_size=32, optimizer=SGD, num_epochs=25\n",
      "Training loss for epoch 1: 8.974411691938128, Validation loss: 8.976537942886353\n",
      "New best score (8.976538).\n",
      "Training loss for epoch 2: 8.972823143005371, Validation loss: 8.975588321685791\n",
      "Validation loss decreased (8.975588 --> 8.975588).\n",
      "Training loss for epoch 3: 8.974120140075684, Validation loss: 8.973894039789835\n",
      "Validation loss decreased (8.973894 --> 8.973894).\n",
      "Training loss for epoch 4: 8.974409898122152, Validation loss: 8.97420597076416\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 5: 8.974409898122152, Validation loss: 8.973963832855224\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 6: 8.974409898122152, Validation loss: 8.97435442606608\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Training loss for epoch 7: 8.974117379439505, Validation loss: 8.974426814488002\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Training loss for epoch 8: 8.973741102218629, Validation loss: 8.97414630651474\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Duration Time:163.4395661354065\n",
      "Experiment 2 completed in 163.46 seconds.\n",
      "Experiment 3/32: Testing parameters: hidden_dim=128, learning_rate=1e-05, batch_size=32, optimizer=Adam, num_epochs=20\n",
      "Training loss for epoch 1: 8.955473082406181, Validation loss: 8.953644275665283\n",
      "New best score (8.953644).\n",
      "Training loss for epoch 2: 8.954819023609161, Validation loss: 8.952919721603394\n",
      "Validation loss decreased (8.952920 --> 8.952920).\n",
      "Training loss for epoch 3: 8.954777605393353, Validation loss: 8.952072223027548\n",
      "Validation loss decreased (8.952072 --> 8.952072).\n",
      "Training loss for epoch 4: 8.953702640533447, Validation loss: 8.951832830905914\n",
      "Validation loss decreased (8.951833 --> 8.951833).\n",
      "Training loss for epoch 5: 8.95233164514814, Validation loss: 8.951076745986938\n",
      "Validation loss decreased (8.951077 --> 8.951077).\n",
      "Training loss for epoch 6: 8.951368977041806, Validation loss: 8.950382192929586\n",
      "Validation loss decreased (8.950382 --> 8.950382).\n",
      "Training loss for epoch 7: 8.951368977041806, Validation loss: 8.949493885040283\n",
      "Validation loss decreased (8.949494 --> 8.949494).\n",
      "Training loss for epoch 8: 8.951182736290825, Validation loss: 8.948993057012558\n",
      "Validation loss decreased (8.948993 --> 8.948993).\n",
      "Training loss for epoch 9: 8.951099546332108, Validation loss: 8.94828897052341\n",
      "Validation loss decreased (8.948289 --> 8.948289).\n",
      "Training loss for epoch 10: 8.951099546332108, Validation loss: 8.948045086860656\n",
      "Validation loss decreased (8.948045 --> 8.948045).\n",
      "Training loss for epoch 11: 8.950233164287749, Validation loss: 8.947478532791138\n",
      "Validation loss decreased (8.947479 --> 8.947479).\n",
      "Training loss for epoch 12: 8.948947467803954, Validation loss: 8.94688751300176\n",
      "Validation loss decreased (8.946888 --> 8.946888).\n",
      "Training loss for epoch 13: 8.947797890367179, Validation loss: 8.946234904802763\n",
      "Validation loss decreased (8.946235 --> 8.946235).\n",
      "Training loss for epoch 14: 8.947344436020147, Validation loss: 8.945573483194623\n",
      "Validation loss decreased (8.945573 --> 8.945573).\n",
      "Training loss for epoch 15: 8.946652845902877, Validation loss: 8.94491949081421\n",
      "Validation loss decreased (8.944919 --> 8.944919).\n",
      "Training loss for epoch 16: 8.94600102599238, Validation loss: 8.944340482354164\n",
      "Validation loss decreased (8.944340 --> 8.944340).\n",
      "Training loss for epoch 17: 8.945779068829262, Validation loss: 8.943739512387443\n",
      "Validation loss decreased (8.943740 --> 8.943740).\n",
      "Training loss for epoch 18: 8.94530671521237, Validation loss: 8.943183978398642\n",
      "Validation loss decreased (8.943184 --> 8.943184).\n",
      "Training loss for epoch 19: 8.94530671521237, Validation loss: 8.942666442770706\n",
      "Validation loss decreased (8.942666 --> 8.942666).\n",
      "Training loss for epoch 20: 8.94515068500073, Validation loss: 8.942188096046447\n",
      "Validation loss decreased (8.942188 --> 8.942188).\n",
      "Duration Time:480.52029061317444\n",
      "Experiment 3 completed in 480.54 seconds.\n",
      "Experiment 4/32: Testing parameters: hidden_dim=128, learning_rate=1e-05, batch_size=32, optimizer=Adam, num_epochs=25\n",
      "Training loss for epoch 1: nan, Validation loss: 8.987157583236694\n",
      "New best score (8.987158).\n",
      "Training loss for epoch 2: 8.988448143005371, Validation loss: 8.985321283340454\n",
      "Validation loss decreased (8.985321 --> 8.985321).\n",
      "Training loss for epoch 3: 8.988448143005371, Validation loss: 8.984566529591879\n",
      "Validation loss decreased (8.984567 --> 8.984567).\n",
      "Training loss for epoch 4: 8.984982837330211, Validation loss: 8.984696805477142\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 5: 8.983196651234346, Validation loss: 8.983885765075684\n",
      "Validation loss decreased (8.983886 --> 8.983886).\n",
      "Training loss for epoch 6: 8.983196651234346, Validation loss: 8.983989040056864\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 7: 8.982490274641249, Validation loss: 8.983048234667097\n",
      "Validation loss decreased (8.983048 --> 8.983048).\n",
      "Training loss for epoch 8: 8.983258827872898, Validation loss: 8.982856273651123\n",
      "Validation loss decreased (8.982856 --> 8.982856).\n",
      "Training loss for epoch 9: 8.982964332287128, Validation loss: 8.982304652531942\n",
      "Validation loss decreased (8.982305 --> 8.982305).\n",
      "Training loss for epoch 10: 8.982082716623943, Validation loss: 8.981896471977233\n",
      "Validation loss decreased (8.981896 --> 8.981896).\n",
      "Training loss for epoch 11: 8.982082716623943, Validation loss: 8.98153060132807\n",
      "Validation loss decreased (8.981531 --> 8.981531).\n",
      "Training loss for epoch 12: 8.981829727397246, Validation loss: 8.981390039126078\n",
      "Validation loss decreased (8.981390 --> 8.981390).\n",
      "Training loss for epoch 13: 8.981004508765968, Validation loss: 8.98082813849816\n",
      "Validation loss decreased (8.980828 --> 8.980828).\n",
      "Training loss for epoch 14: 8.980730594732822, Validation loss: 8.980593000139509\n",
      "Validation loss decreased (8.980593 --> 8.980593).\n",
      "Training loss for epoch 15: 8.980355191230775, Validation loss: 8.98045891125997\n",
      "Validation loss decreased (8.980459 --> 8.980459).\n",
      "Training loss for epoch 16: 8.979383934868707, Validation loss: 8.97998046875\n",
      "Validation loss decreased (8.979980 --> 8.979980).\n",
      "Training loss for epoch 17: 8.979490706261169, Validation loss: 8.979576223036823\n",
      "Validation loss decreased (8.979576 --> 8.979576).\n",
      "Training loss for epoch 18: 8.979131298065186, Validation loss: 8.979342155986362\n",
      "Validation loss decreased (8.979342 --> 8.979342).\n",
      "Training loss for epoch 19: 8.979131298065186, Validation loss: 8.979198920099359\n",
      "Validation loss decreased (8.979199 --> 8.979199).\n",
      "Training loss for epoch 20: 8.979131298065186, Validation loss: 8.978938126564026\n",
      "Validation loss decreased (8.978938 --> 8.978938).\n",
      "Training loss for epoch 21: 8.979131298065186, Validation loss: 8.978544485001336\n",
      "Validation loss decreased (8.978544 --> 8.978544).\n",
      "Training loss for epoch 22: 8.978722187188955, Validation loss: 8.97812020778656\n",
      "Validation loss decreased (8.978120 --> 8.978120).\n",
      "Training loss for epoch 23: 8.978142804112927, Validation loss: 8.977904008782428\n",
      "Validation loss decreased (8.977904 --> 8.977904).\n",
      "Training loss for epoch 24: 8.976923407930316, Validation loss: 8.977565884590149\n",
      "Validation loss decreased (8.977566 --> 8.977566).\n",
      "Training loss for epoch 25: 8.976923407930316, Validation loss: 8.977185592651367\n",
      "Validation loss decreased (8.977186 --> 8.977186).\n",
      "Duration Time:520.7966132164001\n",
      "Experiment 4 completed in 520.83 seconds.\n",
      "Experiment 5/32: Testing parameters: hidden_dim=128, learning_rate=1e-05, batch_size=64, optimizer=SGD, num_epochs=20\n",
      "Training loss for epoch 1: 8.971144676208496, Validation loss: 8.966004371643066\n",
      "New best score (8.966004).\n",
      "Training loss for epoch 2: 8.972227096557617, Validation loss: 8.966196537017822\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 3: 8.968668619791666, Validation loss: 8.966886043548584\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 4: 8.968668619791666, Validation loss: 8.96725857257843\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Training loss for epoch 5: 8.968668619791666, Validation loss: 8.967222595214844\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Training loss for epoch 6: 8.968576007419163, Validation loss: 8.967530965805054\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Duration Time:134.32382082939148\n",
      "Experiment 5 completed in 134.35 seconds.\n",
      "Experiment 6/32: Testing parameters: hidden_dim=128, learning_rate=1e-05, batch_size=64, optimizer=SGD, num_epochs=25\n",
      "Training loss for epoch 1: 8.907539367675781, Validation loss: 8.909778594970703\n",
      "New best score (8.909779).\n",
      "Training loss for epoch 2: 8.907539367675781, Validation loss: 8.909804582595825\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 3: 8.907539367675781, Validation loss: 8.909979820251465\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 4: 8.910043954849243, Validation loss: 8.909968137741089\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Training loss for epoch 5: 8.910043398539225, Validation loss: 8.91003646850586\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Training loss for epoch 6: 8.910302434648786, Validation loss: 8.910086313883463\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Duration Time:118.27704930305481\n",
      "Experiment 6 completed in 118.30 seconds.\n",
      "Experiment 7/32: Testing parameters: hidden_dim=128, learning_rate=1e-05, batch_size=64, optimizer=Adam, num_epochs=20\n",
      "Training loss for epoch 1: 8.892928123474121, Validation loss: 8.895971298217773\n",
      "New best score (8.895971).\n",
      "Training loss for epoch 2: 8.894220352172852, Validation loss: 8.895232200622559\n",
      "Validation loss decreased (8.895232 --> 8.895232).\n",
      "Training loss for epoch 3: 8.894220352172852, Validation loss: 8.894757588704428\n",
      "Validation loss decreased (8.894758 --> 8.894758).\n",
      "Training loss for epoch 4: 8.896388053894043, Validation loss: 8.894473910331726\n",
      "Validation loss decreased (8.894474 --> 8.894474).\n",
      "Training loss for epoch 5: 8.894169171651205, Validation loss: 8.894265174865723\n",
      "Validation loss decreased (8.894265 --> 8.894265).\n",
      "Training loss for epoch 6: 8.893615995134626, Validation loss: 8.894097566604614\n",
      "Validation loss decreased (8.894098 --> 8.894098).\n",
      "Training loss for epoch 7: 8.893879334131876, Validation loss: 8.894067355564662\n",
      "Validation loss decreased (8.894067 --> 8.894067).\n",
      "Training loss for epoch 8: 8.893525396074567, Validation loss: 8.893567383289337\n",
      "Validation loss decreased (8.893567 --> 8.893567).\n",
      "Training loss for epoch 9: 8.893302981058756, Validation loss: 8.89322010676066\n",
      "Validation loss decreased (8.893220 --> 8.893220).\n",
      "Training loss for epoch 10: 8.892687638600668, Validation loss: 8.892766332626342\n",
      "Validation loss decreased (8.892766 --> 8.892766).\n",
      "Training loss for epoch 11: 8.892454242706298, Validation loss: 8.89241972836581\n",
      "Validation loss decreased (8.892420 --> 8.892420).\n",
      "Training loss for epoch 12: 8.892268998282296, Validation loss: 8.892023960749308\n",
      "Validation loss decreased (8.892024 --> 8.892024).\n",
      "Training loss for epoch 13: 8.892079960216176, Validation loss: 8.891666155595045\n",
      "Validation loss decreased (8.891666 --> 8.891666).\n",
      "Training loss for epoch 14: 8.892079960216176, Validation loss: 8.891551324299403\n",
      "Validation loss decreased (8.891551 --> 8.891551).\n",
      "Training loss for epoch 15: 8.89181195134702, Validation loss: 8.89123929341634\n",
      "Validation loss decreased (8.891239 --> 8.891239).\n",
      "Training loss for epoch 16: 8.891653594970704, Validation loss: 8.890937566757202\n",
      "Validation loss decreased (8.890938 --> 8.890938).\n",
      "Training loss for epoch 17: 8.891518042637752, Validation loss: 8.890808890847598\n",
      "Validation loss decreased (8.890809 --> 8.890809).\n",
      "Training loss for epoch 18: 8.891518042637752, Validation loss: 8.890540361404419\n",
      "Validation loss decreased (8.890540 --> 8.890540).\n",
      "Training loss for epoch 19: 8.891823097511574, Validation loss: 8.890420185892205\n",
      "Validation loss decreased (8.890420 --> 8.890420).\n",
      "Training loss for epoch 20: 8.89197589611185, Validation loss: 8.89015576839447\n",
      "Validation loss decreased (8.890156 --> 8.890156).\n",
      "Duration Time:442.7544991970062\n",
      "Experiment 7 completed in 442.77 seconds.\n",
      "Experiment 8/32: Testing parameters: hidden_dim=128, learning_rate=1e-05, batch_size=64, optimizer=Adam, num_epochs=25\n",
      "Training loss for epoch 1: 8.881064414978027, Validation loss: 8.879596710205078\n",
      "New best score (8.879597).\n",
      "Training loss for epoch 2: 8.881064414978027, Validation loss: 8.87960147857666\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 3: 8.878970718383789, Validation loss: 8.879776159922281\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 4: 8.878889401753744, Validation loss: 8.87948489189148\n",
      "Validation loss decreased (8.879485 --> 8.879485).\n",
      "Training loss for epoch 5: 8.87870488848005, Validation loss: 8.880051326751708\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 6: 8.87839388847351, Validation loss: 8.879687229792276\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 7: 8.87932653427124, Validation loss: 8.87954568862915\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Training loss for epoch 8: 8.87932653427124, Validation loss: 8.879673421382904\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Training loss for epoch 9: 8.879446744918823, Validation loss: 8.87932268778483\n",
      "Validation loss decreased (8.879323 --> 8.879323).\n",
      "Training loss for epoch 10: 8.879446744918823, Validation loss: 8.87952470779419\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 11: 8.879446744918823, Validation loss: 8.879243287173184\n",
      "Validation loss decreased (8.879243 --> 8.879243).\n",
      "Training loss for epoch 12: 8.879446744918823, Validation loss: 8.879033525784811\n",
      "Validation loss decreased (8.879034 --> 8.879034).\n",
      "Training loss for epoch 13: 8.878438472747803, Validation loss: 8.878874155191275\n",
      "Validation loss decreased (8.878874 --> 8.878874).\n",
      "Training loss for epoch 14: 8.878438472747803, Validation loss: 8.878941263471331\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 15: 8.87762600183487, Validation loss: 8.878723049163819\n",
      "Validation loss decreased (8.878723 --> 8.878723).\n",
      "Training loss for epoch 16: 8.87762600183487, Validation loss: 8.878470212221146\n",
      "Validation loss decreased (8.878470 --> 8.878470).\n",
      "Training loss for epoch 17: 8.87762600183487, Validation loss: 8.878251384286319\n",
      "Validation loss decreased (8.878251 --> 8.878251).\n",
      "Training loss for epoch 18: 8.87762600183487, Validation loss: 8.878232849968803\n",
      "Validation loss decreased (8.878233 --> 8.878233).\n",
      "Training loss for epoch 19: 8.87714958190918, Validation loss: 8.878047817631773\n",
      "Validation loss decreased (8.878048 --> 8.878048).\n",
      "Training loss for epoch 20: 8.87714958190918, Validation loss: 8.877869391441346\n",
      "Validation loss decreased (8.877869 --> 8.877869).\n",
      "Training loss for epoch 21: 8.87714958190918, Validation loss: 8.877686091831752\n",
      "Validation loss decreased (8.877686 --> 8.877686).\n",
      "Training loss for epoch 22: 8.87714958190918, Validation loss: 8.877611658789895\n",
      "Validation loss decreased (8.877612 --> 8.877612).\n",
      "Training loss for epoch 23: 8.876868867874146, Validation loss: 8.877631000850512\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 24: 8.876529997045344, Validation loss: 8.877413868904114\n",
      "Validation loss decreased (8.877414 --> 8.877414).\n",
      "Training loss for epoch 25: 8.87646716573964, Validation loss: 8.877259445190429\n",
      "Validation loss decreased (8.877259 --> 8.877259).\n",
      "Duration Time:473.14891028404236\n",
      "Experiment 8 completed in 473.21 seconds.\n",
      "Experiment 9/32: Testing parameters: hidden_dim=128, learning_rate=0.001, batch_size=32, optimizer=SGD, num_epochs=20\n",
      "Training loss for epoch 1: nan, Validation loss: 8.900321960449219\n",
      "New best score (8.900322).\n",
      "Training loss for epoch 2: 8.897068296160016, Validation loss: 8.896903395652771\n",
      "Validation loss decreased (8.896903 --> 8.896903).\n",
      "Training loss for epoch 3: 8.897068296160016, Validation loss: 8.895742654800415\n",
      "Validation loss decreased (8.895743 --> 8.895743).\n",
      "Training loss for epoch 4: 8.897068296160016, Validation loss: 8.896612048149109\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 5: 8.896058612399631, Validation loss: 8.89614748954773\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 6: 8.89578742980957, Validation loss: 8.89492678642273\n",
      "Validation loss decreased (8.894927 --> 8.894927).\n",
      "Training loss for epoch 7: 8.89426761203342, Validation loss: 8.894013166427612\n",
      "Validation loss decreased (8.894013 --> 8.894013).\n",
      "Training loss for epoch 8: 8.89426761203342, Validation loss: 8.89311808347702\n",
      "Validation loss decreased (8.893118 --> 8.893118).\n",
      "Training loss for epoch 9: 8.894479608535766, Validation loss: 8.892069551679823\n",
      "Validation loss decreased (8.892070 --> 8.892070).\n",
      "Training loss for epoch 10: 8.893302917480469, Validation loss: 8.890844798088073\n",
      "Validation loss decreased (8.890845 --> 8.890845).\n",
      "Training loss for epoch 11: 8.892914732297262, Validation loss: 8.889964472163808\n",
      "Validation loss decreased (8.889964 --> 8.889964).\n",
      "Training loss for epoch 12: 8.891365894904503, Validation loss: 8.889334996541342\n",
      "Validation loss decreased (8.889335 --> 8.889335).\n",
      "Training loss for epoch 13: 8.891365894904503, Validation loss: 8.888928780188927\n",
      "Validation loss decreased (8.888929 --> 8.888929).\n",
      "Training loss for epoch 14: 8.891365894904503, Validation loss: 8.888633234160286\n",
      "Validation loss decreased (8.888633 --> 8.888633).\n",
      "Training loss for epoch 15: 8.891365894904503, Validation loss: 8.88865229288737\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 16: 8.890955694790545, Validation loss: 8.888007014989853\n",
      "Validation loss decreased (8.888007 --> 8.888007).\n",
      "Training loss for epoch 17: 8.888045439849028, Validation loss: 8.887024444692274\n",
      "Validation loss decreased (8.887024 --> 8.887024).\n",
      "Training loss for epoch 18: 8.888195062294985, Validation loss: 8.885977639092339\n",
      "Validation loss decreased (8.885978 --> 8.885978).\n",
      "Training loss for epoch 19: 8.888430404663087, Validation loss: 8.885058202241597\n",
      "Validation loss decreased (8.885058 --> 8.885058).\n",
      "Training loss for epoch 20: 8.886961522309676, Validation loss: 8.884396135807037\n",
      "Validation loss decreased (8.884396 --> 8.884396).\n",
      "Duration Time:398.2356357574463\n",
      "Experiment 9 completed in 398.25 seconds.\n",
      "Experiment 10/32: Testing parameters: hidden_dim=128, learning_rate=0.001, batch_size=32, optimizer=SGD, num_epochs=25\n",
      "Training loss for epoch 1: nan, Validation loss: 8.9065682888031\n",
      "New best score (8.906568).\n",
      "Training loss for epoch 2: 8.91042709350586, Validation loss: 8.906192421913147\n",
      "Validation loss decreased (8.906192 --> 8.906192).\n",
      "Training loss for epoch 3: 8.908815145492554, Validation loss: 8.905932823816935\n",
      "Validation loss decreased (8.905933 --> 8.905933).\n",
      "Training loss for epoch 4: 8.90818738937378, Validation loss: 8.905072629451752\n",
      "Validation loss decreased (8.905073 --> 8.905073).\n",
      "Training loss for epoch 5: 8.907502280341255, Validation loss: 8.903977203369141\n",
      "Validation loss decreased (8.903977 --> 8.903977).\n",
      "Training loss for epoch 6: 8.907502280341255, Validation loss: 8.903428236643473\n",
      "Validation loss decreased (8.903428 --> 8.903428).\n",
      "Training loss for epoch 7: 8.905188707204966, Validation loss: 8.902447598321098\n",
      "Validation loss decreased (8.902448 --> 8.902448).\n",
      "Training loss for epoch 8: 8.90455709184919, Validation loss: 8.901617735624313\n",
      "Validation loss decreased (8.901618 --> 8.901618).\n",
      "Training loss for epoch 9: 8.90455709184919, Validation loss: 8.901347637176514\n",
      "Validation loss decreased (8.901348 --> 8.901348).\n",
      "Training loss for epoch 10: 8.904017893473307, Validation loss: 8.901165127754211\n",
      "Validation loss decreased (8.901165 --> 8.901165).\n",
      "Training loss for epoch 11: 8.900595621629195, Validation loss: 8.900328094309026\n",
      "Validation loss decreased (8.900328 --> 8.900328).\n",
      "Training loss for epoch 12: 8.900595621629195, Validation loss: 8.899634818236033\n",
      "Validation loss decreased (8.899635 --> 8.899635).\n",
      "Training loss for epoch 13: 8.900053787231446, Validation loss: 8.899016343630278\n",
      "Validation loss decreased (8.899016 --> 8.899016).\n",
      "Training loss for epoch 14: 8.8980224609375, Validation loss: 8.897976994514465\n",
      "Validation loss decreased (8.897977 --> 8.897977).\n",
      "Training loss for epoch 15: 8.896756461172393, Validation loss: 8.897011216481527\n",
      "Validation loss decreased (8.897011 --> 8.897011).\n",
      "Training loss for epoch 16: 8.896756461172393, Validation loss: 8.896261796355247\n",
      "Validation loss decreased (8.896262 --> 8.896262).\n",
      "Training loss for epoch 17: 8.895160330666435, Validation loss: 8.895502118503346\n",
      "Validation loss decreased (8.895502 --> 8.895502).\n",
      "Training loss for epoch 18: 8.894297184088291, Validation loss: 8.89458057615492\n",
      "Validation loss decreased (8.894581 --> 8.894581).\n",
      "Training loss for epoch 19: 8.894297184088291, Validation loss: 8.893737968645597\n",
      "Validation loss decreased (8.893738 --> 8.893738).\n",
      "Training loss for epoch 20: 8.892476370168287, Validation loss: 8.892934906482697\n",
      "Validation loss decreased (8.892935 --> 8.892935).\n",
      "Training loss for epoch 21: 8.89196812022816, Validation loss: 8.891946588243757\n",
      "Validation loss decreased (8.891947 --> 8.891947).\n",
      "Training loss for epoch 22: 8.89196812022816, Validation loss: 8.891231851144271\n",
      "Validation loss decreased (8.891232 --> 8.891232).\n",
      "Training loss for epoch 23: 8.89196812022816, Validation loss: 8.890392780303955\n",
      "Validation loss decreased (8.890393 --> 8.890393).\n",
      "Training loss for epoch 24: 8.88824281325707, Validation loss: 8.889735917250315\n",
      "Validation loss decreased (8.889736 --> 8.889736).\n",
      "Training loss for epoch 25: 8.886738361011853, Validation loss: 8.888712387084961\n",
      "Validation loss decreased (8.888712 --> 8.888712).\n",
      "Duration Time:474.90713024139404\n",
      "Experiment 10 completed in 474.93 seconds.\n",
      "Experiment 11/32: Testing parameters: hidden_dim=128, learning_rate=0.001, batch_size=32, optimizer=Adam, num_epochs=20\n",
      "Training loss for epoch 1: 8.976909955342611, Validation loss: 8.911202430725098\n",
      "New best score (8.911202).\n",
      "Training loss for epoch 2: 8.890018105506897, Validation loss: 8.80915892124176\n",
      "Validation loss decreased (8.809159 --> 8.809159).\n",
      "Training loss for epoch 3: 8.852626419067382, Validation loss: 8.727828661600748\n",
      "Validation loss decreased (8.727829 --> 8.727829).\n",
      "Training loss for epoch 4: 8.82202079079368, Validation loss: 8.661273121833801\n",
      "Validation loss decreased (8.661273 --> 8.661273).\n",
      "Training loss for epoch 5: 8.366105556488037, Validation loss: 8.09597668647766\n",
      "Validation loss decreased (8.095977 --> 8.095977).\n",
      "Training loss for epoch 6: 8.366105556488037, Validation loss: 7.7157035668691\n",
      "Validation loss decreased (7.715704 --> 7.715704).\n",
      "Training loss for epoch 7: 8.094789934158324, Validation loss: 7.237339539187295\n",
      "Validation loss decreased (7.237340 --> 7.237340).\n",
      "Training loss for epoch 8: 7.124332565527696, Validation loss: 6.826527774333954\n",
      "Validation loss decreased (6.826528 --> 6.826528).\n",
      "Training loss for epoch 9: 7.124332565527696, Validation loss: 6.596257991260952\n",
      "Validation loss decreased (6.596258 --> 6.596258).\n",
      "Training loss for epoch 10: 7.124332565527696, Validation loss: 6.304851472377777\n",
      "Validation loss decreased (6.304851 --> 6.304851).\n",
      "Training loss for epoch 11: 6.629759033521016, Validation loss: 6.02244214036248\n",
      "Validation loss decreased (6.022442 --> 6.022442).\n",
      "Training loss for epoch 12: 5.730414575338363, Validation loss: 5.757208228111267\n",
      "Validation loss decreased (5.757208 --> 5.757208).\n",
      "Training loss for epoch 13: 5.730414575338363, Validation loss: 5.53634368456327\n",
      "Validation loss decreased (5.536344 --> 5.536344).\n",
      "Training loss for epoch 14: 5.27275875210762, Validation loss: 5.324523044484002\n",
      "Validation loss decreased (5.324523 --> 5.324523).\n",
      "Training loss for epoch 15: 5.196068258285522, Validation loss: 5.1564392149448395\n",
      "Validation loss decreased (5.156439 --> 5.156439).\n",
      "Training loss for epoch 16: 5.143198363921222, Validation loss: 4.988117741420865\n",
      "Validation loss decreased (4.988118 --> 4.988118).\n",
      "Training loss for epoch 17: 4.9509792371229695, Validation loss: 4.843555434661753\n",
      "Validation loss decreased (4.843555 --> 4.843555).\n",
      "Training loss for epoch 18: 4.9509792371229695, Validation loss: 4.736180472705099\n",
      "Validation loss decreased (4.736180 --> 4.736180).\n",
      "Training loss for epoch 19: 4.848054319097285, Validation loss: 4.627705650894265\n",
      "Validation loss decreased (4.627706 --> 4.627706).\n",
      "Training loss for epoch 20: 4.760503206650416, Validation loss: 4.537578628957272\n",
      "Validation loss decreased (4.537579 --> 4.537579).\n",
      "Duration Time:423.66824555397034\n",
      "Experiment 11 completed in 423.69 seconds.\n",
      "Experiment 12/32: Testing parameters: hidden_dim=128, learning_rate=0.001, batch_size=32, optimizer=Adam, num_epochs=25\n",
      "Training loss for epoch 1: 8.927855491638184, Validation loss: 8.880696773529053\n",
      "New best score (8.880697).\n",
      "Training loss for epoch 2: 8.910661061604818, Validation loss: 8.86664879322052\n",
      "Validation loss decreased (8.866649 --> 8.866649).\n",
      "Training loss for epoch 3: 8.910661061604818, Validation loss: 8.863298654556274\n",
      "Validation loss decreased (8.863299 --> 8.863299).\n",
      "Training loss for epoch 4: 8.910661061604818, Validation loss: 8.860810697078705\n",
      "Validation loss decreased (8.860811 --> 8.860811).\n",
      "Training loss for epoch 5: 8.854776109967913, Validation loss: 8.828118133544923\n",
      "Validation loss decreased (8.828118 --> 8.828118).\n",
      "Training loss for epoch 6: 8.830912351608276, Validation loss: 8.801281849543253\n",
      "Validation loss decreased (8.801282 --> 8.801282).\n",
      "Training loss for epoch 7: 8.830912351608276, Validation loss: 8.779805830546788\n",
      "Validation loss decreased (8.779806 --> 8.779806).\n",
      "Training loss for epoch 8: 8.75815842368386, Validation loss: 8.732325345277786\n",
      "Validation loss decreased (8.732325 --> 8.732325).\n",
      "Training loss for epoch 9: 8.51314863562584, Validation loss: 8.54840714401669\n",
      "Validation loss decreased (8.548407 --> 8.548407).\n",
      "Training loss for epoch 10: 7.196725091934204, Validation loss: 8.110265129804612\n",
      "Validation loss decreased (8.110265 --> 8.110265).\n",
      "Training loss for epoch 11: 6.250146389007568, Validation loss: 7.655032791874626\n",
      "Validation loss decreased (7.655033 --> 7.655033).\n",
      "Training loss for epoch 12: 6.141246241681716, Validation loss: 7.298517356316249\n",
      "Validation loss decreased (7.298517 --> 7.298517).\n",
      "Training loss for epoch 13: 5.830974456591484, Validation loss: 6.954252990392538\n",
      "Validation loss decreased (6.954253 --> 6.954253).\n",
      "Training loss for epoch 14: 5.45819288889567, Validation loss: 6.671061784029007\n",
      "Validation loss decreased (6.671062 --> 6.671062).\n",
      "Training loss for epoch 15: 5.392617946085722, Validation loss: 6.41497387488683\n",
      "Validation loss decreased (6.414974 --> 6.414974).\n",
      "Training loss for epoch 16: 4.9893543243408205, Validation loss: 6.174036301672459\n",
      "Validation loss decreased (6.174036 --> 6.174036).\n",
      "Training loss for epoch 17: 4.9893543243408205, Validation loss: 5.961469695848577\n",
      "Validation loss decreased (5.961470 --> 5.961470).\n",
      "Training loss for epoch 18: 4.877997240777743, Validation loss: 5.7720474650462466\n",
      "Validation loss decreased (5.772047 --> 5.772047).\n",
      "Training loss for epoch 19: 4.877997240777743, Validation loss: 5.613061239844875\n",
      "Validation loss decreased (5.613061 --> 5.613061).\n",
      "Training loss for epoch 20: 4.797340623667983, Validation loss: 5.488726761937142\n",
      "Validation loss decreased (5.488727 --> 5.488727).\n",
      "Training loss for epoch 21: 4.764986626563534, Validation loss: 5.350025452318645\n",
      "Validation loss decreased (5.350025 --> 5.350025).\n",
      "Training loss for epoch 22: 4.732596283867245, Validation loss: 5.217784998091784\n",
      "Validation loss decreased (5.217785 --> 5.217785).\n",
      "Training loss for epoch 23: 4.712282087653875, Validation loss: 5.0929310943769375\n",
      "Validation loss decreased (5.092931 --> 5.092931).\n",
      "Training loss for epoch 24: 4.712282087653875, Validation loss: 4.987525763610999\n",
      "Validation loss decreased (4.987526 --> 4.987526).\n",
      "Training loss for epoch 25: 4.57581952214241, Validation loss: 4.89227071762085\n",
      "Validation loss decreased (4.892271 --> 4.892271).\n",
      "Duration Time:517.1715536117554\n",
      "Experiment 12 completed in 517.19 seconds.\n",
      "Experiment 13/32: Testing parameters: hidden_dim=128, learning_rate=0.001, batch_size=64, optimizer=SGD, num_epochs=20\n",
      "Training loss for epoch 1: nan, Validation loss: 8.94209098815918\n",
      "New best score (8.942091).\n",
      "Training loss for epoch 2: 8.94264030456543, Validation loss: 8.941029787063599\n",
      "Validation loss decreased (8.941030 --> 8.941030).\n",
      "Training loss for epoch 3: 8.941258112589518, Validation loss: 8.94030491511027\n",
      "Validation loss decreased (8.940305 --> 8.940305).\n",
      "Training loss for epoch 4: 8.94067907333374, Validation loss: 8.939721584320068\n",
      "Validation loss decreased (8.939722 --> 8.939722).\n",
      "Training loss for epoch 5: 8.940134811401368, Validation loss: 8.939123821258544\n",
      "Validation loss decreased (8.939124 --> 8.939124).\n",
      "Training loss for epoch 6: 8.939171518598284, Validation loss: 8.938388586044312\n",
      "Validation loss decreased (8.938389 --> 8.938389).\n",
      "Training loss for epoch 7: 8.938125716315376, Validation loss: 8.937626361846924\n",
      "Validation loss decreased (8.937626 --> 8.937626).\n",
      "Training loss for epoch 8: 8.937203233892268, Validation loss: 8.93679815530777\n",
      "Validation loss decreased (8.936798 --> 8.936798).\n",
      "Training loss for epoch 9: 8.937203233892268, Validation loss: 8.936125755310059\n",
      "Validation loss decreased (8.936126 --> 8.936126).\n",
      "Training loss for epoch 10: 8.936607440312704, Validation loss: 8.935528707504272\n",
      "Validation loss decreased (8.935529 --> 8.935529).\n",
      "Training loss for epoch 11: 8.936019604022686, Validation loss: 8.934896078976719\n",
      "Validation loss decreased (8.934896 --> 8.934896).\n",
      "Training loss for epoch 12: 8.934956868489584, Validation loss: 8.934270858764648\n",
      "Validation loss decreased (8.934271 --> 8.934271).\n",
      "Training loss for epoch 13: 8.934475362300873, Validation loss: 8.933634317838228\n",
      "Validation loss decreased (8.933634 --> 8.933634).\n",
      "Training loss for epoch 14: 8.933514912923178, Validation loss: 8.932959999356951\n",
      "Validation loss decreased (8.932960 --> 8.932960).\n",
      "Training loss for epoch 15: 8.933514912923178, Validation loss: 8.932363192240397\n",
      "Validation loss decreased (8.932363 --> 8.932363).\n",
      "Training loss for epoch 16: 8.933041120830335, Validation loss: 8.931797564029694\n",
      "Validation loss decreased (8.931798 --> 8.931798).\n",
      "Training loss for epoch 17: 8.932076227097284, Validation loss: 8.931174979490393\n",
      "Validation loss decreased (8.931175 --> 8.931175).\n",
      "Training loss for epoch 18: 8.932076227097284, Validation loss: 8.930621571011013\n",
      "Validation loss decreased (8.930622 --> 8.930622).\n",
      "Training loss for epoch 19: 8.931105074675187, Validation loss: 8.930041689621774\n",
      "Validation loss decreased (8.930042 --> 8.930042).\n",
      "Training loss for epoch 20: 8.930605371793112, Validation loss: 8.929459762573241\n",
      "Validation loss decreased (8.929460 --> 8.929460).\n",
      "Duration Time:406.50380969047546\n",
      "Experiment 13 completed in 406.56 seconds.\n",
      "Experiment 14/32: Testing parameters: hidden_dim=128, learning_rate=0.001, batch_size=64, optimizer=SGD, num_epochs=25\n",
      "Training loss for epoch 1: 8.905458450317383, Validation loss: 8.906559944152832\n",
      "New best score (8.906560).\n",
      "Training loss for epoch 2: 8.905458450317383, Validation loss: 8.90656590461731\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 3: 8.90467643737793, Validation loss: 8.905889670054117\n",
      "Validation loss decreased (8.905890 --> 8.905890).\n",
      "Training loss for epoch 4: 8.90467643737793, Validation loss: 8.905550003051758\n",
      "Validation loss decreased (8.905550 --> 8.905550).\n",
      "Training loss for epoch 5: 8.90467643737793, Validation loss: 8.905188655853271\n",
      "Validation loss decreased (8.905189 --> 8.905189).\n",
      "Training loss for epoch 6: 8.9045045035226, Validation loss: 8.904545068740845\n",
      "Validation loss decreased (8.904545 --> 8.904545).\n",
      "Training loss for epoch 7: 8.903214984469944, Validation loss: 8.903637681688581\n",
      "Validation loss decreased (8.903638 --> 8.903638).\n",
      "Training loss for epoch 8: 8.903214984469944, Validation loss: 8.902827560901642\n",
      "Validation loss decreased (8.902828 --> 8.902828).\n",
      "Training loss for epoch 9: 8.903214984469944, Validation loss: 8.902210235595703\n",
      "Validation loss decreased (8.902210 --> 8.902210).\n",
      "Training loss for epoch 10: 8.903214984469944, Validation loss: 8.90179123878479\n",
      "Validation loss decreased (8.901791 --> 8.901791).\n",
      "Training loss for epoch 11: 8.903214984469944, Validation loss: 8.901451631025834\n",
      "Validation loss decreased (8.901452 --> 8.901452).\n",
      "Training loss for epoch 12: 8.902883625030517, Validation loss: 8.90110170841217\n",
      "Validation loss decreased (8.901102 --> 8.901102).\n",
      "Training loss for epoch 13: 8.900704247610909, Validation loss: 8.900642578418438\n",
      "Validation loss decreased (8.900643 --> 8.900643).\n",
      "Training loss for epoch 14: 8.900354258219402, Validation loss: 8.899993760245186\n",
      "Validation loss decreased (8.899994 --> 8.899994).\n",
      "Training loss for epoch 15: 8.89981323480606, Validation loss: 8.899373849232992\n",
      "Validation loss decreased (8.899374 --> 8.899374).\n",
      "Training loss for epoch 16: 8.899109790199681, Validation loss: 8.898739129304886\n",
      "Validation loss decreased (8.898739 --> 8.898739).\n",
      "Training loss for epoch 17: 8.899109790199681, Validation loss: 8.89818415922277\n",
      "Validation loss decreased (8.898184 --> 8.898184).\n",
      "Training loss for epoch 18: 8.89848256111145, Validation loss: 8.897572570376926\n",
      "Validation loss decreased (8.897573 --> 8.897573).\n",
      "Training loss for epoch 19: 8.897084153216818, Validation loss: 8.896879045586838\n",
      "Validation loss decreased (8.896879 --> 8.896879).\n",
      "Training loss for epoch 20: 8.897084153216818, Validation loss: 8.896255111694336\n",
      "Validation loss decreased (8.896255 --> 8.896255).\n",
      "Training loss for epoch 21: 8.897084153216818, Validation loss: 8.895747979482016\n",
      "Validation loss decreased (8.895748 --> 8.895748).\n",
      "Training loss for epoch 22: 8.897084153216818, Validation loss: 8.89529065652327\n",
      "Validation loss decreased (8.895291 --> 8.895291).\n",
      "Training loss for epoch 23: 8.897084153216818, Validation loss: 8.894871690998906\n",
      "Validation loss decreased (8.894872 --> 8.894872).\n",
      "Training loss for epoch 24: 8.89605037689209, Validation loss: 8.894353866577148\n",
      "Validation loss decreased (8.894354 --> 8.894354).\n",
      "Training loss for epoch 25: 8.89605037689209, Validation loss: 8.893879985809326\n",
      "Validation loss decreased (8.893880 --> 8.893880).\n",
      "Duration Time:492.9946038722992\n",
      "Experiment 14 completed in 493.01 seconds.\n",
      "Experiment 15/32: Testing parameters: hidden_dim=128, learning_rate=0.001, batch_size=64, optimizer=Adam, num_epochs=20\n",
      "Training loss for epoch 1: 8.977917671203613, Validation loss: 8.92628002166748\n",
      "New best score (8.926280).\n",
      "Training loss for epoch 2: 8.94386911392212, Validation loss: 8.894041538238525\n",
      "Validation loss decreased (8.894042 --> 8.894042).\n",
      "Training loss for epoch 3: 8.94386911392212, Validation loss: 8.881969928741455\n",
      "Validation loss decreased (8.881970 --> 8.881970).\n",
      "Training loss for epoch 4: 8.94386911392212, Validation loss: 8.875406384468079\n",
      "Validation loss decreased (8.875406 --> 8.875406).\n",
      "Training loss for epoch 5: 8.925863647460938, Validation loss: 8.864220333099365\n",
      "Validation loss decreased (8.864220 --> 8.864220).\n",
      "Training loss for epoch 6: 8.909209251403809, Validation loss: 8.849827607472738\n",
      "Validation loss decreased (8.849828 --> 8.849828).\n",
      "Training loss for epoch 7: 8.909209251403809, Validation loss: 8.839072499956403\n",
      "Validation loss decreased (8.839072 --> 8.839072).\n",
      "Training loss for epoch 8: 8.87039315700531, Validation loss: 8.817717611789703\n",
      "Validation loss decreased (8.817718 --> 8.817718).\n",
      "Training loss for epoch 9: 8.87039315700531, Validation loss: 8.80118359459771\n",
      "Validation loss decreased (8.801184 --> 8.801184).\n",
      "Training loss for epoch 10: 8.847410202026367, Validation loss: 8.781931829452514\n",
      "Validation loss decreased (8.781932 --> 8.781932).\n",
      "Training loss for epoch 11: 8.796022761951793, Validation loss: 8.748499826951461\n",
      "Validation loss decreased (8.748500 --> 8.748500).\n",
      "Training loss for epoch 12: 8.796022761951793, Validation loss: 8.724039872487387\n",
      "Validation loss decreased (8.724040 --> 8.724040).\n",
      "Training loss for epoch 13: 8.796022761951793, Validation loss: 8.700211891761192\n",
      "Validation loss decreased (8.700212 --> 8.700212).\n",
      "Training loss for epoch 14: 8.763240973154703, Validation loss: 8.671357870101929\n",
      "Validation loss decreased (8.671358 --> 8.671358).\n",
      "Training loss for epoch 15: 8.723705145028921, Validation loss: 8.635038661956788\n",
      "Validation loss decreased (8.635039 --> 8.635039).\n",
      "Training loss for epoch 16: 8.557434856891632, Validation loss: 8.547054007649422\n",
      "Validation loss decreased (8.547054 --> 8.547054).\n",
      "Training loss for epoch 17: 8.557434856891632, Validation loss: 8.469080335953656\n",
      "Validation loss decreased (8.469080 --> 8.469080).\n",
      "Training loss for epoch 18: 8.094292330741883, Validation loss: 8.203273269865248\n",
      "Validation loss decreased (8.203273 --> 8.203273).\n",
      "Training loss for epoch 19: 8.094292330741883, Validation loss: 7.965605710682116\n",
      "Validation loss decreased (7.965606 --> 7.965606).\n",
      "Training loss for epoch 20: 8.094292330741883, Validation loss: 7.751956057548523\n",
      "Validation loss decreased (7.751956 --> 7.751956).\n",
      "Duration Time:387.66651582717896\n",
      "Experiment 15 completed in 387.69 seconds.\n",
      "Experiment 16/32: Testing parameters: hidden_dim=128, learning_rate=0.001, batch_size=64, optimizer=Adam, num_epochs=25\n",
      "Training loss for epoch 1: nan, Validation loss: 8.880299091339111\n",
      "New best score (8.880299).\n",
      "Training loss for epoch 2: 8.84801959991455, Validation loss: 8.837479591369629\n",
      "Validation loss decreased (8.837480 --> 8.837480).\n",
      "Training loss for epoch 3: 8.84801959991455, Validation loss: 8.81932020187378\n",
      "Validation loss decreased (8.819320 --> 8.819320).\n",
      "Training loss for epoch 4: 8.801971912384033, Validation loss: 8.786940932273865\n",
      "Validation loss decreased (8.786941 --> 8.786941).\n",
      "Training loss for epoch 5: 8.801971912384033, Validation loss: 8.764345073699952\n",
      "Validation loss decreased (8.764345 --> 8.764345).\n",
      "Training loss for epoch 6: 8.76880931854248, Validation loss: 8.734377463658651\n",
      "Validation loss decreased (8.734377 --> 8.734377).\n",
      "Training loss for epoch 7: 8.745222091674805, Validation loss: 8.701600619724818\n",
      "Validation loss decreased (8.701601 --> 8.701601).\n",
      "Training loss for epoch 8: 8.731126594543458, Validation loss: 8.668779015541077\n",
      "Validation loss decreased (8.668779 --> 8.668779).\n",
      "Training loss for epoch 9: 8.702550021084873, Validation loss: 8.632071442074245\n",
      "Validation loss decreased (8.632071 --> 8.632071).\n",
      "Training loss for epoch 10: 8.702550021084873, Validation loss: 8.602794933319093\n",
      "Validation loss decreased (8.602795 --> 8.602795).\n",
      "Training loss for epoch 11: 8.702550021084873, Validation loss: 8.580092126672918\n",
      "Validation loss decreased (8.580092 --> 8.580092).\n",
      "Training loss for epoch 12: 8.702550021084873, Validation loss: 8.557193398475647\n",
      "Validation loss decreased (8.557193 --> 8.557193).\n",
      "Training loss for epoch 13: 8.668605407079061, Validation loss: 8.5270980321444\n",
      "Validation loss decreased (8.527098 --> 8.527098).\n",
      "Training loss for epoch 14: 8.668605407079061, Validation loss: 8.50124979019165\n",
      "Validation loss decreased (8.501250 --> 8.501250).\n",
      "Training loss for epoch 15: 8.629368195166954, Validation loss: 8.466629489262898\n",
      "Validation loss decreased (8.466629 --> 8.466629).\n",
      "Training loss for epoch 16: 8.629368195166954, Validation loss: 8.43620739877224\n",
      "Validation loss decreased (8.436207 --> 8.436207).\n",
      "Training loss for epoch 17: 8.594146728515625, Validation loss: 8.400088702931123\n",
      "Validation loss decreased (8.400089 --> 8.400089).\n",
      "Training loss for epoch 18: 8.534823926289876, Validation loss: 8.344091044531929\n",
      "Validation loss decreased (8.344091 --> 8.344091).\n",
      "Training loss for epoch 19: 8.129728216873971, Validation loss: 8.134039100847746\n",
      "Validation loss decreased (8.134039 --> 8.134039).\n",
      "Training loss for epoch 20: 7.730681498845418, Validation loss: 7.872433310747146\n",
      "Validation loss decreased (7.872433 --> 7.872433).\n",
      "Training loss for epoch 21: 7.484754974191839, Validation loss: 7.644253991899037\n",
      "Validation loss decreased (7.644254 --> 7.644254).\n",
      "Training loss for epoch 22: 7.110264639059703, Validation loss: 7.466083873401988\n",
      "Validation loss decreased (7.466084 --> 7.466084).\n",
      "Training loss for epoch 23: 6.4737649621634645, Validation loss: 7.2703773975372314\n",
      "Validation loss decreased (7.270377 --> 7.270377).\n",
      "Training loss for epoch 24: 6.4737649621634645, Validation loss: 7.0920932938655215\n",
      "Validation loss decreased (7.092093 --> 7.092093).\n",
      "Training loss for epoch 25: 6.245702705075664, Validation loss: 6.913232436180115\n",
      "Validation loss decreased (6.913232 --> 6.913232).\n",
      "Duration Time:535.622150182724\n",
      "Experiment 16 completed in 535.64 seconds.\n",
      "Experiment 17/32: Testing parameters: hidden_dim=256, learning_rate=1e-05, batch_size=32, optimizer=SGD, num_epochs=20\n",
      "Training loss for epoch 1: 8.914505004882812, Validation loss: 8.91103744506836\n",
      "New best score (8.911037).\n",
      "Training loss for epoch 2: 8.915080388387045, Validation loss: 8.910993099212646\n",
      "Validation loss decreased (8.910993 --> 8.910993).\n",
      "Training loss for epoch 3: 8.915080388387045, Validation loss: 8.910788536071777\n",
      "Validation loss decreased (8.910789 --> 8.910789).\n",
      "Training loss for epoch 4: 8.913390874862671, Validation loss: 8.911022961139679\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 5: 8.912718582153321, Validation loss: 8.91116909980774\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 6: 8.911505017961774, Validation loss: 8.910883903503418\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Training loss for epoch 7: 8.911029696464539, Validation loss: 8.91114602770124\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Training loss for epoch 8: 8.911029696464539, Validation loss: 8.911123722791672\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Duration Time:124.81593298912048\n",
      "Experiment 17 completed in 124.84 seconds.\n",
      "Experiment 18/32: Testing parameters: hidden_dim=256, learning_rate=1e-05, batch_size=32, optimizer=SGD, num_epochs=25\n",
      "Training loss for epoch 1: 8.920320510864258, Validation loss: 8.91985535621643\n",
      "New best score (8.919855).\n",
      "Training loss for epoch 2: 8.921028137207031, Validation loss: 8.920472621917725\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 3: 8.920886834462484, Validation loss: 8.920309066772461\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 4: 8.920546395438057, Validation loss: 8.920424401760101\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Training loss for epoch 5: 8.920747661590577, Validation loss: 8.920406770706176\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Training loss for epoch 6: 8.920416895548502, Validation loss: 8.920320351918539\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Duration Time:116.54527640342712\n",
      "Experiment 18 completed in 116.57 seconds.\n",
      "Experiment 19/32: Testing parameters: hidden_dim=256, learning_rate=1e-05, batch_size=32, optimizer=Adam, num_epochs=20\n",
      "Training loss for epoch 1: 8.905172348022461, Validation loss: 8.903477907180786\n",
      "New best score (8.903478).\n",
      "Training loss for epoch 2: 8.90366236368815, Validation loss: 8.903168439865112\n",
      "Validation loss decreased (8.903168 --> 8.903168).\n",
      "Training loss for epoch 3: 8.903349161148071, Validation loss: 8.903138717015585\n",
      "Validation loss decreased (8.903139 --> 8.903139).\n",
      "Training loss for epoch 4: 8.90275796254476, Validation loss: 8.902829229831696\n",
      "Validation loss decreased (8.902829 --> 8.902829).\n",
      "Training loss for epoch 5: 8.902310180664063, Validation loss: 8.9023588180542\n",
      "Validation loss decreased (8.902359 --> 8.902359).\n",
      "Training loss for epoch 6: 8.902310180664063, Validation loss: 8.902013262112936\n",
      "Validation loss decreased (8.902013 --> 8.902013).\n",
      "Training loss for epoch 7: 8.90136298011331, Validation loss: 8.901144198008947\n",
      "Validation loss decreased (8.901144 --> 8.901144).\n",
      "Training loss for epoch 8: 8.900625419616699, Validation loss: 8.900567889213562\n",
      "Validation loss decreased (8.900568 --> 8.900568).\n",
      "Training loss for epoch 9: 8.90006979306539, Validation loss: 8.899629619386461\n",
      "Validation loss decreased (8.899630 --> 8.899630).\n",
      "Training loss for epoch 10: 8.899852202488827, Validation loss: 8.898808598518372\n",
      "Validation loss decreased (8.898809 --> 8.898809).\n",
      "Training loss for epoch 11: 8.899923748440212, Validation loss: 8.898381449959494\n",
      "Validation loss decreased (8.898381 --> 8.898381).\n",
      "Training loss for epoch 12: 8.899296329867456, Validation loss: 8.897737503051758\n",
      "Validation loss decreased (8.897738 --> 8.897738).\n",
      "Training loss for epoch 13: 8.897713715689523, Validation loss: 8.89690270790687\n",
      "Validation loss decreased (8.896903 --> 8.896903).\n",
      "Training loss for epoch 14: 8.896695494651794, Validation loss: 8.896001424108233\n",
      "Validation loss decreased (8.896001 --> 8.896001).\n",
      "Training loss for epoch 15: 8.895754207264293, Validation loss: 8.89509259859721\n",
      "Validation loss decreased (8.895093 --> 8.895093).\n",
      "Training loss for epoch 16: 8.89344936150771, Validation loss: 8.894219607114792\n",
      "Validation loss decreased (8.894220 --> 8.894220).\n",
      "Training loss for epoch 17: 8.893269962734646, Validation loss: 8.893171618966495\n",
      "Validation loss decreased (8.893172 --> 8.893172).\n",
      "Training loss for epoch 18: 8.892837628451261, Validation loss: 8.892414225472344\n",
      "Validation loss decreased (8.892414 --> 8.892414).\n",
      "Training loss for epoch 19: 8.891935526314429, Validation loss: 8.891548708865518\n",
      "Validation loss decreased (8.891549 --> 8.891549).\n",
      "Training loss for epoch 20: 8.890643926767202, Validation loss: 8.890522015094756\n",
      "Validation loss decreased (8.890522 --> 8.890522).\n",
      "Duration Time:431.9007902145386\n",
      "Experiment 19 completed in 431.93 seconds.\n",
      "Experiment 20/32: Testing parameters: hidden_dim=256, learning_rate=1e-05, batch_size=32, optimizer=Adam, num_epochs=25\n",
      "Training loss for epoch 1: nan, Validation loss: 8.964091300964355\n",
      "New best score (8.964091).\n",
      "Training loss for epoch 2: 8.967874526977539, Validation loss: 8.964316964149475\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 3: 8.968255678812662, Validation loss: 8.964316288630167\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 4: 8.962089750501844, Validation loss: 8.963118851184845\n",
      "Validation loss decreased (8.963119 --> 8.963119).\n",
      "Training loss for epoch 5: 8.961356856606223, Validation loss: 8.962039279937745\n",
      "Validation loss decreased (8.962039 --> 8.962039).\n",
      "Training loss for epoch 6: 8.961166790553502, Validation loss: 8.961150685946146\n",
      "Validation loss decreased (8.961151 --> 8.961151).\n",
      "Training loss for epoch 7: 8.961166790553502, Validation loss: 8.960651568004064\n",
      "Validation loss decreased (8.960652 --> 8.960652).\n",
      "Training loss for epoch 8: 8.961166790553502, Validation loss: 8.960158079862595\n",
      "Validation loss decreased (8.960158 --> 8.960158).\n",
      "Training loss for epoch 9: 8.959924030303956, Validation loss: 8.959618435965645\n",
      "Validation loss decreased (8.959618 --> 8.959618).\n",
      "Training loss for epoch 10: 8.959568659464518, Validation loss: 8.958965539932251\n",
      "Validation loss decreased (8.958966 --> 8.958966).\n",
      "Training loss for epoch 11: 8.95906420548757, Validation loss: 8.95853929086165\n",
      "Validation loss decreased (8.958539 --> 8.958539).\n",
      "Training loss for epoch 12: 8.958794441223144, Validation loss: 8.958160360654196\n",
      "Validation loss decreased (8.958160 --> 8.958160).\n",
      "Training loss for epoch 13: 8.958652284410265, Validation loss: 8.957530957001905\n",
      "Validation loss decreased (8.957531 --> 8.957531).\n",
      "Training loss for epoch 14: 8.957890192667643, Validation loss: 8.957087482724871\n",
      "Validation loss decreased (8.957087 --> 8.957087).\n",
      "Training loss for epoch 15: 8.957890192667643, Validation loss: 8.956691614786784\n",
      "Validation loss decreased (8.956692 --> 8.956692).\n",
      "Training loss for epoch 16: 8.955578255653382, Validation loss: 8.95609401166439\n",
      "Validation loss decreased (8.956094 --> 8.956094).\n",
      "Training loss for epoch 17: 8.954759065494981, Validation loss: 8.955362291897044\n",
      "Validation loss decreased (8.955362 --> 8.955362).\n",
      "Training loss for epoch 18: 8.954040320023246, Validation loss: 8.954660336176554\n",
      "Validation loss decreased (8.954660 --> 8.954660).\n",
      "Training loss for epoch 19: 8.953676081718282, Validation loss: 8.95398645651968\n",
      "Validation loss decreased (8.953986 --> 8.953986).\n",
      "Training loss for epoch 20: 8.952924429201612, Validation loss: 8.953329932689666\n",
      "Validation loss decreased (8.953330 --> 8.953330).\n",
      "Training loss for epoch 21: 8.951260254300873, Validation loss: 8.952557438895816\n",
      "Validation loss decreased (8.952557 --> 8.952557).\n",
      "Training loss for epoch 22: 8.950564650238537, Validation loss: 8.95180094242096\n",
      "Validation loss decreased (8.951801 --> 8.951801).\n",
      "Training loss for epoch 23: 8.950564650238537, Validation loss: 8.951108310533607\n",
      "Validation loss decreased (8.951108 --> 8.951108).\n",
      "Training loss for epoch 24: 8.95010279095362, Validation loss: 8.950469066699346\n",
      "Validation loss decreased (8.950469 --> 8.950469).\n",
      "Training loss for epoch 25: 8.949581674429087, Validation loss: 8.949820613861084\n",
      "Validation loss decreased (8.949821 --> 8.949821).\n",
      "Duration Time:504.9217393398285\n",
      "Experiment 20 completed in 504.95 seconds.\n",
      "Experiment 21/32: Testing parameters: hidden_dim=256, learning_rate=1e-05, batch_size=64, optimizer=SGD, num_epochs=20\n",
      "Training loss for epoch 1: 8.9454927444458, Validation loss: 8.944092750549316\n",
      "New best score (8.944093).\n",
      "Training loss for epoch 2: 8.94352650642395, Validation loss: 8.943914413452148\n",
      "Validation loss decreased (8.943914 --> 8.943914).\n",
      "Training loss for epoch 3: 8.94352650642395, Validation loss: 8.943854331970215\n",
      "Validation loss decreased (8.943854 --> 8.943854).\n",
      "Training loss for epoch 4: 8.944048722585043, Validation loss: 8.943910241127014\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 5: 8.943959355354309, Validation loss: 8.94369134902954\n",
      "Validation loss decreased (8.943691 --> 8.943691).\n",
      "Training loss for epoch 6: 8.943959355354309, Validation loss: 8.94374966621399\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 7: 8.943964767456055, Validation loss: 8.943789005279541\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 8: 8.943964767456055, Validation loss: 8.94382494688034\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Training loss for epoch 9: 8.943844708529385, Validation loss: 8.943715413411459\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Training loss for epoch 10: 8.943844708529385, Validation loss: 8.943751907348632\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Duration Time:210.03877449035645\n",
      "Experiment 21 completed in 210.08 seconds.\n",
      "Experiment 22/32: Testing parameters: hidden_dim=256, learning_rate=1e-05, batch_size=64, optimizer=SGD, num_epochs=25\n",
      "Training loss for epoch 1: 8.966019630432129, Validation loss: 8.968971729278564\n",
      "New best score (8.968972).\n",
      "Training loss for epoch 2: 8.966019630432129, Validation loss: 8.968972206115723\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 3: 8.966019630432129, Validation loss: 8.968613624572754\n",
      "Validation loss decreased (8.968614 --> 8.968614).\n",
      "Training loss for epoch 4: 8.96731948852539, Validation loss: 8.968199610710144\n",
      "Validation loss decreased (8.968200 --> 8.968200).\n",
      "Training loss for epoch 5: 8.96731948852539, Validation loss: 8.968352222442627\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 6: 8.96731948852539, Validation loss: 8.967896382013956\n",
      "Validation loss decreased (8.967896 --> 8.967896).\n",
      "Training loss for epoch 7: 8.966222763061523, Validation loss: 8.967888287135533\n",
      "Validation loss decreased (8.967888 --> 8.967888).\n",
      "Training loss for epoch 8: 8.966720581054688, Validation loss: 8.968011498451233\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 9: 8.96698488507952, Validation loss: 8.967829280429417\n",
      "Validation loss decreased (8.967829 --> 8.967829).\n",
      "Training loss for epoch 10: 8.966286235385471, Validation loss: 8.967934465408325\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 11: 8.966697216033936, Validation loss: 8.968021176078103\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Training loss for epoch 12: 8.966697216033936, Validation loss: 8.968005895614624\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Training loss for epoch 13: 8.967292052048903, Validation loss: 8.96807281787579\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Training loss for epoch 14: 8.967292052048903, Validation loss: 8.967891829354423\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Duration Time:273.9493262767792\n",
      "Experiment 22 completed in 273.98 seconds.\n",
      "Experiment 23/32: Testing parameters: hidden_dim=256, learning_rate=1e-05, batch_size=64, optimizer=Adam, num_epochs=20\n",
      "Training loss for epoch 1: 8.941123962402344, Validation loss: 8.945696353912354\n",
      "New best score (8.945696).\n",
      "Training loss for epoch 2: 8.946249961853027, Validation loss: 8.945198059082031\n",
      "Validation loss decreased (8.945198 --> 8.945198).\n",
      "Training loss for epoch 3: 8.945042473929268, Validation loss: 8.944866180419922\n",
      "Validation loss decreased (8.944866 --> 8.944866).\n",
      "Training loss for epoch 4: 8.945181131362915, Validation loss: 8.944521069526672\n",
      "Validation loss decreased (8.944521 --> 8.944521).\n",
      "Training loss for epoch 5: 8.945037206013998, Validation loss: 8.94403772354126\n",
      "Validation loss decreased (8.944038 --> 8.944038).\n",
      "Training loss for epoch 6: 8.945037206013998, Validation loss: 8.943643490473429\n",
      "Validation loss decreased (8.943643 --> 8.943643).\n",
      "Training loss for epoch 7: 8.945107173919677, Validation loss: 8.943493366241455\n",
      "Validation loss decreased (8.943493 --> 8.943493).\n",
      "Training loss for epoch 8: 8.945001602172852, Validation loss: 8.943382263183594\n",
      "Validation loss decreased (8.943382 --> 8.943382).\n",
      "Training loss for epoch 9: 8.944633997403658, Validation loss: 8.943162600199381\n",
      "Validation loss decreased (8.943163 --> 8.943163).\n",
      "Training loss for epoch 10: 8.944633997403658, Validation loss: 8.942936372756957\n",
      "Validation loss decreased (8.942936 --> 8.942936).\n",
      "Training loss for epoch 11: 8.94398110707601, Validation loss: 8.942715514789928\n",
      "Validation loss decreased (8.942716 --> 8.942716).\n",
      "Training loss for epoch 12: 8.94398110707601, Validation loss: 8.942529082298279\n",
      "Validation loss decreased (8.942529 --> 8.942529).\n",
      "Training loss for epoch 13: 8.943616509437561, Validation loss: 8.94232100706834\n",
      "Validation loss decreased (8.942321 --> 8.942321).\n",
      "Training loss for epoch 14: 8.943616509437561, Validation loss: 8.942110402243477\n",
      "Validation loss decreased (8.942110 --> 8.942110).\n",
      "Training loss for epoch 15: 8.943616509437561, Validation loss: 8.941846879323323\n",
      "Validation loss decreased (8.941847 --> 8.941847).\n",
      "Training loss for epoch 16: 8.943616509437561, Validation loss: 8.941720396280289\n",
      "Validation loss decreased (8.941720 --> 8.941720).\n",
      "Training loss for epoch 17: 8.943187130822075, Validation loss: 8.941555864670697\n",
      "Validation loss decreased (8.941556 --> 8.941556).\n",
      "Training loss for epoch 18: 8.943187130822075, Validation loss: 8.94133053885566\n",
      "Validation loss decreased (8.941331 --> 8.941331).\n",
      "Training loss for epoch 19: 8.943187130822075, Validation loss: 8.94120394556146\n",
      "Validation loss decreased (8.941204 --> 8.941204).\n",
      "Training loss for epoch 20: 8.943187130822075, Validation loss: 8.94106891155243\n",
      "Validation loss decreased (8.941069 --> 8.941069).\n",
      "Duration Time:373.588928937912\n",
      "Experiment 23 completed in 373.62 seconds.\n",
      "Experiment 24/32: Testing parameters: hidden_dim=256, learning_rate=1e-05, batch_size=64, optimizer=Adam, num_epochs=25\n",
      "Training loss for epoch 1: 8.96853256225586, Validation loss: 8.973280429840088\n",
      "New best score (8.973280).\n",
      "Training loss for epoch 2: 8.968879890441894, Validation loss: 8.972853660583496\n",
      "Validation loss decreased (8.972854 --> 8.972854).\n",
      "Training loss for epoch 3: 8.97087151663644, Validation loss: 8.97230577468872\n",
      "Validation loss decreased (8.972306 --> 8.972306).\n",
      "Training loss for epoch 4: 8.969906912909615, Validation loss: 8.970174431800842\n",
      "Validation loss decreased (8.970174 --> 8.970174).\n",
      "Training loss for epoch 5: 8.969906912909615, Validation loss: 8.969998741149903\n",
      "Validation loss decreased (8.969999 --> 8.969999).\n",
      "Training loss for epoch 6: 8.969906912909615, Validation loss: 8.970075368881226\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 7: 8.97044792175293, Validation loss: 8.969885757991246\n",
      "Validation loss decreased (8.969886 --> 8.969886).\n",
      "Training loss for epoch 8: 8.970028400421143, Validation loss: 8.96963232755661\n",
      "Validation loss decreased (8.969632 --> 8.969632).\n",
      "Training loss for epoch 9: 8.970208485921225, Validation loss: 8.969409412807888\n",
      "Validation loss decreased (8.969409 --> 8.969409).\n",
      "Training loss for epoch 10: 8.970377385616302, Validation loss: 8.969057607650758\n",
      "Validation loss decreased (8.969058 --> 8.969058).\n",
      "Training loss for epoch 11: 8.970377385616302, Validation loss: 8.968884641473943\n",
      "Validation loss decreased (8.968885 --> 8.968885).\n",
      "Training loss for epoch 12: 8.970377385616302, Validation loss: 8.968738873799643\n",
      "Validation loss decreased (8.968739 --> 8.968739).\n",
      "Training loss for epoch 13: 8.969650692409939, Validation loss: 8.968466391930214\n",
      "Validation loss decreased (8.968466 --> 8.968466).\n",
      "Training loss for epoch 14: 8.969650692409939, Validation loss: 8.968219927379064\n",
      "Validation loss decreased (8.968220 --> 8.968220).\n",
      "Training loss for epoch 15: 8.969463777542114, Validation loss: 8.967959149678547\n",
      "Validation loss decreased (8.967959 --> 8.967959).\n",
      "Training loss for epoch 16: 8.969463777542114, Validation loss: 8.967774599790573\n",
      "Validation loss decreased (8.967775 --> 8.967775).\n",
      "Training loss for epoch 17: 8.968787690867549, Validation loss: 8.967528062708238\n",
      "Validation loss decreased (8.967528 --> 8.967528).\n",
      "Training loss for epoch 18: 8.968126920553354, Validation loss: 8.967230134540134\n",
      "Validation loss decreased (8.967230 --> 8.967230).\n",
      "Training loss for epoch 19: 8.967915287724248, Validation loss: 8.966941431949014\n",
      "Validation loss decreased (8.966941 --> 8.966941).\n",
      "Training loss for epoch 20: 8.967537057810816, Validation loss: 8.96663281917572\n",
      "Validation loss decreased (8.966633 --> 8.966633).\n",
      "Training loss for epoch 21: 8.967537057810816, Validation loss: 8.966348375592913\n",
      "Validation loss decreased (8.966348 --> 8.966348).\n",
      "Training loss for epoch 22: 8.966992855072021, Validation loss: 8.966025981036099\n",
      "Validation loss decreased (8.966026 --> 8.966026).\n",
      "Training loss for epoch 23: 8.966729048526648, Validation loss: 8.965706597203793\n",
      "Validation loss decreased (8.965707 --> 8.965707).\n",
      "Training loss for epoch 24: 8.966729048526648, Validation loss: 8.965356945991516\n",
      "Validation loss decreased (8.965357 --> 8.965357).\n",
      "Training loss for epoch 25: 8.966379530289593, Validation loss: 8.96507740020752\n",
      "Validation loss decreased (8.965077 --> 8.965077).\n",
      "Duration Time:533.9945452213287\n",
      "Experiment 24 completed in 534.04 seconds.\n",
      "Experiment 25/32: Testing parameters: hidden_dim=256, learning_rate=0.001, batch_size=32, optimizer=SGD, num_epochs=20\n",
      "Training loss for epoch 1: 8.9039580481393, Validation loss: 8.904670000076294\n",
      "New best score (8.904670).\n",
      "Training loss for epoch 2: 8.903290835293857, Validation loss: 8.900828242301941\n",
      "Validation loss decreased (8.900828 --> 8.900828).\n",
      "Training loss for epoch 3: 8.903290835293857, Validation loss: 8.89916475613912\n",
      "Validation loss decreased (8.899165 --> 8.899165).\n",
      "Training loss for epoch 4: 8.901607958475749, Validation loss: 8.898319363594055\n",
      "Validation loss decreased (8.898319 --> 8.898319).\n",
      "Training loss for epoch 5: 8.90096238080193, Validation loss: 8.896714353561402\n",
      "Validation loss decreased (8.896714 --> 8.896714).\n",
      "Training loss for epoch 6: 8.899933865195827, Validation loss: 8.895645062128702\n",
      "Validation loss decreased (8.895645 --> 8.895645).\n",
      "Training loss for epoch 7: 8.89690978438766, Validation loss: 8.89378707749503\n",
      "Validation loss decreased (8.893787 --> 8.893787).\n",
      "Training loss for epoch 8: 8.89690978438766, Validation loss: 8.89325025677681\n",
      "Validation loss decreased (8.893250 --> 8.893250).\n",
      "Training loss for epoch 9: 8.89690978438766, Validation loss: 8.89208854569329\n",
      "Validation loss decreased (8.892089 --> 8.892089).\n",
      "Training loss for epoch 10: 8.896929366247994, Validation loss: 8.891208481788635\n",
      "Validation loss decreased (8.891208 --> 8.891208).\n",
      "Training loss for epoch 11: 8.896635417280526, Validation loss: 8.89061258055947\n",
      "Validation loss decreased (8.890613 --> 8.890613).\n",
      "Training loss for epoch 12: 8.896635417280526, Validation loss: 8.890279014905294\n",
      "Validation loss decreased (8.890279 --> 8.890279).\n",
      "Training loss for epoch 13: 8.895833134651184, Validation loss: 8.88977309373709\n",
      "Validation loss decreased (8.889773 --> 8.889773).\n",
      "Training loss for epoch 14: 8.894740833955652, Validation loss: 8.889395594596863\n",
      "Validation loss decreased (8.889396 --> 8.889396).\n",
      "Training loss for epoch 15: 8.894740833955652, Validation loss: 8.888783280054728\n",
      "Validation loss decreased (8.888783 --> 8.888783).\n",
      "Training loss for epoch 16: 8.893893759591238, Validation loss: 8.888026803731918\n",
      "Validation loss decreased (8.888027 --> 8.888027).\n",
      "Training loss for epoch 17: 8.89324492878384, Validation loss: 8.887712226194495\n",
      "Validation loss decreased (8.887712 --> 8.887712).\n",
      "Training loss for epoch 18: 8.892176326952482, Validation loss: 8.88714411523607\n",
      "Validation loss decreased (8.887144 --> 8.887144).\n",
      "Training loss for epoch 19: 8.89092374983288, Validation loss: 8.886288090756064\n",
      "Validation loss decreased (8.886288 --> 8.886288).\n",
      "Training loss for epoch 20: 8.890361608460893, Validation loss: 8.885588610172272\n",
      "Validation loss decreased (8.885589 --> 8.885589).\n",
      "Duration Time:378.0941183567047\n",
      "Experiment 25 completed in 378.12 seconds.\n",
      "Experiment 26/32: Testing parameters: hidden_dim=256, learning_rate=0.001, batch_size=32, optimizer=SGD, num_epochs=25\n",
      "Training loss for epoch 1: nan, Validation loss: 8.93668508529663\n",
      "New best score (8.936685).\n",
      "Training loss for epoch 2: nan, Validation loss: 8.936816811561584\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 3: 8.935071468353271, Validation loss: 8.935485363006592\n",
      "Validation loss decreased (8.935485 --> 8.935485).\n",
      "Training loss for epoch 4: 8.934277004665798, Validation loss: 8.93427985906601\n",
      "Validation loss decreased (8.934280 --> 8.934280).\n",
      "Training loss for epoch 5: 8.933933067321778, Validation loss: 8.933414745330811\n",
      "Validation loss decreased (8.933415 --> 8.933415).\n",
      "Training loss for epoch 6: 8.933933067321778, Validation loss: 8.932916561762491\n",
      "Validation loss decreased (8.932917 --> 8.932917).\n",
      "Training loss for epoch 7: 8.933933067321778, Validation loss: 8.932497160775322\n",
      "Validation loss decreased (8.932497 --> 8.932497).\n",
      "Training loss for epoch 8: 8.933933067321778, Validation loss: 8.932212322950363\n",
      "Validation loss decreased (8.932212 --> 8.932212).\n",
      "Training loss for epoch 9: 8.933473507563273, Validation loss: 8.93182240592109\n",
      "Validation loss decreased (8.931822 --> 8.931822).\n",
      "Training loss for epoch 10: 8.932686360677083, Validation loss: 8.931333923339844\n",
      "Validation loss decreased (8.931334 --> 8.931334).\n",
      "Training loss for epoch 11: 8.932002965141745, Validation loss: 8.930775664069436\n",
      "Validation loss decreased (8.930776 --> 8.930776).\n",
      "Training loss for epoch 12: 8.931351611488743, Validation loss: 8.930291612943014\n",
      "Validation loss decreased (8.930292 --> 8.930292).\n",
      "Training loss for epoch 13: 8.930130585380223, Validation loss: 8.929601540932289\n",
      "Validation loss decreased (8.929602 --> 8.929602).\n",
      "Training loss for epoch 14: 8.930130585380223, Validation loss: 8.928991947855268\n",
      "Validation loss decreased (8.928992 --> 8.928992).\n",
      "Training loss for epoch 15: 8.92863096509661, Validation loss: 8.92824190457662\n",
      "Validation loss decreased (8.928242 --> 8.928242).\n",
      "Training loss for epoch 16: 8.927341891873267, Validation loss: 8.927405923604965\n",
      "Validation loss decreased (8.927406 --> 8.927406).\n",
      "Training loss for epoch 17: 8.92631373685949, Validation loss: 8.92651860854205\n",
      "Validation loss decreased (8.926519 --> 8.926519).\n",
      "Training loss for epoch 18: 8.923691051762278, Validation loss: 8.925594396061367\n",
      "Validation loss decreased (8.925594 --> 8.925594).\n",
      "Training loss for epoch 19: 8.92333307720366, Validation loss: 8.924660645033184\n",
      "Validation loss decreased (8.924661 --> 8.924661).\n",
      "Training loss for epoch 20: 8.922548445788296, Validation loss: 8.923639881610871\n",
      "Validation loss decreased (8.923640 --> 8.923640).\n",
      "Training loss for epoch 21: 8.922548445788296, Validation loss: 8.922839925402688\n",
      "Validation loss decreased (8.922840 --> 8.922840).\n",
      "Training loss for epoch 22: 8.920216616462259, Validation loss: 8.921884569254788\n",
      "Validation loss decreased (8.921885 --> 8.921885).\n",
      "Training loss for epoch 23: 8.917318607198782, Validation loss: 8.920659749404244\n",
      "Validation loss decreased (8.920660 --> 8.920660).\n",
      "Training loss for epoch 24: 8.914722818316836, Validation loss: 8.919515699148178\n",
      "Validation loss decreased (8.919516 --> 8.919516).\n",
      "Training loss for epoch 25: 8.914265063271595, Validation loss: 8.9182080078125\n",
      "Validation loss decreased (8.918208 --> 8.918208).\n",
      "Duration Time:512.0655429363251\n",
      "Experiment 26 completed in 512.09 seconds.\n",
      "Experiment 27/32: Testing parameters: hidden_dim=256, learning_rate=0.001, batch_size=32, optimizer=Adam, num_epochs=20\n",
      "Training loss for epoch 1: nan, Validation loss: 8.931739807128906\n",
      "New best score (8.931740).\n",
      "Training loss for epoch 2: 8.855819463729858, Validation loss: 8.823134660720825\n",
      "Validation loss decreased (8.823135 --> 8.823135).\n",
      "Training loss for epoch 3: 8.799455801645914, Validation loss: 8.731275876363119\n",
      "Validation loss decreased (8.731276 --> 8.731276).\n",
      "Training loss for epoch 4: 8.639370812310112, Validation loss: 8.517194539308548\n",
      "Validation loss decreased (8.517195 --> 8.517195).\n",
      "Training loss for epoch 5: 8.552548217773438, Validation loss: 8.261974501609803\n",
      "Validation loss decreased (8.261975 --> 8.261975).\n",
      "Training loss for epoch 6: 8.270572463671366, Validation loss: 7.666216591993968\n",
      "Validation loss decreased (7.666217 --> 7.666217).\n",
      "Training loss for epoch 7: 7.333457666284898, Validation loss: 7.033111895833697\n",
      "Validation loss decreased (7.033112 --> 7.033112).\n",
      "Training loss for epoch 8: 7.333457666284898, Validation loss: 6.55664436519146\n",
      "Validation loss decreased (6.556644 --> 6.556644).\n",
      "Training loss for epoch 9: 6.582714217049735, Validation loss: 6.17978095346027\n",
      "Validation loss decreased (6.179781 --> 6.179781).\n",
      "Training loss for epoch 10: 6.204432090123494, Validation loss: 5.846008205413819\n",
      "Validation loss decreased (5.846008 --> 5.846008).\n",
      "Training loss for epoch 11: 6.204432090123494, Validation loss: 5.566272231665525\n",
      "Validation loss decreased (5.566272 --> 5.566272).\n",
      "Training loss for epoch 12: 5.949251092397249, Validation loss: 5.3250984499851866\n",
      "Validation loss decreased (5.325098 --> 5.325098).\n",
      "Training loss for epoch 13: 5.699436868940081, Validation loss: 5.151726236710181\n",
      "Validation loss decreased (5.151726 --> 5.151726).\n",
      "Training loss for epoch 14: 5.642401530824858, Validation loss: 4.991878764969962\n",
      "Validation loss decreased (4.991879 --> 4.991879).\n",
      "Training loss for epoch 15: 5.522894581158956, Validation loss: 4.84290292263031\n",
      "Validation loss decreased (4.842903 --> 4.842903).\n",
      "Training loss for epoch 16: 5.255054849566835, Validation loss: 4.7277363948524\n",
      "Validation loss decreased (4.727736 --> 4.727736).\n",
      "Training loss for epoch 17: 5.192199798191295, Validation loss: 4.6066551839604095\n",
      "Validation loss decreased (4.606655 --> 4.606655).\n",
      "Training loss for epoch 18: 5.132980987003871, Validation loss: 4.500798169109556\n",
      "Validation loss decreased (4.500798 --> 4.500798).\n",
      "Training loss for epoch 19: 4.664228916168213, Validation loss: 4.387981648507871\n",
      "Validation loss decreased (4.387982 --> 4.387982).\n",
      "Training loss for epoch 20: 4.434740924367718, Validation loss: 4.316269083321094\n",
      "Validation loss decreased (4.316269 --> 4.316269).\n",
      "Duration Time:396.63660740852356\n",
      "Experiment 27 completed in 396.67 seconds.\n",
      "Experiment 28/32: Testing parameters: hidden_dim=256, learning_rate=0.001, batch_size=32, optimizer=Adam, num_epochs=25\n",
      "Training loss for epoch 1: 8.89268430074056, Validation loss: 8.774043798446655\n",
      "New best score (8.774044).\n",
      "Training loss for epoch 2: 8.89268430074056, Validation loss: 8.775696039199829\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Training loss for epoch 3: 8.864689588546753, Validation loss: 8.758219480514526\n",
      "Validation loss decreased (8.758219 --> 8.758219).\n",
      "Training loss for epoch 4: 8.763919012887138, Validation loss: 8.654167473316193\n",
      "Validation loss decreased (8.654167 --> 8.654167).\n",
      "Training loss for epoch 5: 8.736596465110779, Validation loss: 8.53865361213684\n",
      "Validation loss decreased (8.538654 --> 8.538654).\n",
      "Training loss for epoch 6: 8.044886589050293, Validation loss: 7.738897939523061\n",
      "Validation loss decreased (7.738898 --> 7.738898).\n",
      "Training loss for epoch 7: 7.918324845177787, Validation loss: 7.210220856325967\n",
      "Validation loss decreased (7.210221 --> 7.210221).\n",
      "Training loss for epoch 8: 7.918324845177787, Validation loss: 6.757843144237995\n",
      "Validation loss decreased (6.757843 --> 6.757843).\n",
      "Training loss for epoch 9: 7.918324845177787, Validation loss: 6.557214948866102\n",
      "Validation loss decreased (6.557215 --> 6.557215).\n",
      "Training loss for epoch 10: 7.58617205619812, Validation loss: 6.328287613391876\n",
      "Validation loss decreased (6.328288 --> 6.328288).\n",
      "Training loss for epoch 11: 7.135099102469051, Validation loss: 6.064791934056715\n",
      "Validation loss decreased (6.064792 --> 6.064792).\n",
      "Training loss for epoch 12: 6.705272122433311, Validation loss: 5.8215572734673815\n",
      "Validation loss decreased (5.821557 --> 5.821557).\n",
      "Training loss for epoch 13: 6.2719280177896675, Validation loss: 5.600041545354403\n",
      "Validation loss decreased (5.600042 --> 5.600042).\n",
      "Training loss for epoch 14: 5.543082730523471, Validation loss: 5.403164761407035\n",
      "Validation loss decreased (5.403165 --> 5.403165).\n",
      "Training loss for epoch 15: 5.33834598910424, Validation loss: 5.233416259288788\n",
      "Validation loss decreased (5.233416 --> 5.233416).\n",
      "Training loss for epoch 16: 5.33834598910424, Validation loss: 5.069121740758419\n",
      "Validation loss decreased (5.069122 --> 5.069122).\n",
      "Training loss for epoch 17: 4.958974822147472, Validation loss: 4.920444814597859\n",
      "Validation loss decreased (4.920445 --> 4.920445).\n",
      "Training loss for epoch 18: 4.958974822147472, Validation loss: 4.7807189126809435\n",
      "Validation loss decreased (4.780719 --> 4.780719).\n",
      "Training loss for epoch 19: 4.5918724807825955, Validation loss: 4.675708091572711\n",
      "Validation loss decreased (4.675708 --> 4.675708).\n",
      "Training loss for epoch 20: 4.553386561075846, Validation loss: 4.567062686383724\n",
      "Validation loss decreased (4.567063 --> 4.567063).\n",
      "Training loss for epoch 21: 4.362386598878977, Validation loss: 4.458227103664761\n",
      "Validation loss decreased (4.458227 --> 4.458227).\n",
      "Training loss for epoch 22: 4.362386598878977, Validation loss: 4.378167121247812\n",
      "Validation loss decreased (4.378167 --> 4.378167).\n",
      "Training loss for epoch 23: 4.296273734055314, Validation loss: 4.2919730971688805\n",
      "Validation loss decreased (4.291973 --> 4.291973).\n",
      "Training loss for epoch 24: 4.269056597581277, Validation loss: 4.226225446909666\n",
      "Validation loss decreased (4.226225 --> 4.226225).\n",
      "Training loss for epoch 25: 4.269056597581277, Validation loss: 4.149018875360489\n",
      "Validation loss decreased (4.149019 --> 4.149019).\n",
      "Duration Time:469.48001050949097\n",
      "Experiment 28 completed in 469.51 seconds.\n",
      "Experiment 29/32: Testing parameters: hidden_dim=256, learning_rate=0.001, batch_size=64, optimizer=SGD, num_epochs=20\n",
      "Training loss for epoch 1: 8.905074119567871, Validation loss: 8.900023460388184\n",
      "New best score (8.900023).\n",
      "Training loss for epoch 2: 8.899734179178873, Validation loss: 8.899293661117554\n",
      "Validation loss decreased (8.899294 --> 8.899294).\n",
      "Training loss for epoch 3: 8.898783874511718, Validation loss: 8.898455142974854\n",
      "Validation loss decreased (8.898455 --> 8.898455).\n",
      "Training loss for epoch 4: 8.898783874511718, Validation loss: 8.897832155227661\n",
      "Validation loss decreased (8.897832 --> 8.897832).\n",
      "Training loss for epoch 5: 8.898551305135092, Validation loss: 8.897723293304443\n",
      "Validation loss decreased (8.897723 --> 8.897723).\n",
      "Training loss for epoch 6: 8.897337595621744, Validation loss: 8.897232929865519\n",
      "Validation loss decreased (8.897233 --> 8.897233).\n",
      "Training loss for epoch 7: 8.89695692062378, Validation loss: 8.896500859941755\n",
      "Validation loss decreased (8.896501 --> 8.896501).\n",
      "Training loss for epoch 8: 8.89603583017985, Validation loss: 8.89562451839447\n",
      "Validation loss decreased (8.895625 --> 8.895625).\n",
      "Training loss for epoch 9: 8.89603583017985, Validation loss: 8.895046393076578\n",
      "Validation loss decreased (8.895046 --> 8.895046).\n",
      "Training loss for epoch 10: 8.89603583017985, Validation loss: 8.89449110031128\n",
      "Validation loss decreased (8.894491 --> 8.894491).\n",
      "Training loss for epoch 11: 8.8953674861363, Validation loss: 8.893950982527299\n",
      "Validation loss decreased (8.893951 --> 8.893951).\n",
      "Training loss for epoch 12: 8.894819005330403, Validation loss: 8.893432021141052\n",
      "Validation loss decreased (8.893432 --> 8.893432).\n",
      "Training loss for epoch 13: 8.894819005330403, Validation loss: 8.893144094027006\n",
      "Validation loss decreased (8.893144 --> 8.893144).\n",
      "Training loss for epoch 14: 8.894819005330403, Validation loss: 8.892672845295497\n",
      "Validation loss decreased (8.892673 --> 8.892673).\n",
      "Training loss for epoch 15: 8.894819005330403, Validation loss: 8.89249979654948\n",
      "Validation loss decreased (8.892500 --> 8.892500).\n",
      "Training loss for epoch 16: 8.89441055059433, Validation loss: 8.892064779996872\n",
      "Validation loss decreased (8.892065 --> 8.892065).\n",
      "Training loss for epoch 17: 8.893658690982395, Validation loss: 8.891639400930966\n",
      "Validation loss decreased (8.891639 --> 8.891639).\n",
      "Training loss for epoch 18: 8.893173117386667, Validation loss: 8.891348282496134\n",
      "Validation loss decreased (8.891348 --> 8.891348).\n",
      "Training loss for epoch 19: 8.893173117386667, Validation loss: 8.890896470923172\n",
      "Validation loss decreased (8.890896 --> 8.890896).\n",
      "Training loss for epoch 20: 8.893173117386667, Validation loss: 8.890604782104493\n",
      "Validation loss decreased (8.890605 --> 8.890605).\n",
      "Duration Time:391.9594967365265\n",
      "Experiment 29 completed in 392.00 seconds.\n",
      "Experiment 30/32: Testing parameters: hidden_dim=256, learning_rate=0.001, batch_size=64, optimizer=SGD, num_epochs=25\n",
      "Training loss for epoch 1: 8.944234848022461, Validation loss: 8.944762229919434\n",
      "New best score (8.944762).\n",
      "Training loss for epoch 2: 8.944831371307373, Validation loss: 8.943408966064453\n",
      "Validation loss decreased (8.943409 --> 8.943409).\n",
      "Training loss for epoch 3: 8.944831371307373, Validation loss: 8.942965666453043\n",
      "Validation loss decreased (8.942966 --> 8.942966).\n",
      "Training loss for epoch 4: 8.944392585754395, Validation loss: 8.942517518997192\n",
      "Validation loss decreased (8.942518 --> 8.942518).\n",
      "Training loss for epoch 5: 8.944392585754395, Validation loss: 8.942203330993653\n",
      "Validation loss decreased (8.942203 --> 8.942203).\n",
      "Training loss for epoch 6: 8.94328512464251, Validation loss: 8.941717068354288\n",
      "Validation loss decreased (8.941717 --> 8.941717).\n",
      "Training loss for epoch 7: 8.942072232564291, Validation loss: 8.941111496516637\n",
      "Validation loss decreased (8.941111 --> 8.941111).\n",
      "Training loss for epoch 8: 8.942072232564291, Validation loss: 8.940682888031006\n",
      "Validation loss decreased (8.940683 --> 8.940683).\n",
      "Training loss for epoch 9: 8.942072232564291, Validation loss: 8.94035334057278\n",
      "Validation loss decreased (8.940353 --> 8.940353).\n",
      "Training loss for epoch 10: 8.941636848449708, Validation loss: 8.939950275421143\n",
      "Validation loss decreased (8.939950 --> 8.939950).\n",
      "Training loss for epoch 11: 8.940820614496866, Validation loss: 8.93943738937378\n",
      "Validation loss decreased (8.939437 --> 8.939437).\n",
      "Training loss for epoch 12: 8.940820614496866, Validation loss: 8.939061522483826\n",
      "Validation loss decreased (8.939062 --> 8.939062).\n",
      "Training loss for epoch 13: 8.939998967306954, Validation loss: 8.938591553614689\n",
      "Validation loss decreased (8.938592 --> 8.938592).\n",
      "Training loss for epoch 14: 8.939545249938964, Validation loss: 8.938127279281616\n",
      "Validation loss decreased (8.938127 --> 8.938127).\n",
      "Training loss for epoch 15: 8.939545249938964, Validation loss: 8.937724844614666\n",
      "Validation loss decreased (8.937725 --> 8.937725).\n",
      "Training loss for epoch 16: 8.938177426656088, Validation loss: 8.937205910682678\n",
      "Validation loss decreased (8.937206 --> 8.937206).\n",
      "Training loss for epoch 17: 8.936378912492232, Validation loss: 8.93653653649723\n",
      "Validation loss decreased (8.936537 --> 8.936537).\n",
      "Training loss for epoch 18: 8.936378912492232, Validation loss: 8.935940795474583\n",
      "Validation loss decreased (8.935941 --> 8.935941).\n",
      "Training loss for epoch 19: 8.936378912492232, Validation loss: 8.935404200302926\n",
      "Validation loss decreased (8.935404 --> 8.935404).\n",
      "Training loss for epoch 20: 8.934975509643555, Validation loss: 8.934779572486878\n",
      "Validation loss decreased (8.934780 --> 8.934780).\n",
      "Training loss for epoch 21: 8.934975509643555, Validation loss: 8.934214115142822\n",
      "Validation loss decreased (8.934214 --> 8.934214).\n",
      "Training loss for epoch 22: 8.934050348069933, Validation loss: 8.93362552469427\n",
      "Validation loss decreased (8.933626 --> 8.933626).\n",
      "Training loss for epoch 23: 8.934050348069933, Validation loss: 8.933087908703348\n",
      "Validation loss decreased (8.933088 --> 8.933088).\n",
      "Training loss for epoch 24: 8.933215141296387, Validation loss: 8.932540655136108\n",
      "Validation loss decreased (8.932541 --> 8.932541).\n",
      "Training loss for epoch 25: 8.932752227783203, Validation loss: 8.931966514587403\n",
      "Validation loss decreased (8.931967 --> 8.931967).\n",
      "Duration Time:529.6068549156189\n",
      "Experiment 30 completed in 529.64 seconds.\n",
      "Experiment 31/32: Testing parameters: hidden_dim=256, learning_rate=0.001, batch_size=64, optimizer=Adam, num_epochs=20\n",
      "Training loss for epoch 1: 8.951780319213867, Validation loss: 8.9060378074646\n",
      "New best score (8.906038).\n",
      "Training loss for epoch 2: 8.928327083587646, Validation loss: 8.877593278884888\n",
      "Validation loss decreased (8.877593 --> 8.877593).\n",
      "Training loss for epoch 3: 8.928327083587646, Validation loss: 8.868088563283285\n",
      "Validation loss decreased (8.868089 --> 8.868089).\n",
      "Training loss for epoch 4: 8.901818911234537, Validation loss: 8.848650574684143\n",
      "Validation loss decreased (8.848651 --> 8.848651).\n",
      "Training loss for epoch 5: 8.901818911234537, Validation loss: 8.838532733917237\n",
      "Validation loss decreased (8.838533 --> 8.838533).\n",
      "Training loss for epoch 6: 8.758120264325823, Validation loss: 8.749336957931519\n",
      "Validation loss decreased (8.749337 --> 8.749337).\n",
      "Training loss for epoch 7: 8.758120264325823, Validation loss: 8.691138676234655\n",
      "Validation loss decreased (8.691139 --> 8.691139).\n",
      "Training loss for epoch 8: 8.758120264325823, Validation loss: 8.642829954624176\n",
      "Validation loss decreased (8.642830 --> 8.642830).\n",
      "Training loss for epoch 9: 8.61886379453871, Validation loss: 8.52185614903768\n",
      "Validation loss decreased (8.521856 --> 8.521856).\n",
      "Training loss for epoch 10: 8.61886379453871, Validation loss: 8.432758665084839\n",
      "Validation loss decreased (8.432759 --> 8.432759).\n",
      "Training loss for epoch 11: 8.449654839255594, Validation loss: 8.148794260892002\n",
      "Validation loss decreased (8.148794 --> 8.148794).\n",
      "Training loss for epoch 12: 8.449654839255594, Validation loss: 7.9188458720843\n",
      "Validation loss decreased (7.918846 --> 7.918846).\n",
      "Training loss for epoch 13: 8.449654839255594, Validation loss: 7.724658874365\n",
      "Validation loss decreased (7.724659 --> 7.724659).\n",
      "Training loss for epoch 14: 7.428531663758414, Validation loss: 7.44255610023226\n",
      "Validation loss decreased (7.442556 --> 7.442556).\n",
      "Training loss for epoch 15: 7.428531663758414, Validation loss: 7.199129970868428\n",
      "Validation loss decreased (7.199130 --> 7.199130).\n",
      "Training loss for epoch 16: 7.188466131687164, Validation loss: 7.016195319592953\n",
      "Validation loss decreased (7.016195 --> 7.016195).\n",
      "Training loss for epoch 17: 7.188466131687164, Validation loss: 6.827339551028083\n",
      "Validation loss decreased (6.827340 --> 6.827340).\n",
      "Training loss for epoch 18: 7.188466131687164, Validation loss: 6.645083592997657\n",
      "Validation loss decreased (6.645084 --> 6.645084).\n",
      "Training loss for epoch 19: 7.188466131687164, Validation loss: 6.508870620476572\n",
      "Validation loss decreased (6.508871 --> 6.508871).\n",
      "Training loss for epoch 20: 6.9750105773701385, Validation loss: 6.34503692984581\n",
      "Validation loss decreased (6.345037 --> 6.345037).\n",
      "Duration Time:377.76052260398865\n",
      "Experiment 31 completed in 377.79 seconds.\n",
      "Experiment 32/32: Testing parameters: hidden_dim=256, learning_rate=0.001, batch_size=64, optimizer=Adam, num_epochs=25\n",
      "Training loss for epoch 1: nan, Validation loss: 8.92015552520752\n",
      "New best score (8.920156).\n",
      "Training loss for epoch 2: 8.917448997497559, Validation loss: 8.891997337341309\n",
      "Validation loss decreased (8.891997 --> 8.891997).\n",
      "Training loss for epoch 3: 8.891546726226807, Validation loss: 8.865638573964437\n",
      "Validation loss decreased (8.865639 --> 8.865639).\n",
      "Training loss for epoch 4: 8.834242105484009, Validation loss: 8.813781976699829\n",
      "Validation loss decreased (8.813782 --> 8.813782).\n",
      "Training loss for epoch 5: 8.834242105484009, Validation loss: 8.781028175354004\n",
      "Validation loss decreased (8.781028 --> 8.781028).\n",
      "Training loss for epoch 6: 8.797436904907226, Validation loss: 8.74068800608317\n",
      "Validation loss decreased (8.740688 --> 8.740688).\n",
      "Training loss for epoch 7: 8.627055525779724, Validation loss: 8.605342081614904\n",
      "Validation loss decreased (8.605342 --> 8.605342).\n",
      "Training loss for epoch 8: 8.541411293877495, Validation loss: 8.443732529878616\n",
      "Validation loss decreased (8.443733 --> 8.443733).\n",
      "Training loss for epoch 9: 8.180232568220658, Validation loss: 7.990300178527832\n",
      "Validation loss decreased (7.990300 --> 7.990300).\n",
      "Training loss for epoch 10: 8.180232568220658, Validation loss: 7.628412199020386\n",
      "Validation loss decreased (7.628412 --> 7.628412).\n",
      "Training loss for epoch 11: 7.604786396026611, Validation loss: 7.237908558412031\n",
      "Validation loss decreased (7.237909 --> 7.237909).\n",
      "Training loss for epoch 12: 6.886300250887871, Validation loss: 6.9111731350421906\n",
      "Validation loss decreased (6.911173 --> 6.911173).\n",
      "Training loss for epoch 13: 6.682942376417272, Validation loss: 6.6089284878510695\n",
      "Validation loss decreased (6.608928 --> 6.608928).\n",
      "Training loss for epoch 14: 6.354562960172954, Validation loss: 6.3240356530462\n",
      "Validation loss decreased (6.324036 --> 6.324036).\n",
      "Training loss for epoch 15: 6.237288594245911, Validation loss: 6.087555662790934\n",
      "Validation loss decreased (6.087556 --> 6.087556).\n",
      "Training loss for epoch 16: 6.237288594245911, Validation loss: 5.887466371059418\n",
      "Validation loss decreased (5.887466 --> 5.887466).\n",
      "Training loss for epoch 17: 6.237288594245911, Validation loss: 5.721650291891659\n",
      "Validation loss decreased (5.721650 --> 5.721650).\n",
      "Training loss for epoch 18: 6.237288594245911, Validation loss: 5.575852175553639\n",
      "Validation loss decreased (5.575852 --> 5.575852).\n",
      "Training loss for epoch 19: 5.7884475251902705, Validation loss: 5.428853160456607\n",
      "Validation loss decreased (5.428853 --> 5.428853).\n",
      "Training loss for epoch 20: 5.638610631227493, Validation loss: 5.287976831197739\n",
      "Validation loss decreased (5.287977 --> 5.287977).\n",
      "Training loss for epoch 21: 5.638610631227493, Validation loss: 5.166187507765634\n",
      "Validation loss decreased (5.166188 --> 5.166188).\n",
      "Training loss for epoch 22: 5.534929399490356, Validation loss: 5.043631152673201\n",
      "Validation loss decreased (5.043631 --> 5.043631).\n",
      "Training loss for epoch 23: 5.416023566172673, Validation loss: 4.928964091383892\n",
      "Validation loss decreased (4.928964 --> 4.928964).\n",
      "Training loss for epoch 24: 5.416023566172673, Validation loss: 4.838309879104297\n",
      "Validation loss decreased (4.838310 --> 4.838310).\n",
      "Training loss for epoch 25: 5.416023566172673, Validation loss: 4.758313493728638\n",
      "Validation loss decreased (4.758313 --> 4.758313).\n",
      "Duration Time:501.61715602874756\n",
      "Experiment 32 completed in 501.66 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "train_df, val_df = train_test_split(train_set, test_size=0.2, shuffle=True)\n",
    "\n",
    "def run_experiment(train_df, val_df, hyperparameters, experiment_count, total_experiments):\n",
    "\n",
    "    total_time_start = time.time()\n",
    "    cumulative_time = 0\n",
    "\n",
    "    experiment_number = 1\n",
    "    for hidden_dim in hyperparameters['hidden_dim']:\n",
    "        for learning_rate in hyperparameters['learning_rate']:\n",
    "            for batch_size in hyperparameters['batch_size']:\n",
    "                for optimizer in hyperparameters['optimizer']:\n",
    "                    for num_epochs in hyperparameters['num_epochs']:\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        print(f\"Experiment {experiment_number}/{total_experiments}: Testing parameters: hidden_dim={hidden_dim}, learning_rate={learning_rate}, batch_size={batch_size}, optimizer={optimizer}, num_epochs={num_epochs}\")\n",
    "                        train_dataloader = prepare_dataloader(train_df, batch_size, method='model2')\n",
    "                        val_dataloader = prepare_dataloader(val_df, batch_size, method='model2')\n",
    "\n",
    "                        writer = SummaryWriter()\n",
    "                        model = train_model(train_dataloader, val_dataloader, 300 + 140, hidden_dim, num_epochs, writer, device, vocab_size, learning_rate, optimizer)\n",
    "\n",
    "                        # End timing the experiment\n",
    "                        \n",
    "                        end_time = time.time()\n",
    "                        duration = end_time - start_time\n",
    "                        cumulative_time += duration\n",
    "\n",
    "                        print(f\"Experiment {experiment_number} completed in {duration:.2f} seconds.\")\n",
    "\n",
    "                        experiment_number += 1\n",
    "\n",
    "    writer.close()\n",
    "    \n",
    "run_experiment(train_df, val_df, hyperparameters, 0, total_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for different sizes of validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_hyperparameters = {\n",
    "    'hidden_dim': [128], \n",
    "    'learning_rate': [0.001],  \n",
    "    'batch_size': [32],  \n",
    "    'optimizer': ['Adam'],  \n",
    "    'num_epochs': [25]  \n",
    "}\n",
    "\n",
    "# Validation sizes to test\n",
    "validation_sizes = [0.1, 0.2, 0.3]\n",
    "\n",
    "for val_size in validation_sizes:\n",
    "    print(f\"\\nRunning experiments with validation set size: {val_size}\")\n",
    "    train_df, val_df = train_test_split(train_set, test_size=val_size, shuffle=True)\n",
    "    exp_time = run_experiment(train_df, val_df, specific_hyperparameters,  3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
